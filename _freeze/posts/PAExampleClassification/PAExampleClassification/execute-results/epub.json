{
  "hash": "66b2cb06244b9710950fe881f8c3ccdd",
  "result": {
    "markdown": "---\ntitle: \"People Analytics in R - Job Classification 'Revisited'\"\nauthor: \"Lyndon Sundmark, MBA\"\ndate: \"2016-04-30\"\ncategories: [HR Analytics]\n---\n\n\n\n\n\n# Introduction\n\nAbout a year back I posted a couple of blog articles on Data Driven Job Classification, showing a variety of tools to do this including R, Azure machine Learning and a few others. Its purpose was to encourage HR folks to start thinking about HR's future being more in the use of HR analytics.\n\n<https://www.linkedin.com/pulse/data-driven-job-classification-lyndon-sundmark-mba?trk=mp-author-card>\n\nAs the terminology in People Analytics is continuing to unfold and evolve, it is becoming increasing apparent that one of the best ways to understand People Analytics -both stringently and widely at the same time is to see it as:\n\n-   \"Data Driven\" HR and HR Decision Making\n-   When Data Science as a process and a field meets the HR context\n\nSeeing it this way helps prevent us from unnecessarily limiting the contribution that it can make, and yet at the same time help prevent proliferation of terminology to the point of meaninglessness. 'Data driven' must be how we conduct HR Management and decision making in the future. Data Science contributes to that goal by itself being a 'data driven' process.\n\nAnother way to not artificially limit the application of People Analytics, is to remind ourselves of the potential scope of the relevant HR context. People Analytics can be applied to:\n\n-   **information on what is happening to employees in the organization over time.** Typically this is thought of as HR metrics/demographics. Many still see this as the 'extent' of HR analytics.\n\n-   **how well the HR department conducts its business and operations.** These are metrics related to process improvement. Right now this is more typically thought of in the 'quality improvement realm. But it's still data driven decision making\n\n-   **'direct' embedding of statistical algorithms in our HR methodologies- how we actually 'do' HR.** There is huge application of People Analytics here. HR needs to get out of its traditional non-analytic methodologies paradigm where data science can be brought to bear.\n\n**People Analytics in R- Job Classification** is an example of embedding statistical algorithms in our HR methodologies.\n\nWith that in mind, I thought I would do a **'revisit'** of job classification as an example of People Analytics in R. By revisit, I mean let's restrict the tool to R, and let's apply the data science process/framework to it. This would put into into a format similar to my last 2 blog articles. Additionally, in this article more time will be spent on describing how we generate the data in the first place.\n\nThe files for this currrent blog article can be found in this location:\n\n<https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6433&authkey=!ABv-gHg5jVluYpc&ithint=folder%2cxlsx>\n\n# Applying the Data Science Process\n\nLet's remind ourselves of those steps again.\n\n## The Data Science Steps\n\n*1. Define A Goal*\n\n*2. Collect And Manage Data*\n\n*3. Build the Model*\n\n*4. Evaluate And Critique Model*\n\n*5. Present Results and Document*\n\n*6. Deploy Model*\n\n##1. Define A Goal\n\nOk - what is our goal here? Perhaps a quick primer on job classification would be in order to answer the question.\n\n### A Quick Job Classification Primer\n\nJob Classification is at the heart of compensation and salary administration in HR. We desire to pay our employees fairly- both from an external and internal perspective. Salary surveys help us out on the external side. But job classification helps us out on the internal picture. We try to understand the similarities and differences between jobs in the organization.\n\nAt a base level, this process starts with documenting job descriptions. We document tasks, knowleges, and skills needed to complete the work of the organization as organized within our jobs. Usually as a result of job descriptions being documented, we design broader categories that the job descriptions fall into. We call these job classifications. They attempt to categorize like with like and distinguish between job descriptions that are different. We often are concerned with the characteristics of how responsibility, accountability, supervision, education level, experience etc vary between these classifications. And to tie into our compensations systems, we often have paygrades assigned to the classification.\n\nSo what is our goal in Job Classification? To properly categorize 'job descriptions' into appropriate job classifications. When job classifications are written they are, by definition, of a 'known', 'intended' population. Job descriptions until they are categorized are outside of that population. Once proper categorization is made of a job description, it becomes part of that 'known' population. It is 'unknown' by the population until then. When all the job descriptions are categorized into job classifications, both the job descriptions and the job classifications are part of the known population. Any new job description written in the future is unknown until it is classified as well.\n\nWhy is this significant to People Analytics? The above process indicates that we are trying to classify something, or find the right category, that we **don't know** based on how it compares to a population we do **know**. HR job classification is the context here. It just so happens that in data science and statistics there are all sorts of algorithms designed to create categories or find the best fit among known categories.\n\nFor decades we have had job classification as a process in HR, and classification algorithms in statistics- but HR,in most organizations, is not recognizing this and the potential contribution it could/can make .\n\n**So in People Analytics in R -Job Classification:**\n\n**\\* our primary goal is to classify job descriptions into job classifications using the power of statistical algorithms to assist in prediction of best fit.** **\\* our secondary goal might be to help improve the design of our job classification system/framework.**\n\n## 2.Collect And Manage Data\n\nFor purposes of this application of People Analytics, this step in the data science process will take the longest initially. This is because in almost every organization, the existing job classifications or categories, and the job descriptions themselves are not typically represented in numerical format suitable for statistical analysis. Sometimes, that which we are predicting- the pay grade is numeric because point methods are used in evaluation and different paygrades have different point ranges. But more often the job descriptions are narrative as are the job classification specs or summaries. For this blog article, we will assume that and delineate the steps required.\n\n### Collecting The Data\n\nThe following are typical steps:\n\n1.  Gather together the entire set of narrative, written job classification specifications.\n2.  Review all of them to determine what the common denominators are- what the organization is paying attention to , to differentiate them from each other.\n3.  For each of the common denominators, pay attention to descriptions of how much of that common denominator exists in each narrative, writing down the phrases that are used.\n4.  For each common denominator, develop an ordinal scale which assigns numbers and places them in a 'less to more' order\n5.  Create a datafile where each record (row) is one job classification, and where each column is either a common denominator or the job classification identifier or paygrade.\n6.  Code each job classification narrative into the datafile recording their common denominator information and other pertinent categorical information.\n\n#### Gather together the entire set of narrative, written job classification specifications.\n\nThis initially represents the 'total' population of what will be a 'known' population. Ones that by definition represent the prescribed intended categories and levels of paygrades. These are going to be used to compare an 'unknown' population- unclassified job descriptions, to determine best fit. But before this can happen, we should have confidence that the job classifications themselves are well designed- since they will be the standard against which all job descriptions will be compared.\n\n#### Review all of them to determine what the common denominators are\n\nTechnically speaking, anything that appears in the narrative could be considered a feature that is a common denominator including the tasks, knowledges described. But few organizations have that level of automation in their job descriptions. So generally broader features are used to describe common denominators. Often they may include the following:\n\n-   Education Level\n-   Experience\n-   Organizational Impact\n-   Problem Solving\n-   Supervision Received\n-   Contact Level\n-   Financial Budget Responsibility\n\nTo be a common denominator they need to be mentioned or discernable in every job classification specification\n\n#### Pay attention to the descriptions of how much of that common denominator exists in each narrative\n\nFor each of the above common denominators ( if these are ones you use), go through each narrative identify where the common denominator is mentioned and write down the words used to describe how much of it exists. Go through you entire set of job classification specs and tabulate these for each common denominator and each class spec.\n\n#### For each common denominator, develop an ordinal scale\n\nOrdinal means in order. You order the descriptions from less than to more than. Then apply a numerical indicator to it. 0 might mean it doesnt exist in any significant way, 1 might mean something at a low or introductory level, higher numbers meaning more of it. The scale should have as many numbers as distinguishable descriptions.(You may have to merge or collapse descriptions if it's impossible to distinguish order)\n\n#### Create a datafile\n\nThis might be a spreadsheet.\n\neach record(row) will be one job classification, and each column will be either a common denominator or the job classification identifier or paygrade or other categorical information.\n\n#### Code each job classification narrative into the datafile\n\nRecord their common denominator information and other pertinent categorical or identifying information. At the end of this task you will have as many records as you have written job classification specs.\n\nAt the end of this effort you will have something that looks like the data found at the following link:\n\n<https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6435&authkey=!AL37Wt0sVLrsUYA&ithint=file%2ctxt>\n\n###Manage The Data\n\nIn this step we check the data for errors, organize the data for model building, and take an initial look at what the data is telling us.\n\n#### Check the data for errors\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n#MYdataset <- read.csv(\"jobclassinfo2.txt\")\nMYdataset<- read_csv(\"jobclassinfo2.txt\", \n    col_types = cols(PG = col_factor(levels = c(\"PG01\", \n        \"PG02\", \"PG03\", \"PG04\", \"PG05\", \"PG06\", \n        \"PG07\", \"PG08\", \"PG09\", \"PG10\"))))\n\nstr(MYdataset,width=80,strict.width =\"wrap\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [66 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n$ ID : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...\n$ JobFamily : num [1:66] 1 1 1 1 2 2 2 2 2 3 ...\n$ JobFamilyDescription: chr [1:66] \"Accounting And Finance\" \"Accounting And\n   Finance\" \"Accounting And Finance\" \"Accounting And Finance\" ...\n$ JobClass : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...\n$ JobClassDescription : chr [1:66] \"Accountant I\" \"Accountant II\" \"Accountant\n   III\" \"Accountant IV\" ...\n$ PayGrade : num [1:66] 5 6 8 10 1 2 3 4 5 4 ...\n$ EducationLevel : num [1:66] 3 4 4 5 1 1 1 4 4 2 ...\n$ Experience : num [1:66] 1 1 2 5 0 1 2 0 0 0 ...\n$ OrgImpact : num [1:66] 3 5 6 6 1 1 1 1 4 1 ...\n$ ProblemSolving : num [1:66] 3 4 5 6 1 1 2 2 3 4 ...\n$ Supervision : num [1:66] 4 5 6 7 1 1 1 1 5 1 ...\n$ ContactLevel : num [1:66] 3 7 7 8 1 2 3 3 7 1 ...\n$ FinancialBudget : num [1:66] 5 7 10 11 1 3 3 5 7 2 ...\n$ PG : Factor w/ 10 levels \"PG01\",\"PG02\",..: 5 6 8 10 1 2 3 4 5 4 ...\n- attr(*, \"spec\")=\n.. cols(\n..  ID = col_double(),\n..  JobFamily = col_double(),\n..  JobFamilyDescription = col_character(),\n..  JobClass = col_double(),\n..  JobClassDescription = col_character(),\n..  PayGrade = col_double(),\n..  EducationLevel = col_double(),\n..  Experience = col_double(),\n..  OrgImpact = col_double(),\n..  ProblemSolving = col_double(),\n..  Supervision = col_double(),\n..  ContactLevel = col_double(),\n..  FinancialBudget = col_double(),\n..  PG = col_factor(levels = c(\"PG01\", \"PG02\", \"PG03\", \"PG04\", \"PG05\", \"PG06\",\n   \"PG07\", \"PG08\",\n..  \"PG09\", \"PG10\"), ordered = FALSE, include_na = FALSE)\n.. )\n- attr(*, \"problems\")=<externalptr>\n```\n:::\n\n```{.r .cell-code}\nsummary(MYdataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       ID          JobFamily      JobFamilyDescription    JobClass    \n Min.   : 1.00   Min.   : 1.000   Length:66            Min.   : 1.00  \n 1st Qu.:17.25   1st Qu.: 4.000   Class :character     1st Qu.:17.25  \n Median :33.50   Median : 7.000   Mode  :character     Median :33.50  \n Mean   :33.50   Mean   : 7.606                        Mean   :33.50  \n 3rd Qu.:49.75   3rd Qu.:11.000                        3rd Qu.:49.75  \n Max.   :66.00   Max.   :15.000                        Max.   :66.00  \n                                                                      \n JobClassDescription    PayGrade      EducationLevel    Experience    \n Length:66           Min.   : 1.000   Min.   :1.000   Min.   : 0.000  \n Class :character    1st Qu.: 4.000   1st Qu.:2.000   1st Qu.: 0.000  \n Mode  :character    Median : 5.000   Median :4.000   Median : 1.000  \n                     Mean   : 5.697   Mean   :3.167   Mean   : 1.758  \n                     3rd Qu.: 8.000   3rd Qu.:4.000   3rd Qu.: 2.750  \n                     Max.   :10.000   Max.   :6.000   Max.   :10.000  \n                                                                      \n   OrgImpact     ProblemSolving   Supervision     ContactLevel  \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:3.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :3.000   Median :4.000   Median :4.000   Median :6.000  \n Mean   :3.348   Mean   :3.606   Mean   :3.864   Mean   :4.758  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.750   3rd Qu.:7.000  \n Max.   :6.000   Max.   :6.000   Max.   :7.000   Max.   :8.000  \n                                                                \n FinancialBudget        PG    \n Min.   : 1.000   PG05   :15  \n 1st Qu.: 2.000   PG03   : 7  \n Median : 5.000   PG04   : 7  \n Mean   : 5.303   PG06   : 7  \n 3rd Qu.: 7.750   PG08   : 7  \n Max.   :11.000   PG09   : 6  \n                  (Other):17  \n```\n:::\n:::\n\n\n\n\n\nOn the surface there doesn't seem to be any issues with data. This gives a summary of the layout of the data and the likely values we can expect. PG is the category we will predict. It's a categorical representation of the numeric paygrade. Education level through Financial Budgeting Responsibility will be the independent variables/measures we will use to predict. The other columns in file will be ignored.\n\n#### Organize the data\n\nLets narrow down the information to just the data used in the model.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMYnobs <- nrow(MYdataset) # 66 observations \nMYsample <- MYtrain <- sample(nrow(MYdataset), 0.7*MYnobs) # 46 observations\nMYvalidate <- sample(setdiff(seq_len(nrow(MYdataset)), MYtrain), 0.14*MYnobs) # 9 observations\nMYtest <- setdiff(setdiff(seq_len(nrow(MYdataset)), MYtrain), MYvalidate) # 11 observations\n\n\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:43:48 x86_64-w64-mingw32 \n\n# Note the user selections. \n\n# The following variable selections have been noted.\n\nMYinput <- c(\"EducationLevel\", \"Experience\", \"OrgImpact\", \"ProblemSolving\",\n     \"Supervision\", \"ContactLevel\", \"FinancialBudget\")\n\nMYnumeric <- c(\"EducationLevel\", \"Experience\", \"OrgImpact\", \"ProblemSolving\",\n     \"Supervision\", \"ContactLevel\", \"FinancialBudget\")\n\nMYcategoric <- NULL\n\nMYtarget  <- \"PG\"\nMYrisk    <- NULL\nMYident   <- \"ID\"\nMYignore  <- c(\"JobFamily\", \"JobFamilyDescription\", \"JobClass\", \"JobClassDescription\", \"PayGrade\")\nMYweights <- NULL\n```\n:::\n\n\n\n\n\nWe are predominantly interested in MYinput and MYtarget because they represent the predictors and what is to be predicted respectively. You will notice for the time being that we are not partitioning the data. This will be elaborated upon in model building.\n\n### What the data is initially telling us\n\nLets use the caret library again for some graphical representations of this data.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nfeaturePlot(x=MYdataset[,7:13],y=MYdataset$PG,plot=\"density\",auto.key = list(columns = 2))\n```\n\n::: {.cell-output-display}\n![](PAExampleClassification_files/figure-epub/unnamed-chunk-3-1.png)\n:::\n\n```{.r .cell-code}\nfeaturePlot(x=MYdataset[,7:13],y=MYdataset$PG,plot=\"box\",auto.key = list(columns = 2))\n```\n\n::: {.cell-output-display}\n![](PAExampleClassification_files/figure-epub/unnamed-chunk-3-2.png)\n:::\n:::\n\n\n\n\n\nThe first set of charts show the distribution of the independent variable values(predictors) by PG.\n\nThe second set of charts show the range of values of the predictors by PG. PG is ordered left to right in ascending order from PG1 to PG10. In each of the predictors we would expect increasing levels as we move up the paygrades and from left to right (or at least not dropping from previous paygrade).\n\nThis is the first indication by a graphic 'visual' that we 'may' have problems in the data or the interpretation of the coding of the information. Then again the coding may be accurate based on our descriptions and our assumptions false. We will probably want to recheck our coding from the job description to make sure.\n\n## 3.Build The Model\n\nLets use the rattle library to efficiently generate the code to run the following classification algorithms against our data:\n\n-   Decision Tree\n-   Random Forest\n-   Support Vector Machines\n-   Linear\n\n### Decision Tree\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rattle)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: tibble\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: bitops\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n```\n:::\n\n```{.r .cell-code}\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Decision Tree \n\n# The 'rpart' package provides the 'rpart' function.\n\nlibrary(rpart, quietly=TRUE)\n\n# Reset the random number seed to obtain the same results each time.\n#crv$seed <- 42 \n#set.seed(crv$seed)\n\n# Build the Decision Tree model.\n\nMYrpart <- rpart(PG ~ .,\n    data=MYdataset[, c(MYinput, MYtarget)],\n    method=\"class\",\n    parms=list(split=\"information\"),\n      control=rpart.control(minsplit=10,\n           minbucket=2,\n           maxdepth=10,\n        usesurrogate=0, \n        maxsurrogate=0))\n\n# Generate a textual view of the Decision Tree model.\n\nprint(MYrpart)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 66 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 66 51 PG05 (0.03 0.076 0.11 0.11 0.23 0.11 0.061 0.11 0.091 0.091)  \n   2) ProblemSolving< 4.5 47 32 PG05 (0.043 0.11 0.15 0.15 0.32 0.15 0.085 0 0 0)  \n     4) ContactLevel< 5.5 32 21 PG05 (0.062 0.16 0.22 0.22 0.34 0 0 0 0 0)  \n       8) EducationLevel< 1.5 15  9 PG03 (0.13 0.33 0.4 0.13 0 0 0 0 0 0)  \n        16) ProblemSolving< 1.5 5  2 PG02 (0.4 0.6 0 0 0 0 0 0 0 0) *\n        17) ProblemSolving>=1.5 10  4 PG03 (0 0.2 0.6 0.2 0 0 0 0 0 0)  \n          34) Experience< 0.5 3  1 PG02 (0 0.67 0 0.33 0 0 0 0 0 0) *\n          35) Experience>=0.5 7  1 PG03 (0 0 0.86 0.14 0 0 0 0 0 0) *\n       9) EducationLevel>=1.5 17  6 PG05 (0 0 0.059 0.29 0.65 0 0 0 0 0)  \n        18) Experience< 0.5 8  3 PG04 (0 0 0 0.62 0.37 0 0 0 0 0) *\n        19) Experience>=0.5 9  1 PG05 (0 0 0.11 0 0.89 0 0 0 0 0) *\n     5) ContactLevel>=5.5 15  8 PG06 (0 0 0 0 0.27 0.47 0.27 0 0 0)  \n      10) Experience< 2.5 12  5 PG06 (0 0 0 0 0.33 0.58 0.083 0 0 0)  \n        20) ContactLevel>=6.5 8  4 PG05 (0 0 0 0 0.5 0.38 0.13 0 0 0) *\n        21) ContactLevel< 6.5 4  0 PG06 (0 0 0 0 0 1 0 0 0 0) *\n      11) Experience>=2.5 3  0 PG07 (0 0 0 0 0 0 1 0 0 0) *\n   3) ProblemSolving>=4.5 19 12 PG08 (0 0 0 0 0 0 0 0.37 0.32 0.32)  \n     6) ProblemSolving< 5.5 13  6 PG08 (0 0 0 0 0 0 0 0.54 0.46 0)  \n      12) ContactLevel>=6.5 10  3 PG08 (0 0 0 0 0 0 0 0.7 0.3 0) *\n      13) ContactLevel< 6.5 3  0 PG09 (0 0 0 0 0 0 0 0 1 0) *\n     7) ProblemSolving>=5.5 6  0 PG10 (0 0 0 0 0 0 0 0 0 1) *\n```\n:::\n\n```{.r .cell-code}\nprintcp(MYrpart)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\nrpart(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], \n    method = \"class\", parms = list(split = \"information\"), control = rpart.control(minsplit = 10, \n        minbucket = 2, maxdepth = 10, usesurrogate = 0, maxsurrogate = 0))\n\nVariables actually used in tree construction:\n[1] ContactLevel   EducationLevel Experience     ProblemSolving\n\nRoot node error: 51/66 = 0.77273\n\nn= 66 \n\n        CP nsplit rel error  xerror     xstd\n1 0.137255      0   1.00000 1.00000 0.066756\n2 0.117647      1   0.86275 0.94118 0.070944\n3 0.088235      2   0.74510 0.90196 0.073207\n4 0.058824      4   0.56863 0.84314 0.075902\n5 0.039216      7   0.39216 0.72549 0.079060\n6 0.019608      9   0.31373 0.66667 0.079611\n7 0.010000     10   0.29412 0.62745 0.079611\n```\n:::\n\n```{.r .cell-code}\ncat(\"\\n\")\n```\n\n```{.r .cell-code}\n# Time taken: 0.02 secs\n```\n:::\n\n\n\n\n\n### Random Forest\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Random Forest \n\n# The 'randomForest' package provides the 'randomForest' function.\n\nlibrary(randomForest, quietly=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rattle':\n\n    importance\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\n# Build the Random Forest model.\n\n#set.seed(crv$seed)\nMYrf <- randomForest::randomForest(PG ~ .,\n      data=MYdataset[,c(MYinput, MYtarget)], \n      ntree=500,\n      mtry=2,\n      importance=TRUE,\n      na.action=randomForest::na.roughfix,\n      replace=FALSE)\n\n# Generate textual output of 'Random Forest' model.\n\nMYrf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = PG ~ ., data = MYdataset[, c(MYinput,      MYtarget)], ntree = 500, mtry = 2, importance = TRUE, replace = FALSE,      na.action = randomForest::na.roughfix) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 43.94%\nConfusion matrix:\n     PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 class.error\nPG01    0    2    0    0    0    0    0    0    0    0   1.0000000\nPG02    0    4    1    0    0    0    0    0    0    0   0.2000000\nPG03    0    1    4    2    0    0    0    0    0    0   0.4285714\nPG04    0    0    1    1    5    0    0    0    0    0   0.8571429\nPG05    0    0    0    3   10    2    0    0    0    0   0.3333333\nPG06    0    0    0    0    1    6    0    0    0    0   0.1428571\nPG07    0    0    0    0    0    3    1    0    0    0   0.7500000\nPG08    0    0    0    0    0    0    0    4    2    1   0.4285714\nPG09    0    0    0    0    0    0    0    4    1    1   0.8333333\nPG10    0    0    0    0    0    0    0    0    0    6   0.0000000\n```\n:::\n\n```{.r .cell-code}\n# List the importance of the variables.\n\nrn <- round(randomForest::importance(MYrf), 2)\nrn[order(rn[,3], decreasing=TRUE),]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 PG01  PG02  PG03 PG04  PG05  PG06  PG07  PG08  PG09  PG10\nEducationLevel   3.03 15.01 11.02 5.29  6.62  0.53  5.71  2.64 -4.39  7.53\nProblemSolving   4.32  6.71  9.89 0.31  6.67 13.25  8.27 13.62  9.85 20.17\nExperience      -4.09  9.90  7.71 5.48 -0.07  6.64  7.61 -6.13  6.24  2.62\nSupervision      0.00 10.24  5.21 1.42  6.71  5.28  3.09  4.77  3.98 15.87\nOrgImpact       -1.27 12.34  3.72 0.50  9.85  1.99  4.10  1.00  4.70 10.01\nContactLevel     3.03 13.39  2.95 2.65  2.84 10.17  3.16 11.95  0.93 10.32\nFinancialBudget  3.03  7.47  1.82 0.95  8.33  1.68 -0.43 -6.35 12.60 14.83\n                MeanDecreaseAccuracy MeanDecreaseGini\nEducationLevel                 17.69             4.39\nProblemSolving                 24.32             6.39\nExperience                     13.74             4.40\nSupervision                    17.23             3.94\nOrgImpact                      14.19             3.35\nContactLevel                   16.93             5.11\nFinancialBudget                14.06             5.22\n```\n:::\n\n```{.r .cell-code}\n# Time taken: 0.06 secs\n```\n:::\n\n\n\n\n\n### Support Vector Machine\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Support vector machine. \n\n# The 'kernlab' package provides the 'ksvm' function.\n\nlibrary(kernlab, quietly=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'kernlab'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n```\n:::\n\n```{.r .cell-code}\n# Build a Support Vector Machine model.\n\n#set.seed(crv$seed)\nMYksvm <- ksvm(as.factor(PG) ~ .,\n      data=MYdataset[,c(MYinput, MYtarget)],\n      kernel=\"rbfdot\",\n      prob.model=TRUE)\n\n# Generate a textual view of the SVM model.\n\nMYksvm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  0.1590451706759 \n\nNumber of Support Vectors : 64 \n\nObjective Function Value : -3.8365 -3.6651 -2.9457 -2.3023 -1.5381 -1.3245 -1.3818 -1.5199 -1.3797 -7.7121 -4.3681 -2.5666 -1.6355 -1.32 -1.3711 -1.5112 -1.3667 -8.5012 -6.4392 -3.076 -2.2725 -1.8466 -2.1004 -1.7137 -10.2265 -3.9238 -2.492 -2.0869 -2.4983 -1.8585 -10.1456 -6.4781 -3.8704 -3.8848 -2.2933 -7.6772 -5.55 -4.8013 -2.3109 -4.7105 -4.0305 -2.0938 -9.8884 -6.1275 -6.4759 \nTraining error : 0.272727 \nProbability model included. \n```\n:::\n\n```{.r .cell-code}\n# Time taken: 0.43 secs\n```\n:::\n\n\n\n\n\n### Linear Model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:17 x86_64-w64-mingw32 \n\n# Regression model \n\n# Build a multinomial model using the nnet package.\n\nlibrary(nnet, quietly=TRUE)\n\n# Summarise multinomial model using Anova from the car package.\n\nlibrary(car, quietly=TRUE)\n\n# Build a Regression model.\n\nMYglm <- multinom(PG ~ ., data=MYdataset[,c(MYinput, MYtarget)], trace=FALSE, maxit=1000)\n\n# Generate a textual view of the Linear model.\n\nrattle.print.summary.multinom(summary(MYglm,\n                              Wald.ratios=TRUE))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in sqrt(diag(vc)): NaNs produced\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], \n    trace = FALSE, maxit = 1000)\n\nn=66\n\nCoefficients:\n     (Intercept) EducationLevel  Experience  OrgImpact ProblemSolving\nPG02   31570.140     -29546.246 -10054.5402 -31742.949       22887.11\nPG03    5173.408     -13991.742  17611.5597 -23434.513       27580.11\nPG04  -12639.313       3742.145  -5763.6011 -11968.730       18528.68\nPG05  -29223.399       5164.052   1451.4069 -12579.427       21540.91\nPG06  -64936.850       5381.517   1332.3066 -10153.015       27846.30\nPG07  -55186.714       5379.689   1334.2282 -10155.643       25405.97\nPG08 -124914.252       6197.617    572.4667 -13900.558       42485.72\nPG09  -97759.116     -14428.508   8698.6998  -8681.908      -21901.43\nPG10 -201459.455       9187.762    765.1991 -15483.566       54919.25\n     Supervision ContactLevel FinancialBudget\nPG02   -3826.376   -3466.3364       14125.347\nPG03   -7979.596    4054.9295       -2131.931\nPG04   -1202.967   -4060.4297        6156.244\nPG05   -3137.087    -986.4325        5756.414\nPG06   -3540.354    -746.1415        6259.916\nPG07   -3539.699    -742.8932        6259.885\nPG08   -1862.503   -1725.1769        7267.505\nPG09   21068.207  -12739.5580       34119.536\nPG10   -2773.094    1152.7658        5960.684\n\nStd. Errors:\n      (Intercept) EducationLevel   Experience    OrgImpact ProblemSolving\nPG02 1.749631e-01   1.749631e-01 0.000000e+00 1.749631e-01   1.749631e-01\nPG03 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG04 0.000000e+00            NaN          NaN 1.776635e-14   7.258525e-15\nPG05 4.576029e-16   2.976903e-15 2.398744e-16 1.189550e-16   1.716121e-16\nPG06 3.859338e-01   1.397271e+00 6.650459e-01 1.366421e+00   1.543735e+00\nPG07 3.859338e-01   1.397271e+00 6.650459e-01 1.366421e+00   1.543735e+00\nPG08 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG09 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG10 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\n      Supervision ContactLevel FinancialBudget\nPG02 1.749631e-01 1.749631e-01    1.749631e-01\nPG03 0.000000e+00 0.000000e+00    0.000000e+00\nPG04 4.074351e-15          NaN             NaN\nPG05 7.348316e-17 2.343827e-15    4.111322e-17\nPG06 8.809021e-01 1.298034e+00    3.216738e-01\nPG07 8.809021e-01 1.298034e+00    3.216738e-01\nPG08 0.000000e+00 0.000000e+00    0.000000e+00\nPG09 0.000000e+00 0.000000e+00    0.000000e+00\nPG10 0.000000e+00 0.000000e+00    0.000000e+00\n\nValue/SE (Wald statistics):\n       (Intercept) EducationLevel   Experience     OrgImpact ProblemSolving\nPG02  1.804388e+05  -1.688713e+05         -Inf -1.814265e+05   1.308111e+05\nPG03           Inf           -Inf          Inf          -Inf            Inf\nPG04          -Inf            NaN          NaN -6.736740e+17   2.552679e+18\nPG05 -6.386192e+19   1.734706e+18 6.050696e+18 -1.057495e+20   1.255209e+20\nPG06 -1.682590e+05   3.851448e+03 2.003330e+03 -7.430373e+03   1.803826e+04\nPG07 -1.429953e+05   3.850140e+03 2.006220e+03 -7.432296e+03   1.645746e+04\nPG08          -Inf            Inf          Inf          -Inf            Inf\nPG09          -Inf           -Inf          Inf          -Inf           -Inf\nPG10          -Inf            Inf          Inf          -Inf            Inf\n       Supervision  ContactLevel FinancialBudget\nPG02 -2.186961e+04 -1.981181e+04    8.073327e+04\nPG03          -Inf           Inf            -Inf\nPG04 -2.952536e+17           NaN             NaN\nPG05 -4.269123e+19 -4.208639e+17    1.400137e+20\nPG06 -4.019010e+03 -5.748241e+02    1.946045e+04\nPG07 -4.018266e+03 -5.723216e+02    1.946035e+04\nPG08          -Inf          -Inf             Inf\nPG09           Inf          -Inf             Inf\nPG10          -Inf           Inf             Inf\n\nResidual Deviance: 13.0907 \nAIC: 157.0907 \n```\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"Log likelihood: %.3f (%d df)\n\", logLik(MYglm)[1], attr(logLik(MYglm), \"df\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog likelihood: -6.545 (72 df)\n```\n:::\n\n```{.r .cell-code}\nif (is.null(MYglm$na.action)) omitted <- TRUE else omitted <- -MYglm$na.action\ncat(sprintf(\"Pseudo R-Square: %.8f\n\n\",cor(apply(MYglm$fitted.values, 1, function(x) which(x == max(x))),\nas.integer(MYdataset[omitted,]$PG))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPseudo R-Square: 0.99516038\n```\n:::\n\n```{.r .cell-code}\ncat('==== ANOVA ====\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==== ANOVA ====\n```\n:::\n\n```{.r .cell-code}\nprint(Anova(MYglm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: PG\n                LR Chisq Df Pr(>Chisq)   \nEducationLevel   14.3433  9   0.110626   \nExperience       24.2064  9   0.003987 **\nOrgImpact         1.6140  9   0.996209   \nProblemSolving   21.1081  9   0.012179 * \nSupervision       2.9187  9   0.967430   \nContactLevel      4.8563  9   0.846653   \nFinancialBudget   5.5476  9   0.784206   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nprint(\"\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\n\"\n```\n:::\n:::\n\n\n\n\n\nNow lets plot the Decision Tree\n\n### Decision Tree Plot\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Time taken: 0.16 secs\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:52 x86_64-w64-mingw32 \n\n# Plot the resulting Decision Tree. \n\n# We use the rpart.plot package.\n\nfancyRpartPlot(MYrpart, main=\"Decision Tree MYdataset $ PG\")\n```\n\n::: {.cell-output-display}\n![](PAExampleClassification_files/figure-epub/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n\n\n\nA readable view of the decision tree can be found at the following pdf:\n\n<https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6449&authkey=!ACgJAX951UZuo4s&ithint=file%2cpdf>\n\n##4.Evaluate And Critique Model\n\n###Evaluate\n\nBecause we have multiple categories to be predicted, the only evaluation used is Error Matricies.\n\nLets see how well the models performed.\n\n#### Decision Tree\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#============================================================\n# Rattle timestamp: 2016-04-27 13:11:52 x86_64-w64-mingw32 \n\n# Evaluate model performance. \n\n# Generate an Error Matrix for the Decision Tree model.\n\n# Obtain the response from the Decision Tree model.\n\nMYpr <- predict(MYrpart, newdata=MYdataset[,c(MYinput, MYtarget)], type=\"class\")\n\n# Generate the confusion matrix showing counts.\n\ntable(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    0    2    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    6    0    1    0    0    0    0    0\n  PG04    0    1    1    5    0    0    0    0    0    0\n  PG05    0    0    0    3   12    0    0    0    0    0\n  PG06    0    0    0    0    3    4    0    0    0    0\n  PG07    0    0    0    0    1    0    3    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    3    3    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n```\n:::\n\n```{.r .cell-code}\n# Generate the confusion matrix showing proportions.\n\npcme <- function(actual, cl)\n{\n  x <- table(actual, cl)\n  nc <- nrow(x)\n  tbl <- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) <- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper <- pcme(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr)\nround(per, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01    0 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  1.00\n  PG02    0 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG03    0 0.00 0.09 0.00 0.02 0.00 0.00 0.00 0.00 0.00  0.14\n  PG04    0 0.02 0.02 0.08 0.00 0.00 0.00 0.00 0.00 0.00  0.29\n  PG05    0 0.00 0.00 0.05 0.18 0.00 0.00 0.00 0.00 0.00  0.20\n  PG06    0 0.00 0.00 0.00 0.05 0.06 0.00 0.00 0.00 0.00  0.43\n  PG07    0 0.00 0.00 0.00 0.02 0.00 0.05 0.00 0.00 0.00  0.25\n  PG08    0 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00\n  PG09    0 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.05 0.00  0.50\n  PG10    0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00\n```\n:::\n\n```{.r .cell-code}\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n23\n```\n:::\n\n```{.r .cell-code}\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n28\n```\n:::\n:::\n\n\n\n\n\n#### Random Forest\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate an Error Matrix for the Random Forest model.\n\n# Obtain the response from the Random Forest model.\n\nMYpr <- predict(MYrf, newdata=na.omit(MYdataset[,c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    1    1    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    7    0    0    0    0    0    0    0\n  PG04    0    0    0    7    0    0    0    0    0    0\n  PG05    0    0    0    0   15    0    0    0    0    0\n  PG06    0    0    0    0    0    7    0    0    0    0\n  PG07    0    0    0    0    0    0    4    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    0    6    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n```\n:::\n\n```{.r .cell-code}\n# Generate the confusion matrix showing proportions.\n\npcme <- function(actual, cl)\n{\n  x <- table(actual, cl)\n  nc <- nrow(x)\n  tbl <- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) <- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper <- pcme(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr)\nround(per, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.5\n  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00   0.0\n  PG06 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00   0.0\n  PG07 0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00   0.0\n  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00   0.0\n  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00   0.0\n  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09   0.0\n```\n:::\n\n```{.r .cell-code}\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n```\n:::\n\n```{.r .cell-code}\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5\n```\n:::\n:::\n\n\n\n\n\n#### Support Vector Machine\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate an Error Matrix for the SVM model.\n\n# Obtain the response from the SVM model.\n\nMYpr <- kernlab::predict(MYksvm, newdata=na.omit(MYdataset[,c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    0    1    1    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    5    0    2    0    0    0    0    0\n  PG04    0    0    0    6    1    0    0    0    0    0\n  PG05    0    0    0    1   12    2    0    0    0    0\n  PG06    0    0    0    0    2    5    0    0    0    0\n  PG07    0    0    0    0    0    4    0    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    4    2    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n```\n:::\n\n```{.r .cell-code}\n# Generate the confusion matrix showing proportions.\n\npcme <- function(actual, cl)\n{\n  x <- table(actual, cl)\n  nc <- nrow(x)\n  tbl <- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) <- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper <- pcme(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr)\nround(per, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01    0 0.02 0.02 0.00 0.00 0.00    0 0.00 0.00 0.00  1.00\n  PG02    0 0.08 0.00 0.00 0.00 0.00    0 0.00 0.00 0.00  0.00\n  PG03    0 0.00 0.08 0.00 0.03 0.00    0 0.00 0.00 0.00  0.29\n  PG04    0 0.00 0.00 0.09 0.02 0.00    0 0.00 0.00 0.00  0.14\n  PG05    0 0.00 0.00 0.02 0.18 0.03    0 0.00 0.00 0.00  0.20\n  PG06    0 0.00 0.00 0.00 0.03 0.08    0 0.00 0.00 0.00  0.29\n  PG07    0 0.00 0.00 0.00 0.00 0.06    0 0.00 0.00 0.00  1.00\n  PG08    0 0.00 0.00 0.00 0.00 0.00    0 0.11 0.00 0.00  0.00\n  PG09    0 0.00 0.00 0.00 0.00 0.00    0 0.06 0.03 0.00  0.67\n  PG10    0 0.00 0.00 0.00 0.00 0.00    0 0.00 0.00 0.09  0.00\n```\n:::\n\n```{.r .cell-code}\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n27\n```\n:::\n\n```{.r .cell-code}\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n36\n```\n:::\n:::\n\n\n\n\n\n#### Linear Model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate an Error Matrix for the Linear model.\n\n# Obtain the response from the Linear model.\n\nMYpr <- predict(MYglm, newdata=MYdataset[,c(MYinput, MYtarget)])\n\n# Generate the confusion matrix showing counts.\n\ntable(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    1    1    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    7    0    0    0    0    0    0    0\n  PG04    0    0    0    7    0    0    0    0    0    0\n  PG05    0    0    0    0   15    0    0    0    0    0\n  PG06    0    0    0    0    0    6    1    0    0    0\n  PG07    0    0    0    0    0    2    2    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    0    6    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n```\n:::\n\n```{.r .cell-code}\n# Generate the confusion matrix showing proportions.\n\npcme <- function(actual, cl)\n{\n  x <- table(actual, cl)\n  nc <- nrow(x)\n  tbl <- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) <- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper <- pcme(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr)\nround(per, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.50\n  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00  0.00\n  PG06 0.00 0.00 0.00 0.00 0.00 0.09 0.02 0.00 0.00 0.00  0.14\n  PG07 0.00 0.00 0.00 0.00 0.00 0.03 0.03 0.00 0.00 0.00  0.50\n  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00\n  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00  0.00\n  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00\n```\n:::\n\n```{.r .cell-code}\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6\n```\n:::\n\n```{.r .cell-code}\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11\n```\n:::\n:::\n\n\n\n\n\n### Critique\n\nIt turns out that:\n\n-   The model that performed best was Random Forests at 2% error\n-   The linear model was next at 6% error.\\\n-   Support Vector Machines performed less well at 18% error.\n-   And Decision trees, while being able to give us 'visual' on what rules are being used, performed worst of all at 23% error.\n\nEarlier I said ,that for purposes of this analysis, we would not have training and test datasets. This is because the total population of our data is only 66 records which are scattered among up to 10 categories. Unless we took special care, if we randomly created test and training datasets, we could not guarantee that each of these datasets would have all 10 groups represented. So we use all the data to predict itself.(usually not recommended if lots of data)\n\nIf as an organization , we were just starting out with this kind of endeavor, at this point you would only have the job classification specs coded into the dataset. it would be the only 'known' population. In some ways it is useful to do this here- to gauge how well designed our paygrades are 'differentiated' from each other. Even though we know that the known population predicting itself will give lower error rates, suffice to say that here, the hypothetical organization is seeing results that would make it worthwhile to expand the coding effort. If at this stage, the data could not reliably predict itself, we might to rethink our approach.\n\nIn most organizations who have written job descriptions and job classification specs, their job descriptions **are** already classified as well. So we arent necessarily restricted to just coding the job class specs. We could go ahead and code the job descriptions on the same common denominator features.( outside the context of this blog article) This would make the 'known' population quite a bit bigger. On a bigger population too we could **also** have the population predict itself but additionally do cross validation and have both training and test datasets.\n\nWhile the above results were found on just the job class specs, it would be wise to have a much larger population before deciding on which model is best to deploy in real life.\n\nOne other observation- you noticed in the results of the various models, that some model had predictions that were one or two paygrades off 'higher or lower' than the actual existing paygrade.\n\nIn a practical sense this might mean:\n\n-   these might be candidates for determining whether criteria/features for these pay grades should be redefined\n-   and or whether there are ,in reality, fewer categories needed.\n\nWe could extend our analysis and modelling to 'cluster' analysis.This would create a newer grouping based on the existing characteristics, and then the classification algorithms could be rerun to see if there was any improvement.\n\nSome articles on People Analytics suggest that on a 'maturity level' basis, the step/stage beyond prediction is 'experimental design'. If we are using our results to modify our design of our systems to predict better, that might be an example of this.\n\n## 5.Present Results And Document\n\nAs with previous blog articles, a good way of carrying out this step is this the .rmd file which is used to create this blog article. R Markdown language is used to create this narrative as well as allow for 'inline' inclusion of the R program and its output. The rmd file can be found here:\n\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6467&authkey=!AE4IyMNEaoLgqnw&ithint=file%2cRmd\n\n##6.Deploy The Model\n\nIn R the easiest form of deploying the model, is to run your unknown data against the model . Put the data in a separate dataset and run the following R commands:\n\nHere is dataset:\n\n<https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6478&authkey=!ALYidIIpaCrfnf4&ithint=file%2ccsv>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#DeployDataset <- read.csv(\"~/Documents/OneDrive/Public/R Job Classification Example/Deploydata.csv\")\nDeployDataset <- read.csv(\"Deploydata.csv\")\nDeployDataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  EducationLevel Experience OrgImpact ProblemSolving Supervision ContactLevel\n1              2          0         3              4           1            5\n  FinancialBudget\n1               4\n```\n:::\n\n```{.r .cell-code}\nPredictedJobGrade <- predict(MYrf, newdata=DeployDataset)\nPredictedJobGrade\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1 \nPG05 \nLevels: PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n```\n:::\n:::\n\n\n\n\n\nThe DeployDataset represemts the information coded from a single job description (paygrade not known). PredictedJobGrade compares the coded values against the MYrf (random forest model) and the prediction is determined. In this case - PG05.\n\n# Final Comments\n\nThis is the third in a series of blog articles I have written to illustrate the use of People Analytics- **data driven** decision making for HR.\n\nThese articles have had the intention of providing real tangible examples of the application of data science to HR, to illustrate the data science process in the HR context, and to show that the scope mentioned previously in this article, isnt just theoretical- its real.\n\nNone of the articles is intended to illustrate necessarily best practices, but rather to show a structured process of thinking and analysis. The intention has also been to encourage more of being 'data driven' in HR, making People Analytics not an addon to HR but rather THE way we conducted HR decision making in the future, where applicable, human judgement is 'added on' to a rigorous analysis of the data in the first place.'\n",
    "supporting": [
      "PAExampleClassification_files/figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}