[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html",
    "href": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html",
    "title": "The Challenge for HR Analytics -2020 And Beyond",
    "section": "",
    "text": "It’s been about six years since I started writing blog articles related to HR Analytics. This is because of the increasing visibility of activity in this field.\nAny of us with an interest in this field has witnessed the evidence of the following during that time:\n\nIncreasing numbers of people writing in this field- either books or blog articles or both (myself included)\nThe emergence of increasing options for education in this field whether it be:\n\nOnline through courses offered by AnalyticsInHR, Datacamp, Udemy and others\nDegree programs in Universities offering courses in this area\n\nAn Increasing frequency of HR Analytics meetups across the globe\nAn explosion of job postings on places like Indeed or Monster for specialized analytics roles.\n\nThis increasing visibility is extremely gratifying to see. Those of you who have read my book and/or LinkedIn blog articles will remember one of the things that I have shared- the ability to do HR Analytics is not new. The basic building blocks:\n\nthe existence of HR information\nthe technology to store that information and\nthe means to statistically analyze that information\n\nhave been around for at least 40 years. I think what we have seen in the last 6 years is a great awakening to this. The tools and technologies have increased in their robustness over that time, but the means to be ‘data-driven’ at some level have been there for quite some time.\nIt’s an exciting time to be in HR. One might think then that HR Analytics has finally arrived in HR and that its future is certain. Yet, despite the increasing visibility and presence, it’s future is likely not to be without challenges.\nMost of my writing in HR Analytics focuses on definitions, concepts, frameworks, and hands-on examples to stir up passion and interest in this field. And I will continue to write on those areas periodically for that reason.\nIn this blog article, I will take a slight turn in direction and make this a personal opinion piece or commentary. My reason is that because we have just finished another decade in this millennium. It seemed like an appropriate time for reflection. Your experiences may or may not have been similar to mine.\nWith that in mind, I think that for 2020 and beyond, there are still going to be fundamental challenges for HR Analytics going forward."
  },
  {
    "objectID": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html#challenge-1---continuing-to-get-into-the-mainstream-of-hr-thinking",
    "href": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html#challenge-1---continuing-to-get-into-the-mainstream-of-hr-thinking",
    "title": "The Challenge for HR Analytics -2020 And Beyond",
    "section": "Challenge 1 - Continuing to get into the mainstream of HR thinking",
    "text": "Challenge 1 - Continuing to get into the mainstream of HR thinking\nAs mentioned earlier, there has been much evidence in the last 6 years or so that this field is increasing in its visibility and being recognized. But I am not convinced that it is yet universally in the mainstream of HR thinking. And it’s because of the presence of a few obstacles.\n\nObstacle 1 - lack of standardization of definitions\nNowhere is this more evident than in job postings.\nYou often see:\n\nthe same terminology describing vastly different things and\nvastly different terminology describing the same thing.\n\nA cursory review on any given day on the job boards for HR Analytics jobs will bear this out. The term HR Analytics in jobs covers everything from simple traditional HR Reporting jobs to make them sound more enticing and exciting, to HR data warehousing roles, to data scientist roles, to those responsible for the entire infrastructure of HR Analytics.\nFor the most part - this is due to the reality that recruitment is the ‘marketing’ function of HR. The goal here to attract good candidates. The need is to make your place of employment sound cooler and more attractive than your competitor. And your job postings should achieve that.\nBut not at the expense of consistent use of terminology.\nAlmost 40 years ago I came across a quote that I will never forget (although I forgot the source):\n“Any discipline will rise, or fall based on the reliability and validity of the observations on which it’s based”\nThis applies to terminology in any field or discipline as well.\nThe degree to which we use terminology in a consistent and coherent fashion is particularly important as a field or discipline is emerging. It clarifies our thinking. When we standardize on definitions - then measurement can be standardized. When measurement is standardized, good quality research is possible because research can then be replicated and confirmed by others. When research can be replicated and confirmed by others the field moves forward.\nWhile at any given time there may be a degree of standardization evident in our terminology, when a field is just emerging it tends to be characterized by more a ‘lack of it’ than ‘presence of it’. Often this is because people are still just trying to get their heads around it.\n\nIf a job is truly an HR Reporting job, advertise it as such.\nIf its HR Data Warehousing and Infrastructure- advertise it as such.\nIf you are looking for HR Data Scientists -advertise it as such.\n\nWe shouldn’t throw in the term ‘HR Analytics’ just for the sake of sounding competitive or ‘hip’.\nWhile inconsistent use of terminology is often found in job postings, this isn’t to say that at the same time there aren’t activities/initiatives that work toward consistent use of terminology.\nWhen you look at the four evidences mentioned previously, at least 3 of them generally are mostly geared towards having the effect of helping standardize on the terminology. This is because they are educational in their intent.\n\nYou can’t really write about this field unless you start with defining it and going from there.\nYou can’t really develop educational courseware for it for the same reason.\nAnd I suspect the reason for most HR Analytic meetups is to network with like-minded professionals and for purposes of developing a common language about the subject and for shared experiences.\n\nSuffice to say, lack of standardization of terminology or inconsistent use of it will always work against a discipline being part of the mainstream thinking in organization because it creates confusion and therefore lack of trust.\n\n\nObstacle 2- lack of high-quality foundational definitions\nEven if we are vigilant in our use of consistent terminology, how we ‘define’ HR Analytics can still be an obstacle to mainstream HR thinking.\nIt goes without saying, that ‘quality’ builds of anything start with quality foundations. And foundations generally limit whatever is build after they are laid. This is particularly true of HR Analytics. How you define it determines what you will build. What you build determines what you can do with it.\n\nOrganizations CAN define HR Analytics as just a fancy term for HR Reporting as some job postings do. In this case there is very little to build. And, of course, you will be limited to what ‘this’ HR Analytics role will be able to do for the organization other than HR reporting.\nOrganizations CAN define it as just HR data warehousing and fancy graphics and storytelling and KPIs. Lots of data with nowhere to go. This is the problem with most HR metrics initiatives these days. Traditional HR metrics are a fundamental and important part of the HR Analytics picture- but they are only a part. Define HR Analytics as just HR Metrics with storytelling and again you are limiting your impact, successes etc. The inevitable question becomes: “ok -now what”. This is because HR Metrics often are just provided/generated at the ‘descriptive analytics’ level which simply answers the question- what happened? or what is happening? Very often external vendors will describe HR Analytics in this way (metrics and storytelling) where the agenda is ‘selling their product’. Your organizational success may or may not be on their radar.\n\nThe problem with both of the above is that organizations are defining only part of the picture and thinking it’s the whole picture and having expectations of success that would only be achievable by the whole picture. Additionally, the effect of defining ‘parts’ of the picture – artificially and unintentionally limit the ‘what could be’ in our efforts.\nGood foundational definitions describe the lay of the land in its entirety which then allows for whatever activities we engage in within that entirety to be seen accurately for what they are and equally important for what they aren’t. They allow for an organization to know what is still left to do. This results in expectations that are realistic for the level and nature of the activity that they have engaged in.\nWhile it’s not the only good definition, its why I always come back to defining HR Analytics as:\n“Data-Driven HR Management and Decision Making”\nIt’s wide enough to encompass:\n\nHR Reporting\nHR Data Warehousing\nTraditional HR Metrics\nHR Service Operations Measurement and Improvement\nHR Data Science\nAll HR Functions\n\nThese activities all characterize in some way ‘data-driven’ and the definition covers ‘why we do it’- for HR Management and Decision Making – be it operations, policies, or methodologies. It’s all of HR.\nConsistency in terminology and good foundational definitions will both be necessary for HR Analytics to get into the mainstream of HR thinking."
  },
  {
    "objectID": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html#challenge-2--enduring-for-the-long-term",
    "href": "posts/HRAnalyticsChallenge/HRAnalyticsChallenge.html#challenge-2--enduring-for-the-long-term",
    "title": "The Challenge for HR Analytics -2020 And Beyond",
    "section": "Challenge 2 -Enduring for The Long Term",
    "text": "Challenge 2 -Enduring for The Long Term\nAssuming that HR Analytics finally gets widely into the mainstream of HR, there still is the challenge of it enduring for the long term. Here too there are obstacles that work against this. At least a couple come to mind.\n\nObstacle 3- Seeing HR Analytics as an add-on to HR rather than ‘the way we do HR from now on’\nThis is a huge obstacle. 2020 marks 40 years that I have been in the HR profession. I don’t know if others’ experience in this field has been the same as a mine, but changes in the way we do HR over that period of time have been agonizingly and painfully slow. This is true at least for both truly integrating technology deeply in the way we do business in HR and /or understanding the ‘informational’ dimension of our work.\nAs I periodically attended HR conferences over the years, it became more and more apparent to me that peoples’ interest in these were often more for professional networking and for vendors hawking their wares, or for consultants to make a name for themselves and their business. In other words, what seemed to drive change in these ‘professional development’ events educationally were what was cool currently, or the flavor of the month. I often didn’t get the impression that there was a whole lot of academic research/evidence behind the latest and greatest. Maybe that’s just me.\nIf you are comfortable with the HR Analytics definition I previously shared, it should raise the somewhat uncomfortable and unsettling question ‘If HR Analytics is ’data-driven’ HR Management and Decision Making – what is driving HR Management and Decision Making currently? What drives our choices in HR Mainstream thinking and professional development in the absence of ‘data-driven’?\nWhen we don’t fully grasp that ‘data-driven’ is THE way of conducting HR Management and Decision Making going forward, we tend to see HR Analytics as an add-on. It’s not an add-on. HR Analytics means that we inject ‘data-driven’ into the DNA of HR practices, knowledge and professional education itself.\nEvery aspect of HR practices and methodology to be understood in terms of its informational implications:\n\nwhat does this HR function or practice generate in terms of information\nwhat does it do with it\nhow do we make better decisions from it, and\nhow do we measure that we are making better HR decision outcomes and measure the ‘how of making decisions better’. (i.e. processes and outcomes)\n\nIn other words, truly ‘data-driven’, not business as usual. If we see it as an add-on, we’re missing the point, and it will have the propensity to fail -or at least severely limit the potential contribution it can make.\n\n\nObstacle 4- Seeing HR Analytics as a special project or an initiative requiring special funding/resources/permission to get started\nIt’s been my experience in too many organizations that good ideas often die on that altar of ‘business case’:\n\nHave a good idea?\nOk, make a business case for it.\nAnyone else done it?\nWas it successful?\nWhere’s the proof?\nHow much are we risking in the provision of resources?\nHow does this stack up to other organizational initiatives that are competing for the same pool of resources?\n\nYou get the idea.\nWe often see doing anything here as a special project when we think of HR Analytics as an add-on and when we ask for permission to engage in ‘that’ add-on. It also doesn’t help when we think that ‘that’ special project isn’t our responsibility as current resources within the HR function. Data-driven HR management and decision making– means that, from now on, all of us are involved in changing the way we do business in HR. It ISNT someone else’s job.\nIt used to be that we could use cost as an excuse (i.e. the business case approach needed). And this may have been true when statistical software and data manipulation and data munging and graphics tools were all commercial and costed money. These days you have choices for completely free statistical tools and often free or ‘near-free’ data manipulation tools for developers/prototyping ideas. If you have rightful access to information for organization purposes, there are very few barriers to being ‘data-driven’ if any. It’s more of a ‘current mindset’ limitation than it is an ‘organizational’ limitation. If you know Excel, understand CSV files and are prepared to learn the R Statistical software along with learning Statistics and the Data Science process/framework, then you have the beginning building blocks to be data-driven.\nYes – as the demand for data-driven HR grows, the need for more robust infrastructure grows. This may cost money and require the need for business cases. But the point is that your business case is justified based on what you have already accomplished – not starting will a completely untested unknown.\nStarting HR Analytics begins with:\n\nrightful access to data\nunderstanding HR informationally\nfree statistical tools\nAn organization problem/question requiring an answer\nAn understanding of basic univariate and multivariate statistics.\nAnd as you get more advanced, learning machine learning algorithms for prediction and classification problems.\n\nHR Analytics isn’t them or they- it’s all of us. We simply need to decide to do HR differently."
  },
  {
    "objectID": "posts/ScopeAmdBreadth/ScopeAndBreadth.html",
    "href": "posts/ScopeAmdBreadth/ScopeAndBreadth.html",
    "title": "Is It Time To Revisit The Scope and Breadth of People / HR Analytics?",
    "section": "",
    "text": "Introduction\nAs I continue to peruse much of what is being written on the internet on ‘all things People/HR Analytics’, a huge amount of it is geared towards talking about traditional HR metrics and its relationship to other business metrics. I think this is a ‘good’ and ‘natural’ starting point both in terms of understanding and implementing People/HR analytics initiatives in organizations. And indeed, many organizations strive for this as their first evidence of ‘visibility’ that they are engaging in it. With it, they are often presenting a picture to the organization for the first time, of the human resources in their organization.\nFor some, their perspective may be that this is the limit of what People /HR Analytics is, and can do for them. Their perspective is limited by how they have defined it for themselves. Definitions in People/HR Analytics are critical for that reason. They set the boundaries for us at any given point in time as to what activities are and aren’t deemed to be People/HR Analytics.\n\n\nThe Real Scope And Breadth\nFor those of you who have read my book\nhttps://www.amazon.com/Doing-HR-Analytics-Practitioners-Handbook/dp/1973716372\nand my compendium of Linkedin blog articles on People/HR Analytics found on my LinkedIn profile page, you know that I view this field as being understood and defined as ‘data-driven’ HR management and decision making.\nIf you accept that definition, a few things may emerge as self-evident in your thinking:\n\nYou will probably conclude traditional HR metrics probably is not the full scope of People/HR Analytics. Its an important part, but only part of the picture.\nYou can, and probably should, think of the scope of ‘data-driven’ as potentially ‘ALL’ of HR, so as to not unnecessary limit its potential applicability.\nYou should recognize that this will mean it is NOT ‘business as usual’ HR. While the purposes of almost all of what we do in HR don’t change- and indeed are based on the functions, responsibilities and expectations put on us by the organization and the business it is in- the way we think of how we do HR will likely need to change to accommodate a ‘data-driven’ -new methodologies, new tools and so on.\nYou should probably conclude that applying Data Science to HR information will likely be behind much of the changes in the ‘how we do HR’.\n\nAll this being said, the scope of ‘ALL’ HR may still be somewhat nebulous and fuzzy out there- truthful in the potential- but still difficult to get one’s head around to action this – nevertheless.\nIn my book I suggest 3 major spheres of applicability in that ‘ALL’ scope:\n\nTraditional HR metrics -describing our employee population over time and what is happening to it. (And understanding the relationship of these to other more general business and organization metrics). As mentioned above, this is a part of People/HR Analytics and an important part.\nOur HR operations- monitoring, understanding, and continuously improving the services HR provides to its customers\nEmbedding Data Science frameworks and Machine Learning models into HR decision and policy making and HR processes directly. This is category most impacting on how we do the business of HR differently to be ‘data driven’.\n\nMost of you might be familiar with much of the scope of Traditional HR Metrics. If not, you can peruse the glossary and standards at the following link as an example:\nhttps://www.hrmetricsservice.org/standards-and-glossary/\nof you might be familiar with the sphere of applicability to HR operations. For some of you it may have been in the form of a ‘continuous improvement’ initiative in HR- identifying who your customers are, your products and services to them, and the processes that provide them.\nThe third sphere above is probably the ‘foggiest’ for most people. How do we get our heads around this- both in the doing and engaging in, and understanding the ‘full scope’ of what this may cover? I think the ‘full scope’ of what this is – is still emerging. Probably the reason for that is that this is only limited by the variety of HR issues faced and imaginations of HR professionals.\nIndeed, it may useful for the HR professional community to document the uses it has actually put data science to in HR - so that others can learning from it -thereby fundamentally change (over time) the business of HR.\nBut even so, how do we even know whether something has the potential to be considered as a candidate for the application of data science to HR?\nI think the answer to that question can be found in:\n\nFirst understanding what questions data science and machine learning models were designed to answer.\nThen think about what the equivalent of those types of questions would be within the scope of known HR functions.\n\nI recently came across the following article written by a fellow at Microsoft-Brandon Rohrer:\nhttps://learn.microsoft.com/en-us/archive/blogs/machinelearning/what-types-of-questions-can-data-science-answer\nwhich is an excellent answer to the first bullet point just above and is very much worth the read.\nIn summary:\n\nWhat Type of Questions Can Data Science Answer?\nIs this A or B? -Two-Class Classification\nIs this A or B or C or D? Multi-class classification\nIs this weird or unusual? Anomaly Detection\nHow Much / How Many? Regression\nWhat is the probability that something is A or B or C or D? Multi-class classification as regression\nWhat is the probability that something is A or B? Two-class classification as regression\nHow is this data organized? Clustering or Dimensionality reduction\nWhat should I do now? Reinforcement Learning\n\nThis is a very abbreviated summary of that article, but it very quickly sharpens our focus and structures our thinking with respect to the application of Data Science to HR. The scope of application is huge, both with respect to HR decision making for HR policy purposes and for direct embedding of machine learning models in our HR practices and processes.\nWhat are those questions in the HR Context?\nLet’s do some quick ‘sample brainstorming’ in the HR functions and HR Metrics:\nRecruitment\n· Is this hire going to be successful or unsuccessful in our organization? (Two-Class Classification)\n· Will this candidate pass probation if we hire them? (Two-Class Classification)\n· What is the probability of this candidate being successful if hired? (Two-class classification as regression)\n· What is the probability of this candidate passing probation? Two-class classification as regression\n· What will our hiring need to be next year based on historical terminations? (Regression)\nAbsenteeism\n· Is this person’s absenteeism excessive as compared to other employees? (Anomaly Detection)\n· What is are projected absenteeism costs for next year? (Regression)\nCompensation and Salary Administration\n· What is the proper job family classification for this position I am classifying? (Multi-class classification)\n· We are trying to broadband our numerous job classes into a smaller number of categories. How many groups should we really have? (Clustering)\n· What is the proper paygrade for this new position? (Multi-class classification)\n· Is this position a professional or managerial level position? (Two-Class Classification)\n· What management level is this position (Multi-class classification)\nTerminations\n· Is our turnover rate abnormally high this year? (Anomaly Detection)\n· What is our projected turnover rate next quarter? (Regression)\n· Who exactly would we predict as terminates for next year? (Two-Class Classification)\n· What is the probability that this specific individual will quit next year? (Two-class classification as regression)\nHealth and Safety\n· What will be our predicted accident/injury rate for next year? (Regression)\n· Is the occurrence of this type of accident unusual? (Anomaly Detection)\n· Is this type of work high risk? (Two-Class Classification)\nLabour Relations\n· We have fairly high rate of grievances in this area or this year. Is this unusual? (Anomaly Detection)\nThe above is just a ‘smattering’ of example HR questions that are asked often. And each of them is an HR context ‘specific’ question of what would otherwise be a variation of a data science question. Perhaps is would be useful to gather other questions of some internet site somewhere, that emanate from HR on a regular basis, with the intent of determining whether the HR question is something that is really a data science question in disguise.\nThe whole point of the above examples is to illustrate the wider scope of application of HR Analytics. ‘Data-driven’ really means a rethink of how we do ‘ALL’ of HR. Its not an add-on area of HR. It concerns itself with the very fabric of HR management and how HR decisions are made. It means we look for opportunities to improve and make better, and that the judging and determination of what is ‘improvement and making better’ is ‘evidence-based’ rather than just subjective judgement.\nDo we truly believe that HR Analytics IS ‘data driven HR management and decision making’ or not? If we do, then we must see the scope and breadth of its application as big as what the above definition really implies. To do otherwise, unintentionally limits the contribution of what HR Analytics can do for our organizations.\n\n\nConclusion\nI hope that the above ideas are helpful to you as HR Analytics practitioners to both ‘keep wide’ your understanding of HR Analytics, and to more clearly see how much of the HR analytics picture you are engaging in at any given point in time, and how much is yet to be done."
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "Earlier this year I wrote a few blog articles on HR analytics. In one of those articles I indirectly expressed doubts, concerns, and readiness of the HR profession to capitalize on the promise of HR analytics and take rightful claim and ownership of it within the services that HR provides.\nhttps://www.linkedin.com/pulse/why-hr-might-able-reinvent-itself-lyndon-sundmark-mba?trk=mp-reader-card\nI mentioned a number of obstacles that might exist-historical and current- that could be standing in the way. Many of these are related to how HR views itself as a profession and a lack of readiness to see itself as a far more technical field and profession.\nMy sense is, though, that even if there was an immediate willingness by HR to embrace analytics and its ‘technical’ HR side, there would still be some immediate obstacles that would prevent uptake by HR.\n\n\n\nThere is still a problem of ‘where we start’. Most people, to adopt new methodologies, require a ‘show me’. That ‘show me’ has to be in the context that is relevant to them. In this case with HR, the context has to be HR examples. In any new developing field, there is often slow uptake on adoption of methodology because of a dearth of ‘show me’ examples.\n There are at least three essential ingredients to a ‘show me’ for HR analytics;\n\nCollect data that illustrates what we are attempting to convey\nShow how to apply data science/ predictive modelling and HR analytics methodologies to it\nShare that with the HR community.\n\nSo what’s the problem? We have oodles of HR data. Analytical methodologies are bursting on to the scene. The ingredients are there to do it, but there is little if any action.\nPart of the problem is that there is interdependency of data needed and analytic procedures. What procedures we use will dictate the data we need and the form it needs to be in. We may have ‘data’ but it may not be the data we need. And how we know which analytical technique to use? We can’t collect potentially relevant useful data for analysis, until we decide on the analysis. And we can’t decide on the proper analysis until we understand what are analytical options are. And understanding those usually requires data to work with.\nThis presents us with a bit of a dilemma- analytical procedures require data, in order for us to learn. But the relevant data being able to be gathered being dependent on the procedure being chosen. And none of this being able to be shared with HR community until we have workable HR examples. And to encourage the forward movement of analytics in HR - we need those examples.\n \n \n \n \n \n\n\n\nIn effect, the circular dependency of data and analytical examples has to be broken and a start has to occur somewhere. I think that a start can occur with an identification of a business problem that has HR implications, AND an inquiry. ‘Is this an HR business problem that HR analytics can address and help solve’? If we are courageous enough to ask the question, and if it truly requires an answer by the business we are likely to start the process of data gathering/collection and research to answer the question.\nIn fact if we truly accept some of the common definitions for workforce, HR, People Analytics\nSee https://www.linkedin.com/pulse/workforcehrpeople-analytics-hr-lyndon?trk=mp-reader-card\nwe must ask the above question. Dr. John Sullivan’s definition of People Analytics is particularly illustrative of this\n \nFrom Dr. Sullivan’s article:\nhttp://www.eremedia.com/tlnt/how-google-is-using-people-analytics-to-completely-reinvent-hr/\n“People analytics is a data-driven approach to managing people at work. Those working in people analytics strive to bring data and sophisticated analysis to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job and team design, and compensation …\n \nData driven approaches, sophisticated analyses etc. require us to think outside the box and have the spirit of inquiry regarding data and analytical methods that can be brought to bear.\nBut even if we have the data, how do we begin to get our head around what are option for sophisticated analyses?\n\n\n\n I think there are some basic building blocks of understanding that can be tremendously useful. These building blocks cover:\n\nThe Role and Understanding of Measurement\nThe Importance of Measurement Scope\nThe Purpose and Role of Statistics and Statistical Analysis to HR\nThe Role of Statistical Software, Data Science /Machine Learning/Predictive Modelling to HR Analytics\n\n \n\n\nIt begins with the understanding the definition and the role of measurement. It’s important to understand that measurement is woven into the very fiber of HR DNA whether we acknowledge this consciously or not. Let’s look at a definition of ‘measurement’\nFrom https://en.wikipedia.org/wiki/Measurement\nMeasurement is the assignment of a number to a characteristic of an object or event, which can be compared with other objects or events.\nYou could probably substitute ‘description’ for ‘characteristic’ in the above definition too.\nWhen you think of above definition, the very act or recording HR information into information systems is ‘measurement’. This is because to store the information into the system we assign numbers or descriptions/categories to it. We have employee ‘numbers’, gender, age, birthdates, names, titles, salaries etc. These are either numeric pieces of information or descriptions of things or categories.\nThat definition says nothing about the rules of measurement, or how good the measurement is. These are important concerns too, but at the most basic level, once we are storing information we are measuring. So much for the argument that HR is a non-technical field. And why do we store information and ‘measure’? - To describe and understand end explain the world around us.\nIf we embrace the reality that HR too is about measuring-we have one of the first building blocks of heading in the direction of sophisticated analyses mentioned above.\n\n\n\nOnce we understand the centrality of measurement to what we do in human resources management, it’s important to get our heads around the scope of what can be measured. Indeed, what can be measured is almost limitless. What can we, should we be paying attention to? And once we decide on what to measure, how or where does it stand compared to other things we measure? How do we see and keep track and keep sense of that we measure?\nOne possible framework, and there could be others, that could be useful is one I introduced earlier this year in the following article\nhttps://www.linkedin.com/pulse/what-does-data-driven-hr-look-like-lyndon-sundmark-mba?trk=mp-reader-card\n \nIn it, I had suggested that ‘metrics’, the things we measure, typically fall into 1 of 3 categories for HR purposes:\n\nHR Activity\n\nThese metrics describe what is going on with the people in our organization. Examples are counts of current employees, turnover counts, hire counts, turnover and hiring rates, absenteeism, accident and injury rates, benefits participation rates, training enrollments, employee churn, grievances counts and rates. If it describes what is going on with our employees and what they are doing, or what is happening to them they are in this category\n\nHR Process Efficiency and Effectiveness\n\nThese metrics concern themselves with HR business processes, the demand for them, the length of time to provide HR services within our HR business processes, customer feedback on HR Services, waiting time to receive service. They aren’t about people activity which makes it different than the category above. They are about measuring HR services or business activity.\n\nHR Methodologies\n\nThese are metrics or measures which are generated or used or could be used in the actual methodologies we use to carry out our HR functions or services. It’s when we define the goal of our function or service and try to improve on reaching the goal, have a better or more accurate outcome in the goal and changing our methodologies or processes to improve the outcome. And its measuring what we are striving for and along the way to getting there. The purpose is to do what we are doing even better. It might include measures necessary for more accurate job classification, better sourcing of candidates in recruitment, better screening of candidates, and better selection of leaders within the organization.\n\n\nIf we have a framework to see our HR measurement in, its becomes a whole lot easier to see what we haven’t done, what we have done, and what we could do with respect to bringing sophisticated analyses to bear in HR. Having a framework is a second good building block\n \n \n \n\n\n\nEven though statistics, statistical methodology and statistical software and tools have been around for decades and predate PCs, a lot of people misunderstand statistics and statistical analyses and their purpose.\nPerhaps this is because in most people’s minds and experiences, when you mention statistics or statistical analyses, the first thing than comes to mind is a boring summary table of numbers or confusing business chart, or summary data on a spreadsheet. I think data warehousing tools, unintentionally, also continue to foster this misunderstanding. We are simply given more and easier tools to slice and dice sometimes what appears to be boring business data. The predominance of introduction to charts and summary data and use of spreadsheet for this purpose in financial applications doesn’t help this either.\nIf, in the previous section, at the heart of all data collection is measurement (without mentioning what we are measuring for) - then at the heart of statistical analyses and methodology is understanding the world around us. We want to answer the question of what are we measuring for: inevitably to provide an answer or solution to a business question or problem.\nYes, at its most basic level, statistics and statistical analyses is about descriptive statistics, such as counts percentages, and averages.  But it is so much more than that. While it would take a statistics course to fully understand the typical repertoire of statistical procedures at our disposal and what they do, their purposes fall into probably just a few broad categories:\n\nDescribing what we have or are looking at. This is the purpose of descriptive statistics and charts. Much HR data analysis in organizations doesn’t go beyond this- either because the organization doesn’t demand it, or the skills are present in HR to do more. Examples here are employee counts, hiring counts, termination counts etc.\nExamining relationships in our data. How different things we are measuring are related to each other. If we have information on employee age and absenteeism rates , do our measurements show these two things to be related? If so, does absenteeism go down with age, or up? Or is there no discernable relationship?\nDetermining whether what we are seeing in our data is universal right across the organization. This could apply to both descriptive statistics and relationship. Is something universally true or does it vary across/between the categorical things we capture/measure? In the above example, does absenteeism vary by gender, or organizational level, or by job.\nDetermining whether differences we see are statistically significant. Are the differences we are seeing attributable to the different categories or could they be due to random chance?\nPredicting accurately something that we don’t or have from something we do know or have-concurrently or in the future. This is an extremely important consideration in business. To the extent to which we can predict the future based on what our past data and history tell us, allow us to pro-act and to a certain extent actively manage our future.\nUnderstanding whether what we are measuring is related to time, and if so how? There is a whole category of statistical methodologies, known as time series analysis, who purpose if to answer that question- teasing trend, seasonal, cyclical fluctuations out of our data. Stock market data is a good candidate example of this type of data and analysis.\nPredicting the best fit category for something for a new item where categories preexist for existing items. In HR, a perfect example of this is job classification. A new item (job) needs to be classified into a job level and/or job family. Other jobs are already classified to family or level. But we now have a new job that has been created, and it needs to be classified fairly as to level and family based on its similarities and differences to other jobs in the same of different classifications. By the way, statistics and statistical analyses are a natural fit for this HR function, but have been in very little organizational evidence over the decades.\nDetermining the optimal number of categories for some data where categories don’t preexist. Are their natural patterns in our HR data that help suggest where groups or categories naturally occur? In HR, sometimes salary broad banding is an example of this. We may have had previous categories, but they were no longer clearly differentiating well. So what does our data itself tell us about naturally forming groups or categories?\nDetermining the upcoming choices people will make, based on past choices. In HR an example might be open enrollment for benefits. Looking at past choices people have made, a virtual grocery cart or checkout, predict what future choices they might make on complementing benefits etc. These are normally called ‘recommender’ systems\n\nThis isn’t intended to exhaustive, but it does summarize a wide swath of purposes that statistical analyses are intended to serve. If we are ever going to reach the promise of HR analytics and sophisticated analyses we have to understand that statistics, and statistical analyses are our servant and tools in this picture. And we have to envision how we can use them. This isn’t optional.\n \n\n\n\nFrom the previous two sections, if you accept the importance that measurement and statistical analysis play in HR analytics and sophisticated analysis, then you will understand more clearly the role of data science. Machine learning, predictive modelling and data mining play. These methodologies are very much statistical analysis methodologies. But they are more than that because they attempt to more formalize a methodology around an ‘inception to completion’ analysis approach rather than just a specific type of statistical analyses or procedure.\nThese emerging methodologies all have a common denominator – finding patterns in data. The desire for looking for patterns in data come from wanting to solve or understand a problem. In the context of HR, it comes from wanting to solve a business problem or create new opportunities.\nThere are no guarantees up front, that there will be patterns in our data that are ‘business meaningful’ and relevant. But there no guarantee either that the data we collect is meaningless. We have to make the decision to look at it with these powerful methodologies and tools.\nIt would probably take several books to go into detail on these methodologies, and that isn’t the purpose of this blog article. But I will try to illustrate further their relevance, first of all with web links for books, sites, training etc., and then with some examples of how these could be used in HR. While I will share some examples, my intent is to really spur HR into further creative thinking on application of these methodologies to HR business problems.\nSome links\n-There is no particular importance attached to these as compare to many others. They just happen to be ones I am familiar with.\nData Science and Machine Learning\nI have been at the time of writing this blog article been taking a free online edX course\nhttps://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x\nThis covers Microsoft’s state of the art tool for machine learning –AzureML. They have dozens of example templates that give an idea of the possible uses on machine learning to find patterns. I will share some of these shortly – below.\n \nPredictive Modelling\nA book that I have come across in my travels for predictive modelling is Applied Predictive Modelling by Kuhn and Johnson\nhttp://www.amazon.ca/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?s=books&ie=UTF8&qid=1445460069&sr=1-1&keywords=applied+predictive+modeling\n \nIf uses the R statistical language and software and its ‘caret’ package\n \nData Mining\nhttp://www.amazon.ca/Predictive-Analytics-Data-Mining-RapidMiner/dp/0128014601/ref=sr_1_2?s=books&ie=UTF8&qid=1445460564&sr=1-2&keywords=rapidminer\n \nThis book covers Rapidminer- which is software specifically designed for data mining.\nI have used all 3 of these at one point or another. Again there are many other software packages and books out there. These are 3 I have come across.\n \nPatterns in the Data\nSo if all of these methodologies have at their ‘heart’ finding patterns in the data, why should we care about this? The primary reason is prediction- whether concurrently or for the future. Sometimes being able to predict allows us to solve a business problem (including HR related) or help us to discover new opportunities.\nTo give an idea of the possibilities, a perusal of the sample experiment templates in AzureML gives us a glimpse of the possible application of machine learning\n[http://gallery.cortanaanalytics.com/browse/?categories=$$\\“Experiment\\”$$&examples=true]{.underline}](http://gallery.cortanaanalytics.com/browse/?categories=%5b%22Experiment%22%5d&examples=true](http://gallery.cortanaanalytics.com/browse/?categories=\\[\\\"Experiment\\\"\\]&examples=true]{.underline}](http://gallery.cortanaanalytics.com/browse/?categories=%5b%22Experiment%22%5d&examples=true)\nYou may have click for ‘new’ experiment to see existing examples.\nLooking at the keyword for samples, finding patterns in the data can take the form of:\n\nClassification of items into categories\nAnomaly Detection\nClustering like with like\nFraud detection\nPredicting/forecasting – using regression\nPredicting/forecasting –using time series analysis\nPredicting/forecasting - churn/turnover\nText Classification\n\nWe need to envision how these might apply to various functions in HR\n\nClassification is a ‘natural’ for job classification- in terms on intent\nAnomaly detection- might be useful in measuring the performance/delivery of our HR services to our clients. Did any service take abnormally long to deliver that shouldn’t have? How would we know this? Anomaly detection\nClustering- might be useful for salary broad banding- creating new job families and levels based on characteristics of similar work. Clustering like with like\nPredicting/forecasting using regression – determining competitive salaries for positions that are not similar to key comparators.\nPredicting/forecasting –using time series analysis- being able to plan ahead of time for the cost of maternity leaves\nPredicting/Forecasting churn or turnover. In retail sales, business are concerned with customer churn- those that will leave them for competitors. Incentivizing the customer sometimes will prevent that. For HR, does predicting who will turnover (especially roles and people you don’t want to lose) merit the organization attention? Is it important or not?\nText Classification- suppose you had historical data on successful hires and not successful hires. Suppose you had their original resumes. Supposed there was a way to discover patterns in the text of the resumes? Suppose that those patterns were predictive of those who were successful hire and those that weren’t? Is what worth something to organizations?\nRecommending ‘like’ or ‘similar’ services to employees based on current or past choices. These would be in category of recommender systems.\n\nAs was indicated previously, there are no guarantees of patterns in the data and its predictive value. It’s either there in the data or it isn’t. But if we aren’t looking, how will we know? If we aren’t looking and our competitors are (yes HR has competition) are we at a competitive disadvantage in the HR marketplace?\nUnabashedly, my motive here is to stir HR into thinking about these technologies and their application in their ‘world’, pick up familiarity with the tools, and start examining their data. As HR experts what other areas in your HR business have application for this? Is training and development an area? What about health and safety/workers compensation?\n \n\n\n\n\nAs indicated above, the technologies are there to do sophisticated analysis in HR/People/Workforce Analytics.  And as indicated previously, lots of HR data exists. But it is the right kind of data? Is it tailored to the business questions we are asking? Are we sharing examples so that other can learn?\nI think the correct terminology here might be ‘open’ data.  I recently came across the following article on it, describing it in another context.\n \nhttp://www.zdnet.com/article/open-data-open-mind-why-you-should-share-your-company-data-with-the-world/\n \n Traditionally, and rightly so, much HR information is confidential. And it’s also rightfully protected by privacy legislation. I guess the question is, is it feasible, legal, and possible to share out HR data on an anonymous basis that doesn’t violate a person’s privacy and contravene privacy legislation? Is it possible to create HR datasets for which:\n\nOthers from the HR community can learn from and practice on to do proof of concepts for their own organizations?\nThe additional sets of eyes beyond the organization, may be beneficial?\n\n \nThis already exists in other fields of endeavor and data. An example of this is Kaggle\nhttps://en.wikipedia.org/wiki/Kaggle\n \nCould this apply to HR as well? Can we move HR Analytics ahead by sharing data and real life data science/ predictive modelling, data mining examples? Should a website be created for people to upload examples of their HR data science projects to share knowledge with others? What say you?"
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#introduction",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#introduction",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "Earlier this year I wrote a few blog articles on HR analytics. In one of those articles I indirectly expressed doubts, concerns, and readiness of the HR profession to capitalize on the promise of HR analytics and take rightful claim and ownership of it within the services that HR provides.\nhttps://www.linkedin.com/pulse/why-hr-might-able-reinvent-itself-lyndon-sundmark-mba?trk=mp-reader-card\nI mentioned a number of obstacles that might exist-historical and current- that could be standing in the way. Many of these are related to how HR views itself as a profession and a lack of readiness to see itself as a far more technical field and profession.\nMy sense is, though, that even if there was an immediate willingness by HR to embrace analytics and its ‘technical’ HR side, there would still be some immediate obstacles that would prevent uptake by HR."
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#what-is-the-hold-up",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#what-is-the-hold-up",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "There is still a problem of ‘where we start’. Most people, to adopt new methodologies, require a ‘show me’. That ‘show me’ has to be in the context that is relevant to them. In this case with HR, the context has to be HR examples. In any new developing field, there is often slow uptake on adoption of methodology because of a dearth of ‘show me’ examples.\n There are at least three essential ingredients to a ‘show me’ for HR analytics;\n\nCollect data that illustrates what we are attempting to convey\nShow how to apply data science/ predictive modelling and HR analytics methodologies to it\nShare that with the HR community.\n\nSo what’s the problem? We have oodles of HR data. Analytical methodologies are bursting on to the scene. The ingredients are there to do it, but there is little if any action.\nPart of the problem is that there is interdependency of data needed and analytic procedures. What procedures we use will dictate the data we need and the form it needs to be in. We may have ‘data’ but it may not be the data we need. And how we know which analytical technique to use? We can’t collect potentially relevant useful data for analysis, until we decide on the analysis. And we can’t decide on the proper analysis until we understand what are analytical options are. And understanding those usually requires data to work with.\nThis presents us with a bit of a dilemma- analytical procedures require data, in order for us to learn. But the relevant data being able to be gathered being dependent on the procedure being chosen. And none of this being able to be shared with HR community until we have workable HR examples. And to encourage the forward movement of analytics in HR - we need those examples."
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#resolving-the-dilemma",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#resolving-the-dilemma",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "In effect, the circular dependency of data and analytical examples has to be broken and a start has to occur somewhere. I think that a start can occur with an identification of a business problem that has HR implications, AND an inquiry. ‘Is this an HR business problem that HR analytics can address and help solve’? If we are courageous enough to ask the question, and if it truly requires an answer by the business we are likely to start the process of data gathering/collection and research to answer the question.\nIn fact if we truly accept some of the common definitions for workforce, HR, People Analytics\nSee https://www.linkedin.com/pulse/workforcehrpeople-analytics-hr-lyndon?trk=mp-reader-card\nwe must ask the above question. Dr. John Sullivan’s definition of People Analytics is particularly illustrative of this\n \nFrom Dr. Sullivan’s article:\nhttp://www.eremedia.com/tlnt/how-google-is-using-people-analytics-to-completely-reinvent-hr/\n“People analytics is a data-driven approach to managing people at work. Those working in people analytics strive to bring data and sophisticated analysis to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job and team design, and compensation …\n \nData driven approaches, sophisticated analyses etc. require us to think outside the box and have the spirit of inquiry regarding data and analytical methods that can be brought to bear.\nBut even if we have the data, how do we begin to get our head around what are option for sophisticated analyses?"
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#the-basics",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#the-basics",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "I think there are some basic building blocks of understanding that can be tremendously useful. These building blocks cover:\n\nThe Role and Understanding of Measurement\nThe Importance of Measurement Scope\nThe Purpose and Role of Statistics and Statistical Analysis to HR\nThe Role of Statistical Software, Data Science /Machine Learning/Predictive Modelling to HR Analytics\n\n \n\n\nIt begins with the understanding the definition and the role of measurement. It’s important to understand that measurement is woven into the very fiber of HR DNA whether we acknowledge this consciously or not. Let’s look at a definition of ‘measurement’\nFrom https://en.wikipedia.org/wiki/Measurement\nMeasurement is the assignment of a number to a characteristic of an object or event, which can be compared with other objects or events.\nYou could probably substitute ‘description’ for ‘characteristic’ in the above definition too.\nWhen you think of above definition, the very act or recording HR information into information systems is ‘measurement’. This is because to store the information into the system we assign numbers or descriptions/categories to it. We have employee ‘numbers’, gender, age, birthdates, names, titles, salaries etc. These are either numeric pieces of information or descriptions of things or categories.\nThat definition says nothing about the rules of measurement, or how good the measurement is. These are important concerns too, but at the most basic level, once we are storing information we are measuring. So much for the argument that HR is a non-technical field. And why do we store information and ‘measure’? - To describe and understand end explain the world around us.\nIf we embrace the reality that HR too is about measuring-we have one of the first building blocks of heading in the direction of sophisticated analyses mentioned above.\n\n\n\nOnce we understand the centrality of measurement to what we do in human resources management, it’s important to get our heads around the scope of what can be measured. Indeed, what can be measured is almost limitless. What can we, should we be paying attention to? And once we decide on what to measure, how or where does it stand compared to other things we measure? How do we see and keep track and keep sense of that we measure?\nOne possible framework, and there could be others, that could be useful is one I introduced earlier this year in the following article\nhttps://www.linkedin.com/pulse/what-does-data-driven-hr-look-like-lyndon-sundmark-mba?trk=mp-reader-card\n \nIn it, I had suggested that ‘metrics’, the things we measure, typically fall into 1 of 3 categories for HR purposes:\n\nHR Activity\n\nThese metrics describe what is going on with the people in our organization. Examples are counts of current employees, turnover counts, hire counts, turnover and hiring rates, absenteeism, accident and injury rates, benefits participation rates, training enrollments, employee churn, grievances counts and rates. If it describes what is going on with our employees and what they are doing, or what is happening to them they are in this category\n\nHR Process Efficiency and Effectiveness\n\nThese metrics concern themselves with HR business processes, the demand for them, the length of time to provide HR services within our HR business processes, customer feedback on HR Services, waiting time to receive service. They aren’t about people activity which makes it different than the category above. They are about measuring HR services or business activity.\n\nHR Methodologies\n\nThese are metrics or measures which are generated or used or could be used in the actual methodologies we use to carry out our HR functions or services. It’s when we define the goal of our function or service and try to improve on reaching the goal, have a better or more accurate outcome in the goal and changing our methodologies or processes to improve the outcome. And its measuring what we are striving for and along the way to getting there. The purpose is to do what we are doing even better. It might include measures necessary for more accurate job classification, better sourcing of candidates in recruitment, better screening of candidates, and better selection of leaders within the organization.\n\n\nIf we have a framework to see our HR measurement in, its becomes a whole lot easier to see what we haven’t done, what we have done, and what we could do with respect to bringing sophisticated analyses to bear in HR. Having a framework is a second good building block\n \n \n \n\n\n\nEven though statistics, statistical methodology and statistical software and tools have been around for decades and predate PCs, a lot of people misunderstand statistics and statistical analyses and their purpose.\nPerhaps this is because in most people’s minds and experiences, when you mention statistics or statistical analyses, the first thing than comes to mind is a boring summary table of numbers or confusing business chart, or summary data on a spreadsheet. I think data warehousing tools, unintentionally, also continue to foster this misunderstanding. We are simply given more and easier tools to slice and dice sometimes what appears to be boring business data. The predominance of introduction to charts and summary data and use of spreadsheet for this purpose in financial applications doesn’t help this either.\nIf, in the previous section, at the heart of all data collection is measurement (without mentioning what we are measuring for) - then at the heart of statistical analyses and methodology is understanding the world around us. We want to answer the question of what are we measuring for: inevitably to provide an answer or solution to a business question or problem.\nYes, at its most basic level, statistics and statistical analyses is about descriptive statistics, such as counts percentages, and averages.  But it is so much more than that. While it would take a statistics course to fully understand the typical repertoire of statistical procedures at our disposal and what they do, their purposes fall into probably just a few broad categories:\n\nDescribing what we have or are looking at. This is the purpose of descriptive statistics and charts. Much HR data analysis in organizations doesn’t go beyond this- either because the organization doesn’t demand it, or the skills are present in HR to do more. Examples here are employee counts, hiring counts, termination counts etc.\nExamining relationships in our data. How different things we are measuring are related to each other. If we have information on employee age and absenteeism rates , do our measurements show these two things to be related? If so, does absenteeism go down with age, or up? Or is there no discernable relationship?\nDetermining whether what we are seeing in our data is universal right across the organization. This could apply to both descriptive statistics and relationship. Is something universally true or does it vary across/between the categorical things we capture/measure? In the above example, does absenteeism vary by gender, or organizational level, or by job.\nDetermining whether differences we see are statistically significant. Are the differences we are seeing attributable to the different categories or could they be due to random chance?\nPredicting accurately something that we don’t or have from something we do know or have-concurrently or in the future. This is an extremely important consideration in business. To the extent to which we can predict the future based on what our past data and history tell us, allow us to pro-act and to a certain extent actively manage our future.\nUnderstanding whether what we are measuring is related to time, and if so how? There is a whole category of statistical methodologies, known as time series analysis, who purpose if to answer that question- teasing trend, seasonal, cyclical fluctuations out of our data. Stock market data is a good candidate example of this type of data and analysis.\nPredicting the best fit category for something for a new item where categories preexist for existing items. In HR, a perfect example of this is job classification. A new item (job) needs to be classified into a job level and/or job family. Other jobs are already classified to family or level. But we now have a new job that has been created, and it needs to be classified fairly as to level and family based on its similarities and differences to other jobs in the same of different classifications. By the way, statistics and statistical analyses are a natural fit for this HR function, but have been in very little organizational evidence over the decades.\nDetermining the optimal number of categories for some data where categories don’t preexist. Are their natural patterns in our HR data that help suggest where groups or categories naturally occur? In HR, sometimes salary broad banding is an example of this. We may have had previous categories, but they were no longer clearly differentiating well. So what does our data itself tell us about naturally forming groups or categories?\nDetermining the upcoming choices people will make, based on past choices. In HR an example might be open enrollment for benefits. Looking at past choices people have made, a virtual grocery cart or checkout, predict what future choices they might make on complementing benefits etc. These are normally called ‘recommender’ systems\n\nThis isn’t intended to exhaustive, but it does summarize a wide swath of purposes that statistical analyses are intended to serve. If we are ever going to reach the promise of HR analytics and sophisticated analyses we have to understand that statistics, and statistical analyses are our servant and tools in this picture. And we have to envision how we can use them. This isn’t optional.\n \n\n\n\nFrom the previous two sections, if you accept the importance that measurement and statistical analysis play in HR analytics and sophisticated analysis, then you will understand more clearly the role of data science. Machine learning, predictive modelling and data mining play. These methodologies are very much statistical analysis methodologies. But they are more than that because they attempt to more formalize a methodology around an ‘inception to completion’ analysis approach rather than just a specific type of statistical analyses or procedure.\nThese emerging methodologies all have a common denominator – finding patterns in data. The desire for looking for patterns in data come from wanting to solve or understand a problem. In the context of HR, it comes from wanting to solve a business problem or create new opportunities.\nThere are no guarantees up front, that there will be patterns in our data that are ‘business meaningful’ and relevant. But there no guarantee either that the data we collect is meaningless. We have to make the decision to look at it with these powerful methodologies and tools.\nIt would probably take several books to go into detail on these methodologies, and that isn’t the purpose of this blog article. But I will try to illustrate further their relevance, first of all with web links for books, sites, training etc., and then with some examples of how these could be used in HR. While I will share some examples, my intent is to really spur HR into further creative thinking on application of these methodologies to HR business problems.\nSome links\n-There is no particular importance attached to these as compare to many others. They just happen to be ones I am familiar with.\nData Science and Machine Learning\nI have been at the time of writing this blog article been taking a free online edX course\nhttps://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x\nThis covers Microsoft’s state of the art tool for machine learning –AzureML. They have dozens of example templates that give an idea of the possible uses on machine learning to find patterns. I will share some of these shortly – below.\n \nPredictive Modelling\nA book that I have come across in my travels for predictive modelling is Applied Predictive Modelling by Kuhn and Johnson\nhttp://www.amazon.ca/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?s=books&ie=UTF8&qid=1445460069&sr=1-1&keywords=applied+predictive+modeling\n \nIf uses the R statistical language and software and its ‘caret’ package\n \nData Mining\nhttp://www.amazon.ca/Predictive-Analytics-Data-Mining-RapidMiner/dp/0128014601/ref=sr_1_2?s=books&ie=UTF8&qid=1445460564&sr=1-2&keywords=rapidminer\n \nThis book covers Rapidminer- which is software specifically designed for data mining.\nI have used all 3 of these at one point or another. Again there are many other software packages and books out there. These are 3 I have come across.\n \nPatterns in the Data\nSo if all of these methodologies have at their ‘heart’ finding patterns in the data, why should we care about this? The primary reason is prediction- whether concurrently or for the future. Sometimes being able to predict allows us to solve a business problem (including HR related) or help us to discover new opportunities.\nTo give an idea of the possibilities, a perusal of the sample experiment templates in AzureML gives us a glimpse of the possible application of machine learning\n[http://gallery.cortanaanalytics.com/browse/?categories=$$\\“Experiment\\”$$&examples=true]{.underline}](http://gallery.cortanaanalytics.com/browse/?categories=%5b%22Experiment%22%5d&examples=true](http://gallery.cortanaanalytics.com/browse/?categories=\\[\\\"Experiment\\\"\\]&examples=true]{.underline}](http://gallery.cortanaanalytics.com/browse/?categories=%5b%22Experiment%22%5d&examples=true)\nYou may have click for ‘new’ experiment to see existing examples.\nLooking at the keyword for samples, finding patterns in the data can take the form of:\n\nClassification of items into categories\nAnomaly Detection\nClustering like with like\nFraud detection\nPredicting/forecasting – using regression\nPredicting/forecasting –using time series analysis\nPredicting/forecasting - churn/turnover\nText Classification\n\nWe need to envision how these might apply to various functions in HR\n\nClassification is a ‘natural’ for job classification- in terms on intent\nAnomaly detection- might be useful in measuring the performance/delivery of our HR services to our clients. Did any service take abnormally long to deliver that shouldn’t have? How would we know this? Anomaly detection\nClustering- might be useful for salary broad banding- creating new job families and levels based on characteristics of similar work. Clustering like with like\nPredicting/forecasting using regression – determining competitive salaries for positions that are not similar to key comparators.\nPredicting/forecasting –using time series analysis- being able to plan ahead of time for the cost of maternity leaves\nPredicting/Forecasting churn or turnover. In retail sales, business are concerned with customer churn- those that will leave them for competitors. Incentivizing the customer sometimes will prevent that. For HR, does predicting who will turnover (especially roles and people you don’t want to lose) merit the organization attention? Is it important or not?\nText Classification- suppose you had historical data on successful hires and not successful hires. Suppose you had their original resumes. Supposed there was a way to discover patterns in the text of the resumes? Suppose that those patterns were predictive of those who were successful hire and those that weren’t? Is what worth something to organizations?\nRecommending ‘like’ or ‘similar’ services to employees based on current or past choices. These would be in category of recommender systems.\n\nAs was indicated previously, there are no guarantees of patterns in the data and its predictive value. It’s either there in the data or it isn’t. But if we aren’t looking, how will we know? If we aren’t looking and our competitors are (yes HR has competition) are we at a competitive disadvantage in the HR marketplace?\nUnabashedly, my motive here is to stir HR into thinking about these technologies and their application in their ‘world’, pick up familiarity with the tools, and start examining their data. As HR experts what other areas in your HR business have application for this? Is training and development an area? What about health and safety/workers compensation?"
  },
  {
    "objectID": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#the-importance-of-sharing-examples",
    "href": "posts/HRWhatsStoppingUS/HRANalyticsWhatsStopppingUs.html#the-importance-of-sharing-examples",
    "title": "HR Analytics – What Stopping Us And Where Do Go From Here?",
    "section": "",
    "text": "As indicated above, the technologies are there to do sophisticated analysis in HR/People/Workforce Analytics.  And as indicated previously, lots of HR data exists. But it is the right kind of data? Is it tailored to the business questions we are asking? Are we sharing examples so that other can learn?\nI think the correct terminology here might be ‘open’ data.  I recently came across the following article on it, describing it in another context.\n \nhttp://www.zdnet.com/article/open-data-open-mind-why-you-should-share-your-company-data-with-the-world/\n \n Traditionally, and rightly so, much HR information is confidential. And it’s also rightfully protected by privacy legislation. I guess the question is, is it feasible, legal, and possible to share out HR data on an anonymous basis that doesn’t violate a person’s privacy and contravene privacy legislation? Is it possible to create HR datasets for which:\n\nOthers from the HR community can learn from and practice on to do proof of concepts for their own organizations?\nThe additional sets of eyes beyond the organization, may be beneficial?\n\n \nThis already exists in other fields of endeavor and data. An example of this is Kaggle\nhttps://en.wikipedia.org/wiki/Kaggle\n \nCould this apply to HR as well? Can we move HR Analytics ahead by sharing data and real life data science/ predictive modelling, data mining examples? Should a website be created for people to upload examples of their HR data science projects to share knowledge with others? What say you?"
  },
  {
    "objectID": "posts/PAExampleUsingR/PAExampleUsingR.html",
    "href": "posts/PAExampleUsingR/PAExampleUsingR.html",
    "title": "People Analytics - An Example Using R",
    "section": "",
    "text": "Over last 18 months or so I have been writing LinkedIn blog articles on analytics and their potential use for HR. Most of these articles have hinted that there are many obstacles to the application of analytics to HR.\nThese obstacles for widespread evidence and use are predominantly human not technical:\n\nData generation is not the problem. HR data availability and accessability has never been greater to HR professionals.\nTechnology is not problem. Data science, machine learning, and analytic tools are exploding on to the scene these days. Whether it is enhancements to the R statistical package, or other statistical packages, or developments in other tools like Microsoft Azure Machine Learning or RapidMiner as examples- there have never been more tools to be brought to bear on HR data.\nApplication of the tools on the data IS the problem. (In this case lack thereof).\n\nThis lack of application can occur from:\n\nThe HR profession not seeing that most of HR is highly technical. In the past HR might have acknowleged the traditional areas such as salary and wage administration, or labor relations/collective bargaining costing was technical because it was seen as heavily dependent on calculation and data. But for most of the rest of the HR domain HR professionals think its non-technical- whether it be recruitment,training, Health and Safety, Benefits, engagement. To be sure, there are the ‘human’ sides of all areas of HR, but they are all ’as much technical as well. The technical has far too long being ignored by HR professionals.\nDr. John Sullivan, in the following article:\nhttp://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\ndescribes this as ‘reliance on relationships’. ‘Relationships are the antithesis of analytical decsions making.’ .\nNot enough HR professionals seeking the informational and analytical side of the picture in their university/college HR studies in preparation for the HR field, and not enough universities/colleges offering studies in People Analytics.\nPeople Analytics- where it exists- being defining too narrowly as just HR metrics or predictive analytic tools to the HR domain. These are part of the picture-yes- but ‘data driven’ can include much more of the statistical methods than just predictive.\nObstacles that organizations inadvertently put in the way - the ‘sacred’ requirement to develop a business case to ‘get’ resources to do any of this- particularly when ‘starting’ can be relatively free to develop proof of concepts.\n\nI am a firm believer in ‘skunkworks’ projects-making do with, and showing what can be done with, what you already have or with what can be obtained free. You will often need to show by simple example what can be done to justify more elaborate formal initiatives later and request additional formal resources.\nIts in that spirit, I wanted to walk through an example of People Analytics, and using free tools. This wont be a full fledged best practices or even a robust example. Rather it is intended to be a simple example to illustrate the process and some of the tools. Best practices and robustness comes with learning, use, and mastery over a period of time. Most of us need to start ‘simply’ to start ‘somewhere’.\nThis example will be on one particular HR metric- absenteeism. The data will be contrived. But it will illustrate very rudimentary People Analytics using the R statistical programming language.\nBut before we get into this though, we should cover some basic helpful definitions and frameworks. These will help show why we are doing what we are doing.\n#Terminology\n##People Analytics First of all, I do prefer the term ‘People Analytics’ as compared to some of the other terms used interchangeably. Per the title, I am going to use the term ‘People Analytics’ the way its implicitly defined in the previous article link provided\nPeople analytics is a data-driven approach to managing people at work.\n‘Data-driven’ is key. It dispels any notion that data and measurement are not part of managing people. (Sorry to those of you who felt managing people was restricted only to ‘reliance on relationships’)\nPeople Analytics is what happens when you apply Data Science and its principles to the realm of People Management (HR).\nIt means ‘analysis of data’ then ‘action’. And it means analysis and action. You are managing people through being ‘data driven’ A lot of organizations who try to get people analytics started, get stuck at creation of HR metrics and the use of Business Intelligence tools. These can be and are part of the people analytics picture, to be sure. But slicing and dicing , graphics and visualizations by themselves only take you so far. The addition of statistical analyses and taking informed action on your data are what will propel you forward.\nAnother reason why I like this definition is that ‘data-driven’ doesn’t unintentionally restrict the type of analyses we do to be data driven. This can include exploratory analysis, predictive analysis, and experimental design. (Often people think only ‘predictive’ in the context of ‘data driven’.)\nBecause we mention ‘Data Science’ in the context of People Analytics, it is important to define it next to understand why it is so tied to People Analytics.\n##Data Science I will share a few definitions.\n\nIn their book Practical Data Science With R by Nina Zumel and John Mount https://www.manning.com/books/practical-data-science-with-r on page xix, they define data science as : ‘. managing the process that can transform hypotheses and data into actionable predictions.’\nAnother definition is from the ‘Field Guide to Data Science’ by Booz, Allen , Hamilton- page 21: http://www.boozallen.com/insights/2015/12/data-science-field-guide-second-edition They define data science as : ‘the art of turning data in actions’\nAnd still another definition from ‘Data Science For Business’ by Foster Provost And Tom Fawcett: http://shop.oreilly.com/product/0636920028918.do Data Science is a set of fundamental principles that guide the extraction of knowledge from data (page 2) . The ultimate goal of data science .improving decision making. (page 5)\n\nAll of these definitions clearly line up as being totally consistent with the above definition provided for People Analytics. It means transforming HR hunches, guesses (or really hypotheses) into information/data , analyses, and management actions- actions supported by the data.\n#A Framework\nZumel/Mount ‘s definition mentions ’process’. Processes are always require to ‘transform’ something from ‘what it is’ to ‘what it is to become’ - ‘data’ into ‘actions’. This becomes a framework that can guide our efforts and understanding. In their book ‘Practical Data Science With R’ they define the following process for data science on page 6:\nData Science\n\nDefine a goal\nCollect and Manage Data\nBuild The Model\nEvaluate and Critique Model\nPresent Results and Document\nDeploy Model\n\nI don’t want to belabor the above process significantly in this blog article, because entire books have and are being written on the subjects of data science, predictive analytics, data mining etc. But I will make some general comments about the above steps to set the stage for the illustrative, simple ,rudimentary R example to follow.\n\nDefine a goal ,as mentioned above, means identifying first what HR management business problem you are trying to solve. Without a problem/issue we don’t have a goal.\nCollect and Manage data. At its simplest, you want a ‘dataset’ of information perceived to be relevant to the problem. The collection and management of data could be a simple extract from the corporate Human Resource Information System, or an output from an elaborate Data Warehousing/Business Intelligence tool used on HR information. For purpose of this blog article illustration we will use a simple CSV file. It also involves exploring the data both for data quality issues, and for an initial look at what the data may be telling you\nBuild The Model. This step really means, after you have defined the HR business problem or goal you are trying to achieve, you pick a data mining approach/tool that is designed to address that type of problem. With absenteeism as an HR issue, are you trying to predict employee with propensity to high absenteeism from those who aren’t? Are you trying to predict future absenteeism rates? Are you trying to define what is normal absenteeism from that which is atypical or and anomaly? The business problem/goal determine the appropriate data mining tools to consider. Not exhaustive as a list, but common data mining approaches used in modelling are classification,regression, anomaly detection, time series, clustering, association analyses to name a few. These approaches take information/data as inputs , run them through statistical algorithms, and produce output.\nEvaluate and Critique Model. Each data mining approach can have many different statistical algorithms to bring to bear on the data. The evaluation is both what algorithms provide the most consistent accurate predictions on new data, and do we have all the relevant data or do we need more types of data to increase predictive accuracy of model on new data. This can be necessarily repetitive and circular activity over time to improve the model\nPresent Results And Document. When we have gotten out model to an acceptable ,useful predictive level, we document our activity and present results. The definition of acceptable and useful is really relative to the organization, but in all cases would mean , results show improvement over what would have been otherwise. The principle behind data ‘science’ like any science, is that with the same data, people should be able to reproduce our findings/ results.\nDeploy Model. The whole purpose of building the model ( which is on existing data) is to:\n\n\nuse the model on future data when it becomes available, to predict or prevent something from happening before it occurs or\nto better understand our existing business problem to tailor more specific responses\n\nBoth R and other solutions allow you to save the model, so it can be used on other data. Lets now turn to a rudimentary People Analytics (Data driven People Management(HR) example in R.\nThis blog article is ‘Part 1’. The length of covering this in a single article would push the limits of comfortable reading length for a single article. So part 1 will cover the first 2 steps of the above process.\n#An R Example\n##1.Define the goal (or HR business problem/issue).\nA hypothetical company MFG has decided that it needs to at absenteeism. It wants answers to the following questions:\n\nWhat is its rate of absenteeism?\nDoes anyone have excessive absenteeism?\nIs is the same across the organization?\nDoes it vary by gender?\nDoes it vary by length of service or age? Its guesses are that initially age and length of service may be related to absenteeism rates.\nCan it predict next year’s absenteeism?\nIf so, how well can it predict?\nCan we reduce our absenteeism?\n\nIf they can make future People Management decisions “driven” by what the data is telling them, then they will feel they have started the People Analytics journey.\n##2.Collect and Manage Data.\nLet us suppose this is a skunkworks project. Formal separate resources have not be identified for this initiative. Only an initial look at recent data is possible. The HRIS system is able to provide some rudimentary information covering absences only for 2015 It was able to generate the following information as a CSV file (comma separated values):\n\nEmployeeNumber\nSurname\nGivenName\nGender\nCity\nJobTitle\nDepartmentName\nStoreLocation\nDivision\nAge\nLengthService\nAbsentHours\nBusinessUnit\n\n###Let’s read in the data provided\n\n# MFGEmployees &lt;- read.csv(\"~/R Files/MFGEmployees4.csv\")\nMFGEmployees &lt;- read.csv(\"MFGEmployees4.csv\")\nstr(MFGEmployees,width=80,strict.width =\"wrap\")\n\n'data.frame':   8336 obs. of  13 variables:\n$ EmployeeNumber: int 1 2 3 4 5 6 7 8 9 10 ...\n$ Surname : chr \"Gutierrez\" \"Hardwick\" \"Delgado\" \"Simon\" ...\n$ GivenName : chr \"Molly\" \"Stephen\" \"Chester\" \"Irene\" ...\n$ Gender : chr \"F\" \"M\" \"M\" \"F\" ...\n$ City : chr \"Burnaby\" \"Courtenay\" \"Richmond\" \"Victoria\" ...\n$ JobTitle : chr \"Baker\" \"Baker\" \"Baker\" \"Baker\" ...\n$ DepartmentName: chr \"Bakery\" \"Bakery\" \"Bakery\" \"Bakery\" ...\n$ StoreLocation : chr \"Burnaby\" \"Nanaimo\" \"Richmond\" \"Victoria\" ...\n$ Division : chr \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n$ Age : num 32 40.3 48.8 44.6 35.7 ...\n$ LengthService : num 6.02 5.53 4.39 3.08 3.62 ...\n$ AbsentHours : num 36.6 30.2 83.8 70 0 ...\n$ BusinessUnit : chr \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n\n\nThe first thing we should do is check on quality of data. Data will rarely be clean or perfect when we receive it. Either questionnable data should be corrected(preferred) or deleted.\n\nsummary(MFGEmployees)\n\n EmployeeNumber   Surname           GivenName            Gender         \n Min.   :   1   Length:8336        Length:8336        Length:8336       \n 1st Qu.:2085   Class :character   Class :character   Class :character  \n Median :4168   Mode  :character   Mode  :character   Mode  :character  \n Mean   :4168                                                           \n 3rd Qu.:6252                                                           \n Max.   :8336                                                           \n     City             JobTitle         DepartmentName     StoreLocation     \n Length:8336        Length:8336        Length:8336        Length:8336       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Division              Age         LengthService      AbsentHours    \n Length:8336        Min.   : 3.505   Min.   : 0.0121   Min.   :  0.00  \n Class :character   1st Qu.:35.299   1st Qu.: 3.5759   1st Qu.: 19.13  \n Mode  :character   Median :42.115   Median : 4.6002   Median : 56.01  \n                    Mean   :42.007   Mean   : 4.7829   Mean   : 61.28  \n                    3rd Qu.:48.667   3rd Qu.: 5.6239   3rd Qu.: 94.28  \n                    Max.   :77.938   Max.   :43.7352   Max.   :272.53  \n BusinessUnit      \n Length:8336       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nThe only thing that stands out initially is that age has some questionable data- some one who is 3 and someone who is 77. The range for purposes of this example should be 18 to 65 .Normally you would want to clean the data by getting the correct information and then changing it. For expediency of the example we will delete the problem records\n###Clean the data\n\nMFGEmployees&lt;-subset(MFGEmployees,MFGEmployees$Age&gt;=18)\nMFGEmployees&lt;-subset(MFGEmployees,MFGEmployees$Age&lt;=65)\n\nNow lets summarize again with cleaned up data\n\nsummary(MFGEmployees)\n\n EmployeeNumber   Surname           GivenName            Gender         \n Min.   :   1   Length:8165        Length:8165        Length:8165       \n 1st Qu.:2081   Class :character   Class :character   Class :character  \n Median :4166   Mode  :character   Mode  :character   Mode  :character  \n Mean   :4165                                                           \n 3rd Qu.:6245                                                           \n Max.   :8336                                                           \n     City             JobTitle         DepartmentName     StoreLocation     \n Length:8165        Length:8165        Length:8165        Length:8165       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Division              Age        LengthService       AbsentHours    \n Length:8165        Min.   :18.20   Min.   : 0.05328   Min.   :  0.00  \n Class :character   1st Qu.:35.46   1st Qu.: 3.58261   1st Qu.: 20.07  \n Mode  :character   Median :42.10   Median : 4.59800   Median : 55.86  \n                    Mean   :41.99   Mean   : 4.78887   Mean   : 60.47  \n                    3rd Qu.:48.51   3rd Qu.: 5.62358   3rd Qu.: 93.38  \n                    Max.   :65.00   Max.   :43.73524   Max.   :252.19  \n BusinessUnit      \n Length:8165       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n###Transform the data: Now calculate absenteeism rate by dividing the absent hours by total standard hours for the year (52 weeks* 40 hours =2080)\n\nMFGEmployees$AbsenceRate&lt;-MFGEmployees$AbsentHours/2080*100\nstr(MFGEmployees)\n\n'data.frame':   8165 obs. of  14 variables:\n $ EmployeeNumber: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Surname       : chr  \"Gutierrez\" \"Hardwick\" \"Delgado\" \"Simon\" ...\n $ GivenName     : chr  \"Molly\" \"Stephen\" \"Chester\" \"Irene\" ...\n $ Gender        : chr  \"F\" \"M\" \"M\" \"F\" ...\n $ City          : chr  \"Burnaby\" \"Courtenay\" \"Richmond\" \"Victoria\" ...\n $ JobTitle      : chr  \"Baker\" \"Baker\" \"Baker\" \"Baker\" ...\n $ DepartmentName: chr  \"Bakery\" \"Bakery\" \"Bakery\" \"Bakery\" ...\n $ StoreLocation : chr  \"Burnaby\" \"Nanaimo\" \"Richmond\" \"Victoria\" ...\n $ Division      : chr  \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n $ Age           : num  32 40.3 48.8 44.6 35.7 ...\n $ LengthService : num  6.02 5.53 4.39 3.08 3.62 ...\n $ AbsentHours   : num  36.6 30.2 83.8 70 0 ...\n $ BusinessUnit  : chr  \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n $ AbsenceRate   : num  1.76 1.45 4.03 3.37 0 ...\n\n\nWe can now see our metric AbsenceRate has been calculated and created.\n###Explore The Data Part of collecting and managing data is ‘Exploratory’ Analysis.\nLets start with bar graphs of some of the categorical data\n\ncounts &lt;- table(MFGEmployees$BusinessUnit)\nbarplot(counts, main = \"EmployeeCount By Business Units\", horiz = TRUE)\n\n\n\ncounts &lt;- table(MFGEmployees$Gender)\nbarplot(counts, main = \"EmployeeCount By Gender\", horiz = TRUE)\n\n\n\ncounts &lt;- table(MFGEmployees$Division)\nbarplot(counts, main = \"EmployeeCount By Division\", horiz = TRUE)\n\n\n\n\nLets ask some of our questions answered through this exploratory analysis.\nFirst of all, what is our absenteeism rate?\n\nmean(MFGEmployees$AbsenceRate)\n\n[1] 2.907265\n\nlibrary(ggplot2)\nggplot() + geom_boxplot(aes(y = AbsenceRate, x =1), data = MFGEmployees) + coord_flip()\n\n\n\n\nThe absence rate is 2.9.\nDoes anyone have excessive absenteeism?\nThe boxplot shows the mean and standard deviation of the data. Any observations beyond 3 standard deviations shows up as dots. So at least under that definition of outliers, some people show way more absenteeism than 99% of employees\nDoes it vary across the organization?\n\nlibrary(ggplot2)\n#library(RcmdrMisc)\nggplot() + geom_boxplot(aes(y = AbsenceRate, x = Gender), data = MFGEmployees) + coord_flip()\n\n\n\nAnovaModel.1 &lt;-aov(AbsenceRate ~ Gender, data=MFGEmployees)\nsummary(AnovaModel.1)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nGender         1    496   495.6   97.77 &lt;2e-16 ***\nResiduals   8163  41379     5.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naggregate(MFGEmployees$AbsenceRate, list(MFGEmployees$Gender), FUN=mean)\n\n  Group.1        x\n1       F 3.157624\n2       M 2.664813\n\n\nIt varies significantly by Gender.\n\nggplot() + geom_boxplot(aes(y = AbsenceRate, x = Division), data = MFGEmployees) + coord_flip()\n\n\n\nAnovaModel.2 &lt;-aov(AbsenceRate ~ Division, data=MFGEmployees)\nsummary(AnovaModel.2)\n\n              Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDivision       5     91  18.240   3.562 0.00322 **\nResiduals   8159  41783   5.121                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naggregate(MFGEmployees$AbsenceRate, list(MFGEmployees$Division), FUN=mean)\n\n               Group.1        x\n1            Executive 2.323580\n2 FinanceAndAccounting 1.921890\n3       HumanResources 2.651743\n4             InfoTech 1.925995\n5                Legal 2.471724\n6               Stores 2.920856\n\n\nIt varies significantly by Division.\n\nAnovaModel.3 &lt;-aov(AbsenceRate ~Division*Gender, data=MFGEmployees)\nsummary(AnovaModel.3)\n\n                  Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nDivision           5     91    18.2   3.602 0.00295 ** \nGender             1    496   495.9  97.942 &lt; 2e-16 ***\nDivision:Gender    5      5     0.9   0.178 0.97078    \nResiduals       8153  41283     5.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naggregate(MFGEmployees$AbsenceRate, list(MFGEmployees$Division,MFGEmployees$Gender), FUN=mean)\n\n                Group.1 Group.2        x\n1             Executive       F 2.976419\n2  FinanceAndAccounting       F 2.172804\n3        HumanResources       F 3.014491\n4              InfoTech       F 3.298112\n5                 Legal       F 3.298112\n6                Stores       F 3.169049\n7             Executive       M 1.779546\n8  FinanceAndAccounting       M 1.634077\n9        HumanResources       M 2.214311\n10             InfoTech       M 1.773538\n11                Legal       M 2.058530\n12               Stores       M 2.680788\n\n\nIf varies significantly by the interaction of gender and division.\nThese are just a handful of the categorical summaries we could do.\nDoes AbsenceRate vary by length of service and age?\nScatterplots and correlations help answer this.\n\nlibrary(ggplot2)\n\nggplot() + geom_point(aes(x = Age,y = AbsenceRate),data=MFGEmployees) + \n  geom_smooth(aes(x = Age,y = AbsenceRate),data=MFGEmployees,method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\ncor(MFGEmployees$Age, MFGEmployees$AbsenceRate)\n\n[1] 0.8246129\n\n\nThere is a strong correlation of Age and Absence Rate\n\nlibrary(ggplot2)\n\nggplot() + geom_point(aes(x = LengthService,y = AbsenceRate),data=MFGEmployees) +\n  geom_smooth(aes(x = LengthService,y = AbsenceRate),data=MFGEmployees,method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\ncor(MFGEmployees$LengthService, MFGEmployees$AbsenceRate)\n\n[1] -0.04669242\n\n\nThere is not a strong correlation between length of service and Absence Rate.\n\nggplot() + geom_point(aes(x = LengthService,y = Age),data=MFGEmployees) + \n  geom_smooth(aes(x = LengthService,y = Age),data=MFGEmployees,method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\ncor(MFGEmployees$Age, MFGEmployees$LengthService)\n\n[1] 0.05623405\n\n\nThere is not much correlation between age and length of service either.\nThis is as far as we will go in this article. We will defer the rest of the analyses to the part 2 blog article. In Part 1 of this blog article\nwe:\n\nIndicated that People Analytics is something that organizations can do themselves\nIndicated that it can be done in the R programming language as at least one possible tool\nProvided some basic terminology for People Analytics and Data Science.\nProvided a sample suggested framework for Data Science.\nCovered the first 2 steps of that framework for a People Analytics example- defining the problem and collecting and managing data.\n\nLet us know move on to step 3.\nReload the data and adjust it for corrections again\n\nMFGEmployees &lt;- read.csv(\"MFGEmployees4.csv\")\n#write.csv(MFGEmployees,\"~/R Files/MFGEmployees4.csv\")\nMFGEmployees$AbsenceRate&lt;-MFGEmployees$AbsentHours/2080*100\nsummary(MFGEmployees)\n\n EmployeeNumber   Surname           GivenName            Gender         \n Min.   :   1   Length:8336        Length:8336        Length:8336       \n 1st Qu.:2085   Class :character   Class :character   Class :character  \n Median :4168   Mode  :character   Mode  :character   Mode  :character  \n Mean   :4168                                                           \n 3rd Qu.:6252                                                           \n Max.   :8336                                                           \n     City             JobTitle         DepartmentName     StoreLocation     \n Length:8336        Length:8336        Length:8336        Length:8336       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Division              Age         LengthService      AbsentHours    \n Length:8336        Min.   : 3.505   Min.   : 0.0121   Min.   :  0.00  \n Class :character   1st Qu.:35.299   1st Qu.: 3.5759   1st Qu.: 19.13  \n Mode  :character   Median :42.115   Median : 4.6002   Median : 56.01  \n                    Mean   :42.007   Mean   : 4.7829   Mean   : 61.28  \n                    3rd Qu.:48.667   3rd Qu.: 5.6239   3rd Qu.: 94.28  \n                    Max.   :77.938   Max.   :43.7352   Max.   :272.53  \n BusinessUnit        AbsenceRate     \n Length:8336        Min.   : 0.0000  \n Class :character   1st Qu.: 0.9196  \n Mode  :character   Median : 2.6926  \n                    Mean   : 2.9463  \n                    3rd Qu.: 4.5329  \n                    Max.   :13.1024  \n\nMFGEmployees&lt;-subset(MFGEmployees,MFGEmployees$Age&gt;=18)\nMFGEmployees&lt;-subset(MFGEmployees,MFGEmployees$Age&lt;=65)\n\n##3. Build The model One of the questions asked in the defining the goal step was ‘whether it was possible to predict absenteeism?’\nAbsence Rate is a numeric continuous value. In the ‘Building a model’ step we have to chose what models/statistical algorithms to use. Prediction of a numerics continous values suggests a couple of models that could be brought to bear: Regression trees and linear regression. There are many more but for purposes of this article we will look at these\n###3.1 Regression Trees Regression Trees will allow for use of both categorical and numeric values as predictors.Lets choose the following data as potential predictors in this analysis:\n\nGender\nDepartment Name\nStore Location\nDivision\nAge\nLength of Service\nBusiness Unit\n\nAbsence Rate will be the the ‘target’ or thing to be predicted.\n\nlibrary(rattle)   \n\nLoading required package: tibble\n\n\nLoading required package: bitops\n\n\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n\nlibrary(magrittr) \nbuilding &lt;- TRUE\nscoring  &lt;- ! building\ncrv$seed &lt;- 42 \nMYdataset &lt;- MFGEmployees\nstr(MYdataset)\n\n'data.frame':   8165 obs. of  14 variables:\n $ EmployeeNumber: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Surname       : chr  \"Gutierrez\" \"Hardwick\" \"Delgado\" \"Simon\" ...\n $ GivenName     : chr  \"Molly\" \"Stephen\" \"Chester\" \"Irene\" ...\n $ Gender        : chr  \"F\" \"M\" \"M\" \"F\" ...\n $ City          : chr  \"Burnaby\" \"Courtenay\" \"Richmond\" \"Victoria\" ...\n $ JobTitle      : chr  \"Baker\" \"Baker\" \"Baker\" \"Baker\" ...\n $ DepartmentName: chr  \"Bakery\" \"Bakery\" \"Bakery\" \"Bakery\" ...\n $ StoreLocation : chr  \"Burnaby\" \"Nanaimo\" \"Richmond\" \"Victoria\" ...\n $ Division      : chr  \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n $ Age           : num  32 40.3 48.8 44.6 35.7 ...\n $ LengthService : num  6.02 5.53 4.39 3.08 3.62 ...\n $ AbsentHours   : num  36.6 30.2 83.8 70 0 ...\n $ BusinessUnit  : chr  \"Stores\" \"Stores\" \"Stores\" \"Stores\" ...\n $ AbsenceRate   : num  1.76 1.45 4.03 3.37 0 ...\n\nMYinput &lt;- c(\"Gender\", \"DepartmentName\", \"StoreLocation\", \"Division\",\n     \"Age\", \"LengthService\", \"BusinessUnit\")\nMYnumeric &lt;- c(\"Age\", \"LengthService\")\nMYcategoric &lt;- c(\"Gender\", \"DepartmentName\", \"StoreLocation\", \"Division\",\n     \"BusinessUnit\")\nMYtarget  &lt;- \"AbsenceRate\"\nMYrisk    &lt;- NULL\nMYident   &lt;- \"EmployeeNumber\"\nMYignore  &lt;- c(\"Surname\", \"GivenName\", \"City\", \"JobTitle\", \"AbsentHours\")\nMYweights &lt;- NULL\nlibrary(rpart, quietly=TRUE)\nset.seed(crv$seed)\nMYrpart &lt;- rpart(AbsenceRate ~ .,\n    data=MYdataset[, c(MYinput, MYtarget)],\n    method=\"anova\",\n    parms=list(split=\"information\"),\n      control=rpart.control(minsplit=10,\n           maxdepth=10,\n        usesurrogate=0, \n        maxsurrogate=0))\nfancyRpartPlot(MYrpart, main=\"Decision Tree MFGEmployees $ AbsenceRate\")\n\n\n\n\nThe regression decision tree shows that age is a big factor in determining absence rate with gender playing a small part in one of the age ranges: &gt;43 and &lt;52 with males having a lower absence rate in this group. Almost all categorical information other than gender doesnt look like its helps in prediction.\nNow lets look at linear regression as another model. The restriction in linear regression is that it can only accept non-categorical variables. Categorical variables can sometimes be made numeric through transformation, but that is beyond the scope of this article.\n###3.2 Linear Regression\nIn linear regression, then, we will need to restrict it to numeric variables:\n\nAge\nLength of Service\n\nbeing used to predict absence rate.\n\n#Linear Regression Model\nRegressionCurrentData &lt;- lm(AbsenceRate~Age+LengthService, data=MFGEmployees)\nsummary(RegressionCurrentData)\n\n\nCall:\nlm(formula = AbsenceRate ~ Age + LengthService, data = MFGEmployees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3578 -0.8468 -0.0230  0.8523  5.1030 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.190213   0.068959  -75.27   &lt;2e-16 ***\nAge            0.202593   0.001510  134.16   &lt;2e-16 ***\nLengthService -0.085309   0.005652  -15.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.264 on 8162 degrees of freedom\nMultiple R-squared:  0.6887,    Adjusted R-squared:  0.6886 \nF-statistic:  9027 on 2 and 8162 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary shows an adjusted R-squared of .68 which means approximately 68% of the variance is accounted by age and length of service. The variables are both significant at Pr(&gt;|t|) of &lt;2e-16. These results are using the entirety of the existing data to predict itself.\nGraphically it look like this:\n\n#2D plot of Age and AbsenceRate\nlibrary(ggplot2)\n\nggplot() + geom_point(aes(x = Age,y = AbsenceRate),data=MFGEmployees) +geom_smooth(aes(x = Age,y = AbsenceRate),data=MFGEmployees,method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n#3D Scatterplot  of Age and Length of Service with Absence Rate - with Coloring and Vertical Lines\n# and Regression Plane \nlibrary(scatterplot3d) \n\ns3d &lt;-scatterplot3d(MFGEmployees$Age,MFGEmployees$LengthService,MFGEmployees$AbsenceRate, pch=16, highlight.3d=TRUE,\n                    type=\"h\", main=\"Absence Rate By Age And Length of Service\")\nfit &lt;- lm(MFGEmployees$AbsenceRate ~ MFGEmployees$Age+MFGEmployees$LengthService) \ns3d$plane3d(fit)\n\n\n\n\n\n\nUp till now we have concentrated on producing a couple of models. The effort so far has had one weakness. We have used all of our data for 2015 to generate the models. They can both predict, but the prediction are based on existing data- dat already known. We dont know how well it will predict on data it hasnt seen yet.\nTo evaluate and critique the models, we need to train the model using part of the data and hold out a portion to test on.We will divide the data into 10 parts- using 9 parts as training data and 1 part as testing data, and alternate which are the 9 and the 1, so that each of the 10 parts gets to be training data 9 times and testing data once.\nThe R “caret” library helps us do that. We will run both a regression tree and linear regression and compare how they do against each other.\nFirst the Linear Regression\n\n# MFGEmployees &lt;- readRDS(file=\"c:/Users/Lyndon.A3HR/Documents/MFGEmployees.Rda\")\nlibrary(caret)\n\nLoading required package: lattice\n\nset.seed(998)\ninTraining &lt;- createDataPartition(MFGEmployees$BusinessUnit, p = .75, list = FALSE)\ntraining &lt;- MFGEmployees[inTraining,]\ntesting &lt;- MFGEmployees[ - inTraining,]\n\nfitControl &lt;- trainControl(## 10-fold CV\nmethod = \"repeatedcv\",\n                           number = 10,\n                           ## repeated ten times\nrepeats = 10)\n\nset.seed(825)\nlmFit1 &lt;- train(AbsenceRate ~ Age + LengthService, data = training,\n                 method = \"lm\",\n                 trControl = fitControl)\nlmFit1\n\nLinear Regression \n\n6124 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.269977  0.6909094  1.007153\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nThe rSquared shows a value of .688 which means even with sampling different parts of the data on 10 fold cross validation the use of age and length of service seems to be pretty robust so far.\nNext the decision tree. The first time with just the numeric variables.\n\nset.seed(825)\nrpartFit1 &lt;- train(AbsenceRate ~ Age + LengthService, data = training,\n                 method = \"rpart\",\n                 trControl = fitControl,\n                 maxdepth = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrpartFit1\n\nCART \n\n6124 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results across tuning parameters:\n\n  cp          RMSE      Rsquared   MAE     \n  0.06330829  1.431901  0.6062148  1.140352\n  0.09451716  1.557060  0.5344593  1.260302\n  0.48920642  1.963226  0.4741241  1.607013\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.06330829.\n\n\nYou will notice that the decision tree with 10 fold cross validation didnt perform as well with an RSquared of approximately .60\nThe second time with the original categorical and numeric varibles used.\n\nset.seed(825)\nrpartFit2 &lt;- train(AbsenceRate ~ Gender + DepartmentName + StoreLocation + Division + Age + LengthService + BusinessUnit, data = training,\n                 method = \"rpart\",\n                 trControl = fitControl,\n                 maxdepth = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrpartFit2\n\nCART \n\n6124 samples\n   7 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results across tuning parameters:\n\n  cp          RMSE      Rsquared   MAE     \n  0.06330829  1.431901  0.6062148  1.140352\n  0.09451716  1.557060  0.5344593  1.260302\n  0.48920642  1.963226  0.4741241  1.607013\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.06330829.\n\n\nHere when you include all originally used vaiables in 10 fold cross validation, the RSquared changed little and is still around .60.\nSo far the linear regression is performing better\n##5.Present Results and Document\nThe presenting of results and documenting is something that R helps in. You may not have realized it, but the R Markdown language has been used to create the full layout of these two blog articles. HTML,PDF and Word formats can be produced.\nR Markdown allows the reader to see exactly what you having been doing , so that an independent person can replicate your results, to confirm what you have done. It shows the R code/commands, the statistical results and graphics.\nThese are:\n\nAbsenteeism-Part1.Rmd and\nAbsenteeism-Part2.Rmd\n\nFor presentation formats beyond this, you may have to use other tools.\n##6.Deploy Model Once you have evaluated your model(s) and chosen to use them, they need to be deployed so that they can be used. At the simplest level, ‘deploy’ can mean using the ‘predict’ function in R (where applicable) in conjunction with your model.\nIn R, you can also ‘publish’ you model as an R HTTP service so that others can use it.(That is beyond the scope of this article)\nCan it predict next year absenteeism?\nLets predict the 2016 Absenteeism from the 2015 model.\nIf we make the simplifying assumption that nobody quits and nobody new comes in, we can take the 2015 data and add 1 to age and 1 to years of service for an approximation of new 2016 data before we get to 2016.\n\n#Apply model\n#Generate 2016 data\nAbsence2016Data&lt;-MFGEmployees\nAbsence2016Data$Age&lt;-Absence2016Data$Age+1\nAbsence2016Data$LengthService&lt;-Absence2016Data$LengthService+1\nAbsence2016Data$AbsenceRate&lt;-0\nAbsence2016Data$AbsenceRate&lt;-predict(lmFit1,Absence2016Data)\n\nTo get single estimate for 2016 we ask for mean of absence rate.\n\nmean(Absence2016Data$AbsenceRate)\n\n[1] 3.040885\n\nmean(MFGEmployees$AbsenceRate)\n\n[1] 2.907265\n\n\nThe first figure above is the 2016 prediction, the second is the 2015 actual for comparison.\nIf so, how well can it predict?\nAs mentioned previously, about 68% of the variation is accounted for in a linear regression model using age and length of service.\nCan we reduce our absenteeism?\nOn the surface, only getting the age reduced and length of service increased will reduce absensteeism with this model.\nObviously, absenteeism is much more complex that just the rudimentary data we have collected. A serious look at this metric and problem would require more and different kinds of data. As mentioned before , the raw data used in this article and analysis is totally contrived to illustrate an example.\n#Final Comments\nThe purposes of these two blogs articles was to:\n\nshow that R could be used to do People Analytics\nshow that People Analytics is the application of the Data Science to People (HR) Management and decision making.\nshow by a rudimentary/simple (not necessarily rigorous) example that the ‘data science’ capability is in your hands.\nshow that ‘free’ tools can be used to start the ‘People Analytics’ journey.\n\nIts time to apply data science to People Management, and be data-driven.\nEnjoy the journey!"
  },
  {
    "objectID": "posts/PAExampleUsingR/PAExampleUsingR.html#evaluate-and-critique-model",
    "href": "posts/PAExampleUsingR/PAExampleUsingR.html#evaluate-and-critique-model",
    "title": "People Analytics - An Example Using R",
    "section": "",
    "text": "Up till now we have concentrated on producing a couple of models. The effort so far has had one weakness. We have used all of our data for 2015 to generate the models. They can both predict, but the prediction are based on existing data- dat already known. We dont know how well it will predict on data it hasnt seen yet.\nTo evaluate and critique the models, we need to train the model using part of the data and hold out a portion to test on.We will divide the data into 10 parts- using 9 parts as training data and 1 part as testing data, and alternate which are the 9 and the 1, so that each of the 10 parts gets to be training data 9 times and testing data once.\nThe R “caret” library helps us do that. We will run both a regression tree and linear regression and compare how they do against each other.\nFirst the Linear Regression\n\n# MFGEmployees &lt;- readRDS(file=\"c:/Users/Lyndon.A3HR/Documents/MFGEmployees.Rda\")\nlibrary(caret)\n\nLoading required package: lattice\n\nset.seed(998)\ninTraining &lt;- createDataPartition(MFGEmployees$BusinessUnit, p = .75, list = FALSE)\ntraining &lt;- MFGEmployees[inTraining,]\ntesting &lt;- MFGEmployees[ - inTraining,]\n\nfitControl &lt;- trainControl(## 10-fold CV\nmethod = \"repeatedcv\",\n                           number = 10,\n                           ## repeated ten times\nrepeats = 10)\n\nset.seed(825)\nlmFit1 &lt;- train(AbsenceRate ~ Age + LengthService, data = training,\n                 method = \"lm\",\n                 trControl = fitControl)\nlmFit1\n\nLinear Regression \n\n6124 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.269977  0.6909094  1.007153\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nThe rSquared shows a value of .688 which means even with sampling different parts of the data on 10 fold cross validation the use of age and length of service seems to be pretty robust so far.\nNext the decision tree. The first time with just the numeric variables.\n\nset.seed(825)\nrpartFit1 &lt;- train(AbsenceRate ~ Age + LengthService, data = training,\n                 method = \"rpart\",\n                 trControl = fitControl,\n                 maxdepth = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrpartFit1\n\nCART \n\n6124 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results across tuning parameters:\n\n  cp          RMSE      Rsquared   MAE     \n  0.06330829  1.431901  0.6062148  1.140352\n  0.09451716  1.557060  0.5344593  1.260302\n  0.48920642  1.963226  0.4741241  1.607013\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.06330829.\n\n\nYou will notice that the decision tree with 10 fold cross validation didnt perform as well with an RSquared of approximately .60\nThe second time with the original categorical and numeric varibles used.\n\nset.seed(825)\nrpartFit2 &lt;- train(AbsenceRate ~ Gender + DepartmentName + StoreLocation + Division + Age + LengthService + BusinessUnit, data = training,\n                 method = \"rpart\",\n                 trControl = fitControl,\n                 maxdepth = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrpartFit2\n\nCART \n\n6124 samples\n   7 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... \nResampling results across tuning parameters:\n\n  cp          RMSE      Rsquared   MAE     \n  0.06330829  1.431901  0.6062148  1.140352\n  0.09451716  1.557060  0.5344593  1.260302\n  0.48920642  1.963226  0.4741241  1.607013\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.06330829.\n\n\nHere when you include all originally used vaiables in 10 fold cross validation, the RSquared changed little and is still around .60.\nSo far the linear regression is performing better\n##5.Present Results and Document\nThe presenting of results and documenting is something that R helps in. You may not have realized it, but the R Markdown language has been used to create the full layout of these two blog articles. HTML,PDF and Word formats can be produced.\nR Markdown allows the reader to see exactly what you having been doing , so that an independent person can replicate your results, to confirm what you have done. It shows the R code/commands, the statistical results and graphics.\nThese are:\n\nAbsenteeism-Part1.Rmd and\nAbsenteeism-Part2.Rmd\n\nFor presentation formats beyond this, you may have to use other tools.\n##6.Deploy Model Once you have evaluated your model(s) and chosen to use them, they need to be deployed so that they can be used. At the simplest level, ‘deploy’ can mean using the ‘predict’ function in R (where applicable) in conjunction with your model.\nIn R, you can also ‘publish’ you model as an R HTTP service so that others can use it.(That is beyond the scope of this article)\nCan it predict next year absenteeism?\nLets predict the 2016 Absenteeism from the 2015 model.\nIf we make the simplifying assumption that nobody quits and nobody new comes in, we can take the 2015 data and add 1 to age and 1 to years of service for an approximation of new 2016 data before we get to 2016.\n\n#Apply model\n#Generate 2016 data\nAbsence2016Data&lt;-MFGEmployees\nAbsence2016Data$Age&lt;-Absence2016Data$Age+1\nAbsence2016Data$LengthService&lt;-Absence2016Data$LengthService+1\nAbsence2016Data$AbsenceRate&lt;-0\nAbsence2016Data$AbsenceRate&lt;-predict(lmFit1,Absence2016Data)\n\nTo get single estimate for 2016 we ask for mean of absence rate.\n\nmean(Absence2016Data$AbsenceRate)\n\n[1] 3.040885\n\nmean(MFGEmployees$AbsenceRate)\n\n[1] 2.907265\n\n\nThe first figure above is the 2016 prediction, the second is the 2015 actual for comparison.\nIf so, how well can it predict?\nAs mentioned previously, about 68% of the variation is accounted for in a linear regression model using age and length of service.\nCan we reduce our absenteeism?\nOn the surface, only getting the age reduced and length of service increased will reduce absensteeism with this model.\nObviously, absenteeism is much more complex that just the rudimentary data we have collected. A serious look at this metric and problem would require more and different kinds of data. As mentioned before , the raw data used in this article and analysis is totally contrived to illustrate an example.\n#Final Comments\nThe purposes of these two blogs articles was to:\n\nshow that R could be used to do People Analytics\nshow that People Analytics is the application of the Data Science to People (HR) Management and decision making.\nshow by a rudimentary/simple (not necessarily rigorous) example that the ‘data science’ capability is in your hands.\nshow that ‘free’ tools can be used to start the ‘People Analytics’ journey.\n\nIts time to apply data science to People Management, and be data-driven.\nEnjoy the journey!"
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html",
    "href": "posts/ddjobclass/ddjobclass.html",
    "title": "Data Driven Job Classification",
    "section": "",
    "text": "As I have shared in recent previous blog articles, a lot of attention is being paid to ‘data driven’ HR currently. Along with that is the fact that the terminology and what constitutes the totality of ‘data driven’ HR is still hugely in flux as well.\nIn my last blog article:\nhttps://www.linkedin.com/pulse/what-does-data-driven-hr-look-like-lyndon-sundmark-mba?trk=prof-post\nI mentioned that ultimately the ‘totality’ from a content perspective could largely fall into 3 major categories- based on the ‘why’ we capture HR metrics and measures:\n\nWe capture metrics to help us understand what is going on with our human resources- human resource activity related metrics\nWe capture metrics that help us understand how well our HR business processes themselves are performing- metrics related to process measurement and quality improvement\nWe capture metrics to help fundamentally change the HR methodologies themselves-making the how we do the various HR functions more robust by applying data driven statistical methods to them.\n\nThis article is intended to illustrate an example of the last bullet point above. How can we make the HR functions more robust by infusing deeper measures and applying statistical analysis to make improved HR decisions?\nIn this blog, I will illustrate how this can be done with an example of one method of job classification. The particular method illustrated will be an approach that is typically done in a non-quantitative way. (I am well aware that there are ‘point methods’ etc. for job evaluation that attempt to quantify information through assigning of points and the summing of them)\n\nI will start out by describing one ‘traditional’ non-statistical way this illustrated method is often done.\nThen I show how that can be transformed into measures/metrics.\nFinally I will show an example of a statistical procedure that can be used to support this HR function- and actually apply it to the measures created- to show how ‘data driven’ statistics can help support or improve the job classification process.\n\nI should also mention that I have no intent or purpose in this blog article to get into a wider discussion of job classification methods that are out there, many of which have successfully been around for years- quantitative or non-quantitative. My only purpose here is to illustrate the application of statistical methods to an example HR function- especially a traditional non quantitative method. I simply want to show that non quantitative information can often be quantified in some way, and that statistical procedures can be applied, models can be built, and the function can be more data driven as a result.\nFor those to whom the application of statistical methods to a business problem is new or unfamiliar, this activity does not require manual mathematical/statistical calculations. Computerized statistical software packages do that for you. Your work is not in the calculations. Having said that, your work is to generate and prepare the data, understand the appropriate statistical tests to run, and how to interpret the output.\nThe statistical method I will show is also just one example. As you will see later in this article, predictive ‘classification’ or ‘categorization’ is a huge area of applied statistics. There are many methods possible. In a real life situation, you might want to apply many different statistical methods to see how they vary in their predictive accuracy. Because I would like to keep the blog article at a reasonable length, I will just show one example and touch the surface of what is possible here."
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html#what-does-linear-discriminant-analysis-lda-require",
    "href": "posts/ddjobclass/ddjobclass.html#what-does-linear-discriminant-analysis-lda-require",
    "title": "Data Driven Job Classification",
    "section": "What does Linear Discriminant Analysis (LDA) require?",
    "text": "What does Linear Discriminant Analysis (LDA) require?\n\nThere needs to be a population of something. (In our case it’s the full list of job specs – and eventually job descriptions too)\nThat population must be categorized into known groups. (in our case paygrades)\nEach member of the population must be characterized by a set of measured features. (In our case it could be contact level, financial responsibility, supervision received etc.)\nIn addition to these, there may be other data requirements for ‘interpretability’ of the statistical results -i.e. normality of data. (The coverage of these is beyond my intent in this article)\nGiven the ‘known groups’ that the population is assigned to, and the level of those features in each member of the population, LDA will figure out mathematically the combination of those features that best categorize each group –ultimately through formulas. Those formulas can then be used to figure out mathematically the best fit group would be- when applied to items not yet in the population. (In our case- what combination of the levels of features of contact level, financial responsibility, and supervision received, etc. can best predict the closest fit paygrade)."
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html#what-does-lda-do",
    "href": "posts/ddjobclass/ddjobclass.html#what-does-lda-do",
    "title": "Data Driven Job Classification",
    "section": "What does LDA do?",
    "text": "What does LDA do?\nGiven the ‘known groups’ that the population is assigned to, and the level of those features in each member of the population, LDA will figure out mathematically the combination of those features that best categorize each group –ultimately through formulas. Those formulas can then be used to figure out mathematically the best fit group would be- when applied to items not yet in the population. (In our case- what combination of the levels of features of contact level, financial responsibility, and supervision received, etc. can best predict the closest fit paygrade)."
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html#quantify-and-categorize-the-information",
    "href": "posts/ddjobclass/ddjobclass.html#quantify-and-categorize-the-information",
    "title": "Data Driven Job Classification",
    "section": "Quantify and Categorize the Information",
    "text": "Quantify and Categorize the Information\nIn this step we are going to use an Excel Spreadsheet (CSV- comma separated value file) to capture the information that is necessary. We will need to translate the information contained in the job specifications into categories and measures. In the example illustrated below- the categorical information is the Paygrade, the quantitative information are the features we decide to measure.\nLet us look at some hypothetical data:\n\nWe have 66 job specifications covering 11 paygrades.\nEach of the 66 job specification narratives include detailed information about the nature of that work. They also include descriptions related to various features that all the job specifications have in common. For purposes of our example, these ‘features’ include:\n\nMinimum education level required\nMinimum years of experience required\nLevel of organizational impact\nDegree of problem solving\nLevel of supervision exercised\nHighest contact Level\nLevel of Financial Budget Responsibilities\n\nEach of those features is mentioned in every job specification but is a narrative description.\nWe need to go through all of these job class specifications and develop a scale for each of these features. The scale would on the one hand have a value of 0 if that job specification contains none of that feature. As an example, a basic clerk might have no financial budget responsibility. The idea is to summarize all statements describing that feature and at least order them at an ‘ordinal’ level- in other words identify the order of lowest to highest on a scale for that feature. A highest level for Financial/Budget responsibility might be total financial budgetary responsibility across an enterprise and be assigned a value of 5. A 1 might be responsibility for a particular organizational program. A 2 might be at a level of a department etc. The idea is that the level that these features exist at will vary from job specification and pay grade. So to quantify these features, we need to know the range of levels these features can exist at. Once we know these, we can then take each job specification and rate its features levels one at a time.\nIn the example above, it means for each job classification specification we have, we would have one row in our spreadsheet and the columns would represent that which we are trying to eventually predict and the values of each of those ‘features’ for that particular job specification.\nIn a spreadsheet it would look something like this:\n\n\nAnd in my hypothetical example there would be 66 rows.\n\nThe id column in this example is simply an integer- but it would relate to a job class specification id in the real world as a label in your data.\nThe Paygrade is what you will eventually want to predict based on the features of Education Level, Experience, Organizational Impact, Problem Solving, Supervision, Contact Level and Financial budget.\nEvery feature would have a value for each job specification, even if the value is 0\nEach feature would have a range of valid values specific to it. The values can only be in that range.\n\nAt this point you have taken what is essentially narrative information and converted it to quantitative data in a meaningful way."
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html#run-the-lda-statistical-procedure",
    "href": "posts/ddjobclass/ddjobclass.html#run-the-lda-statistical-procedure",
    "title": "Data Driven Job Classification",
    "section": "Run the LDA Statistical Procedure",
    "text": "Run the LDA Statistical Procedure\nFor this next step you actually perform that statistic analysis. A wide variety of software tools exist for this purpose. Some of the more well-known ones are SPSS, SAS and R. SPSS and SAS are proprietary, R is free.\nI will use R to perform the statistical analysis. (Please note – for this blog article please do not worry if you don’t completely understand the R coding or commands below. My intention here is to illustrate a process. There are other statistical packages out there where the analysis is done by a more point and click type of method. In any approach used though you do need to understand what is being predicted and what are the predictors being used for it).\nPlease also note that even within the R statistical package, there are multiple different libraries and functions all capable of producing discriminant analysis output. You have choices.\nFinally, it’s important to understand that many statistical procedures not only have the intent of being used for predictive purposes- but in getting to the prediction -provide you with a lot of descriptive information as well to understand better the dataset you are using.\nHere is an actual set of R commands that will perform this discriminant analysis:\nlibrary(MASS)\nldaModel &lt;- lda(PayGrade~EducationLevel+Experience+OrgImpact+ProblemSolving+Supervision+ContactLevel+FinancialBudget, data=jobclassinfo2, method=“moment”, tol=0.0001)\nThe above statement essential says:\n\nGo find the library in R called MASS\nCreate a storage object called ldamodel.\nRun a LDA procedure –linear discriminant analysis found in the MASS library, with Paygrade being what we are trying to predict, and Education level through to Financial Budget as being the ‘features’ upon which the prediction will be based on. Get the raw data from a file called jobclassinfo2\nStore the results into ldaModel\n\nThis code above is what actually does the process of figuring out what the discriminant equations are. In addition, it generates and provides a wealth of other useful information."
  },
  {
    "objectID": "posts/ddjobclass/ddjobclass.html#review-and-interpret-the-results",
    "href": "posts/ddjobclass/ddjobclass.html#review-and-interpret-the-results",
    "title": "Data Driven Job Classification",
    "section": "Review and Interpret the Results",
    "text": "Review and Interpret the Results\nHere is some sample output:\n\nRemember earlier I had mentioned that Linear Discriminant Analysis has both a descriptive part and a predictive part? What you see above is the default descriptive part. We will just briefly look at this first.\nThe above is provided so that one sees intermediate results eventually used to do the prediction.\n\nThe first summary shows the prior probabilities of the groups. That is, given that there were 66 job specs and 11 pay grade groups- what proportion are in each group before the discriminant calculations occur.\nThe second summary shows you group means (averages) for each of the 11 pay groups- what the average value of each of the 7 features was for each group. This doesn’t necessarily look as cool in the form of a table, but adding graphics to it-gives a good visual as to how the groups are different. For example you can do a box plots on a couple of the features to get an idea of how they differ by group.\n\n\n\nYou could do this for each of the ‘features’ to see how they vary by paygrade.\n\nThe third summary shows the coefficients of linear discriminants. These are necessary for R to eventually predict the category.\nThe final summary is the Proportion of Trace. What this summary shows is what proportion of the entire variance of the data is accounted for by each discriminant equation. In this example between LD1 and LD2- the first two discriminant equations- 95.16 + 3.54 % (98.6) of the variance in our data is accounted for.\n\nIt’s not my intent in this article, to discuss the above output much further. Suffice it to say, the descriptive part of this has the purpose of giving us a deep picture and understanding of our data itself. The descriptive output above is still just a sampling of some of the descriptive output provided. One could write a blog article alone just on illustrating all that can be provided, what it means, and why it’s provided.\nBy running the following additional commands in the R statistic package we can get a prediction. This code grabs the features values for a single new position description not yet classified in JobClassInfo2testdata.txt and tells it to predict best fit pay group based on our ldaModel and store the results into GradePred.\nThe data case in JobClassInfo2testdata.txt is as follows:\nDataset &lt;- read.table(“C:/Users/Lyndon.A3HR/Documents/JobClassInfo2testdata.txt”,\nheader=TRUE, sep=“\\t”, na.strings=“NA”, dec=“.”, strip.white=TRUE)\nsave(“Dataset”, file=“C:/Users/Lyndon.A3HR/Documents/jcinfo2.RData”)\nGradePred &lt;- predict(ldaModel,Dataset)\ntable(GradePred$class)\nIn effect, we are saying to R- here are the values of the features from a job description for which we do not know what the best fit paygrade is. You tell us based on the model you created from our known population of job class specifications.\nThis results in the following output:\ntable(GradePred$class)\n\nThis shows a 1 under the value 5 and 0’s everywhere else. It means that paygroup 5 is the predicted best fit paygroup based on the features of this new job description as compared to the features of the population of all 11 pay groups.\nWhile we showed the example of taking one new job description above and classifying it into best fit paygrade, doing that at this point would be premature in a real life situation. We did it at this point to illustrate that prediction CAN be done- based on the results of the discriminant analysis.\nIn real life we would want to do much more work on this ‘model’ before putting it into extensive use. This ‘work’ would be ‘validation’ and perhaps tweaking of the model. We want to know\n\nHow well is the model predicting?\nHow accurate is our prediction?\nCan we make our prediction even more accurate?\nValidation\n\nOne way to validate our model, is to take the original data and paygrades that we already know for the 66 job class specifications and use the data we have on their feature values to predict what pay grades would be predicted for the population we already know. In other words, given our model, how often would it predict the actual values of paygrade that we already know for each of the 66 job class specs that we have?\nLet’s do that and see:\nHere are the R commands used and the output:\nldaPredict &lt;- predict(ldaModel, jobclassinfo2 )\ntablepred &lt;- table(jobclassinfo2$PG,ldaPredict$class)\n\nThe interpretation of the above is as follows:\n\nThe line PG01 represents some of the actual original values for the job class specifications in our data. Two of the specs had pay grades of PG01 (1+1 above). When predicted based on the features of the paygrades- one was predicted to be at pay grade 1 and the other at paygrade 2.\nIF the model predicted the paygrades perfectly- only the main diagonal from top left to bottom right would have non 0 counts in them. If anything falls off that main diagonal in terms of comparing the actual value to the predicted, it is deemed to be an ‘error’- because it didn’t predict its original value.\nSo the error rate is what percentage of the predictions turned out not to be correct based on the original values. Above there was a 22% error rate which means a 78% accuracy rate.\n\nSo you might say- ok – is that good or not? Well, you need to remember that if you had none of this and you had neither discriminant analysis or prior knowledge of what proportion were in each pay grade based on what we already knew about the real population of 66 job class specifications, you would have 1 chance in 11 (9%) of getting it correct by chance, and 10 chances out of 11 to get it wrong (91% error rate) by guessing- because there are 11 possible groups. So upping that accuracy rate from 9% to 78% I would say is pretty good.\nDoes that get your attention?"
  },
  {
    "objectID": "posts/HRAWayOfDoing/HRWayOfDoing.html",
    "href": "posts/HRAWayOfDoing/HRWayOfDoing.html",
    "title": "HR Analytics Is A ‘Way of Doing HR’- Not An ‘Add-On’",
    "section": "",
    "text": "HR Analytics Is a different way of doing HR -period. It’s not an add-on. It’s typically not a single initiative, but rather a series of initiatives to change the way we do HR. That’s how we should see it.\nThe way we see, understand, and define HR Analytics profoundly affects both our ability to implement it and to see long term success in it. If we don’t see this, we are destined to put off its potential for giving us a competitive advantage for decades.\nThe rest of this blog article is an ‘opinion’ piece to try to help HR professionals understand why this is the case."
  },
  {
    "objectID": "posts/HRAWayOfDoing/HRWayOfDoing.html#hr",
    "href": "posts/HRAWayOfDoing/HRWayOfDoing.html#hr",
    "title": "HR Analytics Is A ‘Way of Doing HR’- Not An ‘Add-On’",
    "section": "HR",
    "text": "HR\nIf we aren’t already doing it:\n\nSee HR Analytics as a change in the way we do HR and not as a special separate initiative. See it as ‘data-driven’, ‘evidence-based’ HR management, and decision making. It may contain a series of ‘initiatives’ because we are changing the way we do HR, but those initiatives need to be seen as being core to HR methodologies and practices that re-engineer HR for the future. See these initiatives in that context.\n‘Own it!’ HR Analytics is not someone else’s, some other areas responsibility. Its HR’s responsibility. And all of HR- not just an analytics/reporting team in HR. No one else has HR domain knowledge other than HR. Other areas such as IT and Data Scientists in the organization may be involved to support you- but its your responsibility, and your ‘HR methodologies and practices ’change’ initiatives’.\nDon’t assume that the initial efforts necessarily require additional resources and therefore a business case. HR Analytics at its core requires 3 things:\n\nHR Domain knowledge- which you have – otherwise you wouldn’t be working this field\nRightful access to HR information -consistent with whatever your organizational data governance and government legislative requirements are.\nTools to organize and analyze your data- typically statistical analysis software There are free or ‘free to prototype’ tools available.\n\n\nWhen you consider the above - as the start - where do those 3 things require ‘extra’ resources that cost money?\n\nObtain the necessary analytical skills. HR Analytics requires HR domain knowledge, Information technology skills-at a minimum specifically data access and data organizing skills, and statistical analysis skills. It also requires you to see HR from an informational and analytical perspective. (For example, if you have an applicant tracking system- know what information it contains and how its organized) Even if you believe you can rely on other organizational areas for this in the short term, that does not equate to ‘owning it’ and dependency on others is often unsustainable in the long term. Infusing ‘data-driven’ and ‘evidence-based’ into HR cannot be delegated to other areas. Its your responsibility. OWN IT.\nWherever possible -start your initiatives with an idea and a prototype. Assuming you have the necessary skills and rightful access to data - given your HR organizational responsibilities, its far easier to enlist more organizational support when you have a working small-scale prototype created from free tools.\n\nDiagnostic analyses often reveal previously hidden valuable insights to an organization. You can do these with free tools on existing data.\nPredictive analyses typically start their validation by being able to take existing data and existing outcomes and developing models to see if those outcomes can be predicted- comparing the real outcomes to the predicted outcomes for how close or how often the prediction came or got it right. If the prediction is good, often the next step is to try the predictive model on data the model has not yet seen. You can do these with free tools on existing data.\n\n\nEither way, you are showing by concrete ‘simple’ examples what might be possible with further effort. If business cases are then required for additional resources for moving forward, it’s a whole lot easier to make that case.\n\nBecome ‘measurement’ driven. Data-driven requires ‘data’. Measurement, at its most fundamental meaning is both the capturing of data, and the summarizing of it for either answering questions or to bring organizational visibility to things that may be of consequence. Within the context of HR this may often mean:\n\nDeveloping traditional HR metrics and dashboards to understand what is going in the interactions between applicants, employees and terminates in your organization -the life cycle of employees.\nMeasuring the activity in HR and the demands put upon you. This is often best achieved through HR Service Request Tracking Systems. Most of what you do and are working on in HR is a request from someone- a ‘service’ request. As an HR professional you provide services. Service Request Tracking (ticketing) Systems track ‘these’ requests. They often record when they came in, and often record when you started it and when you completed it. By the nature of what they are, they then also capture the volume of requests that are coming in, what their current status is, and who it was assigned to. This allows for being able to measure how long typical services will take which in turn can allow for better setting of customer expectations. It also allows for better resource utilization and a way of measuring whether we are doing better over time. Without systems like this, it’s almost impossible to provide yourself in HR and the organization a clear picture of the work going on in HR and its volume.\nMeasuring how good your HR business process or practices/methodologies and techniques are with an eye for improvement in reliability and timeliness of their end results.\n\n\nI can’t underscore enough the consequences of not being ‘data-driven’ in the nature of HR work and how we do it. As mentioned earlier, my own experience over 30 years in HR had me conclude that much of HR work in organizations is ‘reactive’. Much of the work is waiting for requests from the organization for something and then acting on it. It gets completed and the result given to the requestor. Once done, unless there was an issue, it’s out of sight and out of mind because a new request is demanding our time.\nBusinesses in general, usually put a premium on getting information about their customers and their needs for repeat sales and business. If they operate totally reactive and with organizational ‘amnesia’ with respect to customers, they usually don’t stay in business too long."
  },
  {
    "objectID": "posts/HRAWayOfDoing/HRWayOfDoing.html#executives",
    "href": "posts/HRAWayOfDoing/HRWayOfDoing.html#executives",
    "title": "HR Analytics Is A ‘Way of Doing HR’- Not An ‘Add-On’",
    "section": "Executives",
    "text": "Executives\nIf you aren’t already doing it:\n\nSimilar to HR, see HR Analytics as a change in the way HR is done- requiring wherever possible that ‘data-driven’ be infused into the organization’s HR practices and policies.\nRecognize that HR Analytics isn’t a one-time single initiative and recognize that this isn’t short term either. Recognize that this may be a culture change itself in HR, so a long-term commitment and accountability may be required. Make this part of the role, responsibilities and goals of the Chief Human Resources Officer or Executive in your organization.\nRecognize that HR changing the way it does HR will take time and that it may consist of many initiatives. Each one when complete- moves HR and the organization further- on the journey towards data-driven HR. Some of these initiatives will be able to be done within existing resources, others will require additional resources.\nSupport HR in providing you with an ongoing picture of what is happening with respect to your employees /people resources/ human resources through provision of traditional HR Metrics. Give them the resources to do this on an ongoing basis and in an automated way- so that most of the time spent is on analysis of what’s going on, not in the preparation of these.\nSupport HR in encouraging the usage of Service Request Tracking Systems, so that they can give you a ‘true’ picture of the volume of work and demands that are put on HR on an ongoing basis\nSet an expectation of, and provide the climate for, innovation in HR practices and methodologies and as much as possible within organizational resources and business priorities. Data Science as applied to HR practices and issues could have much potential for providing you with a competitive advantage."
  },
  {
    "objectID": "posts/HRAWayOfDoing/HRWayOfDoing.html#colleges-universities-and-other-institutions-of-learning",
    "href": "posts/HRAWayOfDoing/HRWayOfDoing.html#colleges-universities-and-other-institutions-of-learning",
    "title": "HR Analytics Is A ‘Way of Doing HR’- Not An ‘Add-On’",
    "section": "Colleges, Universities, and Other Institutions of Learning",
    "text": "Colleges, Universities, and Other Institutions of Learning\nYou may have much work ahead of you since you are the educators of new HR professionals and skills upgrading for existing HR professionals.\nIf you aren’t already doing it:\n\nInclude statistical analysis, data science, and information technology courses as part of core requirements in the curriculum for HR academic and professional education. This has to go beyond a statistics elective from a different faculty on campus. If the goal is to infuse analytics into the very fabric of HR work, the textbooks, the course materials and the examples themselves must be HR examples. Although I say this in the context of HR, it is equally applicable to most areas of business study. These should include:\n\nBasic statistical measures and concepts\nUnivariate, bivariate, and multivariate statistical analyses\nMachine learning algorithms\nThe Data Science process\nData Mining\nStatistical software such as R and possibly also Python (at least the data manipulation and machine learning parts)\nData cleansing, organization, and engineering\n\nIn addition to the above focus, require that the teaching of ALL HR courses in all HR functional areas include sections on how that function in HR is understood and represented ‘process-wise and informationally’:\n\n-   What is a typical business process or processes for this function?\n\n-   What are the process information inputs, in process data, and process output data?\n\n-   What data elements are needed and typically kept and why Where this information is generally found in an HR information system.\nThis is how to infuse HR analytics into the very fabric of how HR is done. As mentioned earlier, too much of the traditional course materials, books, and authors have not covered this. They often haven’t treated HR from an additional ‘process orientation’, but rather as a series of functions and the concepts and traditional methodologies that go along with that. And they often don’t treat HR domain knowledge from an informational and analytical perspective."
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html",
    "href": "posts/PAExampleChurn/PAExampleChurn.html",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "",
    "text": "This is the second is a series of blog articles on using R for doing People Analytics. The first was my last article:\nhttps://www.linkedin.com/pulse/people-analytics-example-using-r-lyndon-sundmark-mba?trk=prof-post\nIt gave an example of People Analytics being applied to absenteeism data. Lets now take a look at another HR example- this time employee churn. (Once again, the example is intended to be illustrative, not necessarily robust or best practices)\nYou may be asking what is Employee Churn? In a word -“turnover’- its when employees leave the organization. In another word-”terminates”, whether is be voluntary or involuntary. In the widest sense churn/turnover is concerned both the calculation of rates of people leaving the organization and the individual terminates themselves.\nMost of the focus in the past has been on the ‘rates’, not on the individual terminates.We calaculate past rates or turnover in an attempt to predict future turnover rates. And indeed it is important to do that and to continue to do so. Data warehousing tools are very powerful in this regard to slice and dice this data efficiently over different time periods at different levels of granularity. BUT it is only half the picture. These rates only show the impact of churn/turnover in the ‘aggregate’. In addition to this you might be interested in predicting exactly ‘who’ or ‘which employees’ exactly may be at high risk for leaving the organization. Hence the reason for being interested in the ‘individual’ records in addition to the aggegrate.\nStatistically speaking, ‘churn’ is ‘churn’ regardless of context. Its when a member of a population leaves a population. One of the examples your will see in Microsoft AzureML and in many data science textbooks out there is ‘customer’ churn. This is from the marketing context. In many businesses such a cell phone companies and others, it is far harder to generate and attract new customers than it is to keep old ones. So businesses want to do what they can to keep existing customers. When they leave, that is ‘customer churn’ for that particular company.\nThere is applicability of this kind of thinking and mindset to Human Resources in an organization as well. It is far less expensive to ‘keep’ good employees once you have them, then the cost of attracting and training new ones. Hmmmmm- a marketing principle that applies to the management of human resources, and a data science set of algorithms that can help determine whether there are patterns of churn in our data that could help predict future churn.\nHR truly needs to start thinking outside of its traditional thinking and methodologies to powerfully address the HR challenges and issues in the future\nAs I indicated in my previous article (mentioned above)- on a personal level I like to think of People Analytics as when the data science process is applied to HR information. For that reason i would to revisit what that process is and use it as the framework to guide the rest of the example illustrated in this blog article."
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#first-look-at-the-data--the-structure",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#first-look-at-the-data--the-structure",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "First Look at The Data- The Structure",
    "text": "First Look at The Data- The Structure\nLets load in the data. (By the way, the data below is totally contrived)\n\n# Load an R data frame.\nlibrary(readr)\nMFG10YearTerminationData &lt;- read_csv(\"MFG10YearTerminationData.csv\", \n    col_types = cols(gender_full = col_factor(levels = c(\"Male\", \n        \"Female\")), STATUS = col_factor(levels = c(\"ACTIVE\", \n        \"TERMINATED\")), BUSINESS_UNIT = col_factor(levels = c(\"HEADOFFICE\", \n        \"STORES\"))))\nMYdataset &lt;- MFG10YearTerminationData\nstr(MYdataset)\n\nspc_tbl_ [49,653 × 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ EmployeeID         : num [1:49653] 1318 1318 1318 1318 1318 ...\n $ recorddate_key     : chr [1:49653] \"12/31/2006 0:00\" \"12/31/2007 0:00\" \"12/31/2008 0:00\" \"12/31/2009 0:00\" ...\n $ birthdate_key      : Date[1:49653], format: \"1954-01-03\" \"1954-01-03\" ...\n $ orighiredate_key   : Date[1:49653], format: \"1989-08-28\" \"1989-08-28\" ...\n $ terminationdate_key: Date[1:49653], format: \"1900-01-01\" \"1900-01-01\" ...\n $ age                : num [1:49653] 52 53 54 55 56 57 58 59 60 61 ...\n $ length_of_service  : num [1:49653] 17 18 19 20 21 22 23 24 25 26 ...\n $ city_name          : chr [1:49653] \"Vancouver\" \"Vancouver\" \"Vancouver\" \"Vancouver\" ...\n $ department_name    : chr [1:49653] \"Executive\" \"Executive\" \"Executive\" \"Executive\" ...\n $ job_title          : chr [1:49653] \"CEO\" \"CEO\" \"CEO\" \"CEO\" ...\n $ store_name         : chr [1:49653] \"035\" \"035\" \"035\" \"035\" ...\n $ gender_short       : chr [1:49653] \"M\" \"M\" \"M\" \"M\" ...\n $ gender_full        : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ termreason_desc    : chr [1:49653] \"Not Applicable\" \"Not Applicable\" \"Not Applicable\" \"Not Applicable\" ...\n $ termtype_desc      : chr [1:49653] \"Not Applicable\" \"Not Applicable\" \"Not Applicable\" \"Not Applicable\" ...\n $ STATUS_YEAR        : num [1:49653] 2006 2007 2008 2009 2010 ...\n $ STATUS             : Factor w/ 2 levels \"ACTIVE\",\"TERMINATED\": 1 1 1 1 1 1 1 1 1 1 ...\n $ BUSINESS_UNIT      : Factor w/ 2 levels \"HEADOFFICE\",\"STORES\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   EmployeeID = col_double(),\n  ..   recorddate_key = col_character(),\n  ..   birthdate_key = col_date(format = \"\"),\n  ..   orighiredate_key = col_date(format = \"\"),\n  ..   terminationdate_key = col_date(format = \"\"),\n  ..   age = col_double(),\n  ..   length_of_service = col_double(),\n  ..   city_name = col_character(),\n  ..   department_name = col_character(),\n  ..   job_title = col_character(),\n  ..   store_name = col_character(),\n  ..   gender_short = col_character(),\n  ..   gender_full = col_factor(levels = c(\"Male\", \"Female\"), ordered = FALSE, include_na = FALSE),\n  ..   termreason_desc = col_character(),\n  ..   termtype_desc = col_character(),\n  ..   STATUS_YEAR = col_double(),\n  ..   STATUS = col_factor(levels = c(\"ACTIVE\", \"TERMINATED\"), ordered = FALSE, include_na = FALSE),\n  ..   BUSINESS_UNIT = col_factor(levels = c(\"HEADOFFICE\", \"STORES\"), ordered = FALSE, include_na = FALSE)\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nlibrary(plyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#second-look-at-the-data--data-quality",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#second-look-at-the-data--data-quality",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "Second Look at The Data- Data Quality",
    "text": "Second Look at The Data- Data Quality\n\nsummary(MYdataset)\n\n   EmployeeID   recorddate_key     birthdate_key        orighiredate_key    \n Min.   :1318   Length:49653       Min.   :1941-01-15   Min.   :1989-08-28  \n 1st Qu.:3360   Class :character   1st Qu.:1958-05-28   1st Qu.:1995-06-02  \n Median :5031   Mode  :character   Median :1968-12-04   Median :2000-03-31  \n Mean   :4859                      Mean   :1969-01-09   Mean   :2000-09-04  \n 3rd Qu.:6335                      3rd Qu.:1979-07-18   3rd Qu.:2005-10-13  \n Max.   :8336                      Max.   :1994-12-31   Max.   :2013-12-11  \n terminationdate_key       age        length_of_service  city_name        \n Min.   :1900-01-01   Min.   :19.00   Min.   : 0.00     Length:49653      \n 1st Qu.:1900-01-01   1st Qu.:31.00   1st Qu.: 5.00     Class :character  \n Median :1900-01-01   Median :42.00   Median :10.00     Mode  :character  \n Mean   :1916-05-10   Mean   :42.08   Mean   :10.43                       \n 3rd Qu.:1900-01-01   3rd Qu.:53.00   3rd Qu.:15.00                       \n Max.   :2015-12-30   Max.   :65.00   Max.   :26.00                       \n department_name     job_title          store_name        gender_short      \n Length:49653       Length:49653       Length:49653       Length:49653      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n gender_full    termreason_desc    termtype_desc       STATUS_YEAR  \n Male  :23755   Length:49653       Length:49653       Min.   :2006  \n Female:25898   Class :character   Class :character   1st Qu.:2008  \n                Mode  :character   Mode  :character   Median :2011  \n                                                      Mean   :2011  \n                                                      3rd Qu.:2013  \n                                                      Max.   :2015  \n        STATUS         BUSINESS_UNIT  \n ACTIVE    :48168   HEADOFFICE:  585  \n TERMINATED: 1485   STORES    :49068  \n                                      \n                                      \n                                      \n                                      \n\n\nA cursory look at the above summary doesnt have anything jump out as being data quality issues."
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#third-look-at-the-data---generally-what-is-the-data-telling-us",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#third-look-at-the-data---generally-what-is-the-data-telling-us",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "Third Look at the Data - Generally What Is The Data Telling Us?",
    "text": "Third Look at the Data - Generally What Is The Data Telling Us?\nEarlier we had indicated that we had both active records at end of year and terminates during the year for each of 10 years going from 2006 to 2015. To have a population to model from (to differentiate ACTIVES from TERMINATES) we have to include both status types .\nIts useful then to get a baseline of what percent/proportion the terminates are of the entire population. It also answers our first question. Let’s look at that next.\nWhat proportion of our staff are leaving?\n\nStatusCount&lt;- as.data.frame.matrix(MYdataset %&gt;%\n                              group_by(STATUS_YEAR) %&gt;%\n                              select(STATUS) %&gt;%\n                              table())\n\nAdding missing grouping variables: `STATUS_YEAR`\n\nStatusCount$TOTAL&lt;-StatusCount$ACTIVE + StatusCount$TERMINATED\nStatusCount$PercentTerminated &lt;-StatusCount$TERMINATED/(StatusCount$TOTAL)*100\nStatusCount\n\n     ACTIVE TERMINATED TOTAL PercentTerminated\n2006   4445        134  4579          2.926403\n2007   4521        162  4683          3.459321\n2008   4603        164  4767          3.440319\n2009   4710        142  4852          2.926628\n2010   4840        123  4963          2.478340\n2011   4972        110  5082          2.164502\n2012   5101        130  5231          2.485184\n2013   5215        105  5320          1.973684\n2014   4962        253  5215          4.851390\n2015   4799        162  4961          3.265471\n\nmean(StatusCount$PercentTerminated)\n\n[1] 2.997124\n\n\nIt looks like it ranges from 1.97 to 4.85% with an average of 2.99%\nWhere are the terminations occurring?\nLets look at some charts\nBy Business Unit\n\nlibrary(ggplot2)\nggplot() + geom_bar(aes(y = ..count..,x =as.factor(BUSINESS_UNIT),fill = as.factor(STATUS)),data=MYdataset,position = position_stack())\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nIt looks like terminates is the last 10 years have predominantly occurred in the STORES business unit. Only 1 terminate in HR Technology which is in the head office.\nLets explore just the terminates for a few moments.\nJust Terminates By Termination Type And Status Year\n\nTerminatesData&lt;- as.data.frame(MYdataset %&gt;%\n                          filter(STATUS==\"TERMINATED\"))\n\nggplot() + geom_bar(aes(y = ..count..,x =as.factor(STATUS_YEAR),fill = as.factor(termtype_desc)),data=TerminatesData,position = position_stack())\n\n\n\n\nGenerally most terminations seem to be voluntary year by year,except in the most recent years where is are some involutary terminates.\nJust Terminates By Status Year and Termination Reason\n\nggplot() + geom_bar(aes(y = ..count..,x =as.factor(STATUS_YEAR),fill = as.factor(termreason_desc)),data=TerminatesData, position = position_stack())\n\n\n\n\nIt seems that there were layoffs in 2014 and 2015 which accounts for the involuntary terminates.\nJust Terminates By Termination Reason and Department\n\nggplot() + geom_bar(aes(y = ..count..,x =as.factor(department_name),fill = as.factor(termreason_desc)),data=TerminatesData,position = position_stack())+\n  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))\n\n\n\n\nWhen we look at the terminate by Department, a few thing stick out. Customer Service has a much larger proportion of resignation compared to other departments. And retirement in general is high is a number of departments.\nHow does Age and Length of Service affect termination?\n\nlibrary(caret)\n\nLoading required package: lattice\n\nfeaturePlot(x=MYdataset[,6:7],y=MYdataset$STATUS,plot=\"density\",auto.key = list(columns = 2))\n\n\n\n\nDensity plots show some interesting things. For terminates there is some elevation from 20 to 30 and a spike at 60. For length of service there are 5 spikes. One around 1 year, another one around 5 years, and a big one around 15 year, and a couple at 20 and 25 years.\nAge and Length of Service Distributions By Status\n\nfeaturePlot(x=MYdataset[,6:7],y=MYdataset$STATUS,plot=\"box\",auto.key = list(columns = 2))\n\n\n\n\nBoxplots show high average age for terminates as compared to active. Length of service shows not much difference between active and terminated.\nThat’s a brief general look at some of what the data is telling us. Our next step of course is model building."
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#lets-partition-the-data",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#lets-partition-the-data",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "Let’s Partition The Data",
    "text": "Let’s Partition The Data\n\nlibrary(rattle)   \n\nLoading required package: tibble\n\n\nLoading required package: bitops\n\n\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n\nlibrary(magrittr) # For the %&gt;% and %&lt;&gt;% operators.\n\n\n\nbuilding &lt;- TRUE\nscoring  &lt;- ! building\n\n\n# A pre-defined value is used to reset the random seed so that results are repeatable.\n\ncrv$seed &lt;- 42 \n\n\n# Load an R data frame.\nlibrary(readr)\nMFG10YearTerminationData &lt;- read_csv(\"MFG10YearTerminationData.csv\", \n    col_types = cols(gender_full = col_factor(levels = c(\"Male\", \n        \"Female\")), STATUS = col_factor(levels = c(\"ACTIVE\", \n        \"TERMINATED\")), BUSINESS_UNIT = col_factor(levels = c(\"HEADOFFICE\", \n        \"STORES\"))))\n\nMYdataset &lt;- MFG10YearTerminationData\n\n\n\n#Create training and testing datasets\n\n#Create training and testing datasets\n\nset.seed(crv$seed) \nMYnobs &lt;- nrow(MYdataset) # 52692 observations \nMYsample &lt;- MYtrain &lt;- subset(MYdataset,STATUS_YEAR&lt;=2014)\nMYvalidate &lt;- NULL\nMYtest &lt;- subset(MYdataset,STATUS_YEAR== 2015)\n\n# The following variable selections have been noted.\n\nMYinput &lt;- c(\"age\", \"length_of_service\",    \"gender_full\",\n               \"STATUS_YEAR\", \"BUSINESS_UNIT\")\n\nMYnumeric &lt;- c(\"age\", \"length_of_service\", \"STATUS_YEAR\")\n\nMYcategoric &lt;- c(  \n                   \"gender_full\", \"BUSINESS_UNIT\")\n\nMYtarget  &lt;- \"STATUS\"\nMYrisk    &lt;- NULL\nMYident   &lt;- \"EmployeeID\"\nMYignore  &lt;- c(\"recorddate_key\", \"birthdate_key\", \"orighiredate_key\", \"terminationdate_key\", \"city_name\", \"gender_short\", \"termreason_desc\", \"termtype_desc\",\"department_name\",\n               \"job_title\", \"store_name\")\nMYweights &lt;- NULL\n\nMYTrainingData&lt;-MYtrain[c(MYinput, MYtarget)]\nMYTestingData&lt;-MYtest[c(MYinput, MYtarget)]"
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#choosing-and-running-models",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#choosing-and-running-models",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "Choosing and Running Models",
    "text": "Choosing and Running Models\nOne of the things that characterizes R, is that the number of functions and procedures that can be used are huge. So there often many ways of doing things. Two of the best R packages designed to be used for data science are caret and rattle.\nWe introduced caret in the last blog article. In this one I will use rattle. What is noteworthy about rattle is that it provides a GUI front end and generates the code for it in the log on the backend. So you can generate models quickly.\nI wont be illustrating how to use rattle in this article as a GUI, but rather show the code it generated along with the statistical results and graphs. Please dont get hung up/turned off by the code presented. The GUI front end generated all the code below. I simply made cosmetic changes to it. Please do concentrate on the flow of the data science process in the article as one example of how it can be done.. As a GUI rattle was able to generate all the below output in about 15 minutes of my effort. One tutorial on the rattle GUI cam be found here:\nhttp://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/en_Tanagra_Rattle_Package_for_R.pdf\nAnd here is a book on rattle:\nhttp://www.amazon.com/gp/product/1441998896/ref=as_li_qf_sp_asin_tl?ie=UTF8&tag=togaware-20&linkCode=as2&camp=217145&creative=399373&creativeASIN=1441998896\nWe should step back for a moment and review what are doing here, and what are opening questions were. We are wanting to predict who might terminate in the future. That is a ‘binary result’ or ‘category’. A person is either ‘ACTIVE’ or ‘TERMINATED’. __Since it is a category to be predicted we will choose among models/algorithms that can predict categories._\nThe models we will look at in rattle are:\n\nDecision Trees (rpart)\nBoosted Models (adaboost)\nRandom Forests (rf)\nSupport Vactor Models (svm)\nLinear Models (glm)\n\n\nDecision Tree\nLets first u take a look at a decision tree model. This is always useful because with these, you can get a visual tree model to get some idea of how the prediction occurs in an easy to understand way.\n\nlibrary(rattle)\nlibrary(rpart, quietly=TRUE)\n\n# Reset the random number seed to obtain the same results each time.\n\nset.seed(crv$seed)\n\n# Build the Decision Tree model.\n\nMYrpart &lt;- rpart(STATUS ~ .,\n                   data=MYtrain[, c(MYinput, MYtarget)],\n                   method=\"class\",\n                   parms=list(split=\"information\"),\n                   control=rpart.control(usesurrogate=0, \n                                         maxsurrogate=0))\n\n# Generate a textual view of the Decision Tree model.\n\n#print(MYrpart)\n#printcp(MYrpart)\n#cat(\"\\n\")\n\n# Time taken: 0.63 secs\n\n#============================================================\n# Rattle timestamp: 2016-03-25 09:45:25 x86_64-w64-mingw32 \n\n# Plot the resulting Decision Tree. \n\n# We use the rpart.plot package.\n\nfancyRpartPlot(MYrpart, main=\"Decision Tree MFG10YearTerminationData $ STATUS\")\n\n\n\n\nWe can now answer our next guestion from above:\nWhat, if anything, else contributes to it?\nFrom even the graphical tree output it looks like gender, and status year also affect it.\n\n\nRandom Forests\nNow for Random Forests\n\n#============================================================\n# Rattle timestamp: 2016-03-25 18:21:29 x86_64-w64-mingw32 \n\n# Random Forest \n\n# The 'randomForest' package provides the 'randomForest' function.\n\nlibrary(randomForest, quietly=TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Build the Random Forest model.\n\nset.seed(crv$seed)\nMYrf &lt;- randomForest::randomForest(STATUS ~ .,\n                                   data=MYtrain[c(MYinput, MYtarget)],\n                                   ntree=500,\n                                   mtry=2,\n                                   importance=TRUE,\n                                   na.action=randomForest::na.roughfix,\n                                   replace=FALSE)\n\n# Generate textual output of 'Random Forest' model.\n\nMYrf\n\n\nCall:\n randomForest(formula = STATUS ~ ., data = MYtrain[c(MYinput,      MYtarget)], ntree = 500, mtry = 2, importance = TRUE, replace = FALSE,      na.action = randomForest::na.roughfix) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 1.13%\nConfusion matrix:\n           ACTIVE TERMINATED  class.error\nACTIVE      43366          3 6.917383e-05\nTERMINATED    501        822 3.786848e-01\n\n# The `pROC' package implements various AUC functions.\n\n# Calculate the Area Under the Curve (AUC).\n\npROC::roc(MYrf$y, as.numeric(MYrf$predicted))\n\nSetting levels: control = ACTIVE, case = TERMINATED\n\n\nSetting direction: controls &lt; cases\n\n\n\nCall:\nroc.default(response = MYrf$y, predictor = as.numeric(MYrf$predicted))\n\nData: as.numeric(MYrf$predicted) in 43369 controls (MYrf$y ACTIVE) &lt; 1323 cases (MYrf$y TERMINATED).\nArea under the curve: 0.8106\n\n# Calculate the AUC Confidence Interval.\n\npROC::ci.auc(MYrf$y, as.numeric(MYrf$predicted))\n\nSetting levels: control = ACTIVE, case = TERMINATED\nSetting direction: controls &lt; cases\n\n\n95% CI: 0.7975-0.8237 (DeLong)\n\n# List the importance of the variables.\n\nrn &lt;- round(randomForest::importance(MYrf), 2)\nrn[order(rn[,3], decreasing=TRUE),]\n\n                  ACTIVE TERMINATED MeanDecreaseAccuracy MeanDecreaseGini\nage                36.57     156.31                53.12           744.59\ngender_full        27.22      39.05                35.50            76.76\nSTATUS_YEAR        29.99      32.35                34.69            61.70\nlength_of_service  17.68      17.75                20.94            95.03\nBUSINESS_UNIT       5.48       7.43                 7.28             3.72\n\n# Time taken: 18.66 secs\n\n\n\nAdaboost\nNow for adaboost\n\n#============================================================\n# Rattle timestamp: 2016-03-25 18:22:22 x86_64-w64-mingw32 \n\n# Ada Boost \n\n# The `ada' package implements the boost algorithm.\n\n# Build the Ada Boost model.\n\nset.seed(crv$seed)\nMYada &lt;- ada::ada(STATUS ~ .,\n                  data=MYtrain[c(MYinput, MYtarget)],\n                  control=rpart::rpart.control(maxdepth=30,\n                                               cp=0.010000,\n                                               minsplit=20,\n                                               xval=10),\n                  iter=50)\n\n# Print the results of the modelling.\n\nprint(MYada)\n\nCall:\nada(STATUS ~ ., data = MYtrain[c(MYinput, MYtarget)], control = rpart::rpart.control(maxdepth = 30, \n    cp = 0.01, minsplit = 20, xval = 10), iter = 50)\n\nLoss: exponential Method: discrete   Iteration: 50 \n\nFinal Confusion Matrix for Data:\n            Final Prediction\nTrue value   ACTIVE TERMINATED\n  ACTIVE      43366          3\n  TERMINATED    501        822\n\nTrain Error: 0.011 \n\nOut-Of-Bag Error:  0.011  iteration= 6 \n\nAdditional Estimates of number of iterations:\n\ntrain.err1 train.kap1 \n         1          1 \n\nround(MYada$model$errs[MYada$iter,], 2)\n\ntrain.err train.kap \n     0.01      0.24 \n\ncat('Variables actually used in tree construction:\\n')\n\nVariables actually used in tree construction:\n\nprint(sort(names(listAdaVarsUsed(MYada))))\n\n[1] \"age\"               \"gender_full\"       \"length_of_service\"\n[4] \"STATUS_YEAR\"      \n\ncat('\\nFrequency of variables actually used:\\n')\n\n\nFrequency of variables actually used:\n\nprint(listAdaVarsUsed(MYada))\n\n\n              age       STATUS_YEAR length_of_service       gender_full \n               45                40                39                25 \n\n\n\n\nSupport Vector Machines\nNow lets look at Support Vector Machines\n\n#============================================================\n# Rattle timestamp: 2016-03-25 18:22:56 x86_64-w64-mingw32 \n\n# Support vector machine. \n\n# The 'kernlab' package provides the 'ksvm' function.\n\nlibrary(kernlab, quietly=TRUE)\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n# Build a Support Vector Machine model.\n\nset.seed(crv$seed)\nMYksvm &lt;- ksvm(as.factor(STATUS) ~ .,\n               data=MYtrain[c(MYinput, MYtarget)],\n               kernel=\"rbfdot\",\n               prob.model=TRUE)\n\n# Generate a textual view of the SVM model.\n\nMYksvm\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  0.370016942069632 \n\nNumber of Support Vectors : 2425 \n\nObjective Function Value : -1999.052 \nTraining error : 0.017811 \nProbability model included. \n\n# Time taken: 42.91 secs\n\n\n\nLinear Models\nFinally lets look at linear models.\n\n#============================================================\n# Rattle timestamp: 2016-03-25 18:23:56 x86_64-w64-mingw32 \n\n# Regression model \n\n# Build a Regression model.\n\nMYglm &lt;- glm(STATUS ~ .,\n             data=MYtrain[c(MYinput, MYtarget)],\n             family=binomial(link=\"logit\"))\n\n# Generate a textual view of the Linear model.\n\nprint(summary(MYglm))\n\n\nCall:\nglm(formula = STATUS ~ ., family = binomial(link = \"logit\"), \n    data = MYtrain[c(MYinput, MYtarget)])\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3245  -0.2076  -0.1564  -0.1184   3.4080  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -892.99983   33.95417 -26.300  &lt; 2e-16 ***\nage                    0.21944    0.00438  50.095  &lt; 2e-16 ***\nlength_of_service     -0.43146    0.01086 -39.738  &lt; 2e-16 ***\ngender_fullFemale     -0.51900    0.06766  -7.671  1.7e-14 ***\nSTATUS_YEAR            0.44122    0.01687  26.148  &lt; 2e-16 ***\nBUSINESS_UNITSTORES   -2.73943    0.16616 -16.486  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11920.1  on 44691  degrees of freedom\nResidual deviance:  9053.3  on 44686  degrees of freedom\nAIC: 9065.3\n\nNumber of Fisher Scoring iterations: 7\n\ncat(sprintf(\"Log likelihood: %.3f (%d df)\\n\",\n            logLik(MYglm)[1],\n            attr(logLik(MYglm), \"df\")))\n\nLog likelihood: -4526.633 (6 df)\n\ncat(sprintf(\"Null/Residual deviance difference: %.3f (%d df)\\n\",\n            MYglm$null.deviance-MYglm$deviance,\n            MYglm$df.null-MYglm$df.residual))\n\nNull/Residual deviance difference: 2866.813 (5 df)\n\ncat(sprintf(\"Chi-square p-value: %.8f\\n\",\n            dchisq(MYglm$null.deviance-MYglm$deviance,\n                   MYglm$df.null-MYglm$df.residual)))\n\nChi-square p-value: 0.00000000\n\ncat(sprintf(\"Pseudo R-Square (optimistic): %.8f\\n\",\n            cor(MYglm$y, MYglm$fitted.values)))\n\nPseudo R-Square (optimistic): 0.38428451\n\ncat('\\n==== ANOVA ====\\n\\n')\n\n\n==== ANOVA ====\n\nprint(anova(MYglm, test=\"Chisq\"))\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: STATUS\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                              44691    11920.1              \nage                1   861.75     44690    11058.3 &lt; 2.2e-16 ***\nlength_of_service  1  1094.72     44689     9963.6 &lt; 2.2e-16 ***\ngender_full        1    14.38     44688     9949.2 0.0001494 ***\nSTATUS_YEAR        1   716.39     44687     9232.8 &lt; 2.2e-16 ***\nBUSINESS_UNIT      1   179.57     44686     9053.3 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n\")\n# Time taken: 1.62 secs\n\nThese were simply the vanilla running of these models.In evaluating the models we have the means to compare their results on a common basis."
  },
  {
    "objectID": "posts/PAExampleChurn/PAExampleChurn.html#evaluate-models",
    "href": "posts/PAExampleChurn/PAExampleChurn.html#evaluate-models",
    "title": "People Analytics Using R - Employee Churn- An Example",
    "section": "Evaluate Models",
    "text": "Evaluate Models\nIn the evaluating models step, we are able to answer our final 2 original questions stated at the beginning:\nCan we predict?\nIn a word ‘yes’.\nHow Well can we predict?\nIn two words ‘fairly well’.\nWhen it comes to evaluating models for predicting categories, we are defining accuracy as to how many times did the model predict the actual. So we are interested in a number of things.\nThe first of these are error martricies. In error matricies, you are cross tabulating the actual results with predicted results. If prediction was ‘perfect’ 100%, every prediction would be the same as actual. (almost never happens). The higher the correct prediction rate and lower the error rate- the better.\n\nError Matricies\n\nDecision Trees\n\n#============================================================\n# Rattle timestamp: 2016-03-25 18:50:22 x86_64-w64-mingw32 \n\n# Evaluate model performance. \n\n# Generate an Error Matrix for the Decision Tree model.\n\n# Obtain the response from the Decision Tree model.\n\nMYpr &lt;- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)], type=\"class\")\n\n# Generate the confusion matrix showing counts.\n\ntable(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,\n      dnn=c(\"Actual\", \"Predicted\"))\n\n            Predicted\nActual       ACTIVE TERMINATED\n  ACTIVE       4799          0\n  TERMINATED     99         63\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)\nround(per, 2)\n\n            Predicted\nActual       ACTIVE TERMINATED Error\n  ACTIVE       0.97       0.00  0.00\n  TERMINATED   0.02       0.01  0.61\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n2\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n30\n\n\n\n\nAdaboost\n\n# Generate an Error Matrix for the Ada Boost model.\n\n# Obtain the response from the Ada Boost model.\n\nMYpr &lt;- predict(MYada, newdata=MYtest[c(MYinput, MYtarget)])\n\n# Generate the confusion matrix showing counts.\n\ntable(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,\n      dnn=c(\"Actual\", \"Predicted\"))\n\n            Predicted\nActual       ACTIVE TERMINATED\n  ACTIVE       4799          0\n  TERMINATED     99         63\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)\nround(per, 2)\n\n            Predicted\nActual       ACTIVE TERMINATED Error\n  ACTIVE       0.97       0.00  0.00\n  TERMINATED   0.02       0.01  0.61\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n2\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n30\n\n\n\n\nRandom Forest\n\n# Generate an Error Matrix for the Random Forest model.\n\n# Obtain the response from the Random Forest model.\n\nMYpr &lt;- predict(MYrf, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,\n      dnn=c(\"Actual\", \"Predicted\"))\n\n            Predicted\nActual       ACTIVE TERMINATED\n  ACTIVE       4799          0\n  TERMINATED     99         63\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)\nround(per, 2)\n\n            Predicted\nActual       ACTIVE TERMINATED Error\n  ACTIVE       0.97       0.00  0.00\n  TERMINATED   0.02       0.01  0.61\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n2\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n30\n\n\n\n\nSupport Vector Model\n\n# Generate an Error Matrix for the SVM model.\n\n# Obtain the response from the SVM model.\n\nMYpr &lt;- kernlab::predict(MYksvm, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,\n      dnn=c(\"Actual\", \"Predicted\"))\n\n            Predicted\nActual       ACTIVE TERMINATED\n  ACTIVE       4799          0\n  TERMINATED    150         12\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)\nround(per, 2)\n\n            Predicted\nActual       ACTIVE TERMINATED Error\n  ACTIVE       0.97          0  0.00\n  TERMINATED   0.03          0  0.93\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n3\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n47\n\n\n\n\nLinear Model\n\n# Generate an Error Matrix for the Linear model.\n\n# Obtain the response from the Linear model.\n\nMYpr &lt;- as.vector(ifelse(predict(MYglm, type=\"response\", newdata=MYtest[c(MYinput, MYtarget)]) &gt; 0.5, \"TERMINATED\", \"ACTIVE\"))\n\n# Generate the confusion matrix showing counts.\n\ntable(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,\n      dnn=c(\"Actual\", \"Predicted\"))\n\n            Predicted\nActual       ACTIVE\n  ACTIVE       4799\n  TERMINATED    162\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)\nround(per, 2)\n\n            Predicted\nActual       ACTIVE Error\n  ACTIVE       0.97     0\n  TERMINATED   0.03     1\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n-97\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n50\n\n\nWell that was interesting!\nSummarizing the confusion matrix showed that decision trees,random forests, and adaboost all predicted similarly. BUT Support Vector Machines and the Linear Models did worse for this data.\n###Area Under Curve (AUC)\nAnother way to evaluate the models is the AUC. The higher the AUC the better. The code below generates the information necessary to produce the graphs.\n\n#============================================================\n# Rattle timestamp: 2016-03-25 19:44:22 x86_64-w64-mingw32 \n\n# Evaluate model performance. \n\n# ROC Curve: requires the ROCR package.\n\nlibrary(ROCR)\n\n# ROC Curve: requires the ggplot2 package.\n\nlibrary(ggplot2, quietly=TRUE)\n\n# Generate an ROC Curve for the rpart model on MFG10YearTerminationData [test].\n\nMYpr &lt;- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)])[,2]\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\n\npe &lt;- performance(pred, \"tpr\", \"fpr\")\nau &lt;- performance(pred, \"auc\")@y.values[[1]]\npd &lt;- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))\np &lt;- ggplot(pd, aes(x=fpr, y=tpr))\np &lt;- p + geom_line(colour=\"red\")\np &lt;- p + xlab(\"False Positive Rate\") + ylab(\"True Positive Rate\")\np &lt;- p + ggtitle(\"ROC Curve Decision Tree MFG10YearTerminationData [test] STATUS\")\np &lt;- p + theme(plot.title=element_text(size=10))\np &lt;- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\")\np &lt;- p + annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n                  label=paste(\"AUC =\", round(au, 2)))\nprint(p)\n\n\n\n# Calculate the area under the curve for the plot.\n\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\nperformance(pred, \"auc\")\n\nA performance instance\n  'Area under the ROC curve'\n\n# ROC Curve: requires the ROCR package.\n\nlibrary(ROCR)\n\n# ROC Curve: requires the ggplot2 package.\n\nlibrary(ggplot2, quietly=TRUE)\n\n# Generate an ROC Curve for the ada model on MFG10YearTerminationData [test].\n\nMYpr &lt;- predict(MYada, newdata=MYtest[c(MYinput, MYtarget)], type=\"prob\")[,2]\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\n\npe &lt;- performance(pred, \"tpr\", \"fpr\")\nau &lt;- performance(pred, \"auc\")@y.values[[1]]\npd &lt;- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))\np &lt;- ggplot(pd, aes(x=fpr, y=tpr))\np &lt;- p + geom_line(colour=\"red\")\np &lt;- p + xlab(\"False Positive Rate\") + ylab(\"True Positive Rate\")\np &lt;- p + ggtitle(\"ROC Curve Ada Boost MFG10YearTerminationData [test] STATUS\")\np &lt;- p + theme(plot.title=element_text(size=10))\np &lt;- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\")\np &lt;- p + annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n                  label=paste(\"AUC =\", round(au, 2)))\nprint(p)\n\n\n\n# Calculate the area under the curve for the plot.\n\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\nperformance(pred, \"auc\")\n\nA performance instance\n  'Area under the ROC curve'\n\n# ROC Curve: requires the ROCR package.\n\nlibrary(ROCR)\n\n# ROC Curve: requires the ggplot2 package.\n\nlibrary(ggplot2, quietly=TRUE)\n\n# Generate an ROC Curve for the rf model on MFG10YearTerminationData [test].\n\nMYpr &lt;- predict(MYrf, newdata=na.omit(MYtest[c(MYinput, MYtarget)]), type=\"prob\")[,2]\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\n\npe &lt;- performance(pred, \"tpr\", \"fpr\")\nau &lt;- performance(pred, \"auc\")@y.values[[1]]\npd &lt;- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))\np &lt;- ggplot(pd, aes(x=fpr, y=tpr))\np &lt;- p + geom_line(colour=\"red\")\np &lt;- p + xlab(\"False Positive Rate\") + ylab(\"True Positive Rate\")\np &lt;- p + ggtitle(\"ROC Curve Random Forest MFG10YearTerminationData [test] STATUS\")\np &lt;- p + theme(plot.title=element_text(size=10))\np &lt;- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\")\np &lt;- p + annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n                  label=paste(\"AUC =\", round(au, 2)))\nprint(p)\n\n\n\n# Calculate the area under the curve for the plot.\n\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\nperformance(pred, \"auc\")\n\nA performance instance\n  'Area under the ROC curve'\n\n# ROC Curve: requires the ROCR package.\n\nlibrary(ROCR)\n\n# ROC Curve: requires the ggplot2 package.\n\nlibrary(ggplot2, quietly=TRUE)\n\n# Generate an ROC Curve for the ksvm model on MFG10YearTerminationData [test].\n\nMYpr &lt;- kernlab::predict(MYksvm, newdata=na.omit(MYtest[c(MYinput, MYtarget)]), type=\"probabilities\")[,2]\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\n\npe &lt;- performance(pred, \"tpr\", \"fpr\")\nau &lt;- performance(pred, \"auc\")@y.values[[1]]\npd &lt;- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))\np &lt;- ggplot(pd, aes(x=fpr, y=tpr))\np &lt;- p + geom_line(colour=\"red\")\np &lt;- p + xlab(\"False Positive Rate\") + ylab(\"True Positive Rate\")\np &lt;- p + ggtitle(\"ROC Curve SVM MFG10YearTerminationData [test] STATUS\")\np &lt;- p + theme(plot.title=element_text(size=10))\np &lt;- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\")\np &lt;- p + annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n                  label=paste(\"AUC =\", round(au, 2)))\nprint(p)\n\n\n\n# Calculate the area under the curve for the plot.\n\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\nperformance(pred, \"auc\")\n\nA performance instance\n  'Area under the ROC curve'\n\n# ROC Curve: requires the ROCR package.\n\nlibrary(ROCR)\n\n# ROC Curve: requires the ggplot2 package.\n\nlibrary(ggplot2, quietly=TRUE)\n\n# Generate an ROC Curve for the glm model on MFG10YearTerminationData [test].\n\nMYpr &lt;- predict(MYglm, type=\"response\", newdata=MYtest[c(MYinput, MYtarget)])\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\n\npe &lt;- performance(pred, \"tpr\", \"fpr\")\nau &lt;- performance(pred, \"auc\")@y.values[[1]]\npd &lt;- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))\np &lt;- ggplot(pd, aes(x=fpr, y=tpr))\np &lt;- p + geom_line(colour=\"red\")\np &lt;- p + xlab(\"False Positive Rate\") + ylab(\"True Positive Rate\")\np &lt;- p + ggtitle(\"ROC Curve Linear MFG10YearTerminationData [test] STATUS\")\np &lt;- p + theme(plot.title=element_text(size=10))\np &lt;- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\")\np &lt;- p + annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n                  label=paste(\"AUC =\", round(au, 2)))\nprint(p)\n\n\n\n# Calculate the area under the curve for the plot.\n\n\n# Remove observations with missing target.\n\nno.miss   &lt;- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)\nmiss.list &lt;- attr(no.miss, \"na.action\")\nattributes(no.miss) &lt;- NULL\n\nif (length(miss.list))\n{\n  pred &lt;- prediction(MYpr[-miss.list], no.miss)\n} else\n{\n  pred &lt;- prediction(MYpr, no.miss)\n}\nperformance(pred, \"auc\")\n\nA performance instance\n  'Area under the ROC curve'\n\n\nA couple of things to notice:\n\nIt turns out that the adaboost model produces the highest AUC. So we will use it to predict the 2016 terminates in just a little bit.\nThe Linear model was worst."
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "",
    "text": "TL;DR; The HR Innovator Role is hard to find in organizations"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#be-browser-based",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#be-browser-based",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Be Browser Based",
    "text": "Be Browser Based\n\nWhat The User Sees\n\n\n\n\nWhat The Team Sees\n\n\n\n\nWhat The Administrator Sees"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-tracking-by-requester-and-the-organization",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-tracking-by-requester-and-the-organization",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Allow For Tracking By Requester And The Organization",
    "text": "Allow For Tracking By Requester And The Organization\n\nRequester\n\nList\n\n\n\n\nDetails\n\n\n\n\n\nOrganization\n\nList\n\n\n\n\nDetails"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-different-hr-teams-to-get-their-specific-requests-and-only-see-theirs.",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-different-hr-teams-to-get-their-specific-requests-and-only-see-theirs.",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Allow for different HR teams to get their specific requests and only see theirs.",
    "text": "Allow for different HR teams to get their specific requests and only see theirs.\n\nTeam\n\nList\n\n\n\n\nDetails\n\n\n\n\n\nIndividual Assigned To\n\nList\n\n\n\n\nDetails"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-current-status",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-current-status",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Show Current Status",
    "text": "Show Current Status"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#automatically-log-date-and-time-of-receipt",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#automatically-log-date-and-time-of-receipt",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Automatically Log Date and Time of Receipt",
    "text": "Automatically Log Date and Time of Receipt"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-attachments",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-attachments",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Allow For Attachments",
    "text": "Allow For Attachments"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#log-when-started-and-completed",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#log-when-started-and-completed",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Log When Started And Completed",
    "text": "Log When Started And Completed"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-history-of-who-it-was-assigned-to-and-when",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-history-of-who-it-was-assigned-to-and-when",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Show History of Who It Was Assigned To And When",
    "text": "Show History of Who It Was Assigned To And When"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-more-granular-steps-of-the-request-and-their-timing",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#show-more-granular-steps-of-the-request-and-their-timing",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Show More Granular Steps of The Request And Their Timing",
    "text": "Show More Granular Steps of The Request And Their Timing"
  },
  {
    "objectID": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-subsequent-notes-to-be-added",
    "href": "posts/HRInnovator/HR Innovator Caretaker Undertaker.html#allow-for-subsequent-notes-to-be-added",
    "title": "Are You An ‘HR’ Innovator or Caretaker or Undertaker?",
    "section": "Allow For Subsequent Notes To Be Added",
    "text": "Allow For Subsequent Notes To Be Added\n\n\nThe use of Service Request Tracking Systems- goes far beyond email:\n\nEvery request is logged.\nEvery request is tracked from inception to completion.\nCustomers(requesters), teams, and administrators all kept in loop on demand- with each entity seeing and modifying only what they are supposed to\nData is accumulated on an ongoing basis which can allow for measurement of how we are actually ‘performing’.\nWith that accumulated data and other analytical and statistical tools we can analyze\n\nthe volume of requests over time\nwhat types of requests they are\nwho’s worked on them\nhow long they sat in waiting\nhow long they took to perform\nwhat extra resources might be necessary to reduce wait time and for how long.\nstart seeing what can be done to reduce performance time by process/work redesign\n\n\nThose of you who have read my book\nDoing HR Analytics- A Practitioner’s Handbook With R Examples\nwill recall that one of the examples of HR Analytics was the measurement of HR operations with statistical process control charts. The types of data used come from Service Request Tracking Systems such as this."
  },
  {
    "objectID": "posts/PAExampleClassification/PAExampleClassification.html",
    "href": "posts/PAExampleClassification/PAExampleClassification.html",
    "title": "People Analytics in R - Job Classification ‘Revisited’",
    "section": "",
    "text": "About a year back I posted a couple of blog articles on Data Driven Job Classification, showing a variety of tools to do this including R, Azure machine Learning and a few others. Its purpose was to encourage HR folks to start thinking about HR’s future being more in the use of HR analytics.\nhttps://www.linkedin.com/pulse/data-driven-job-classification-lyndon-sundmark-mba?trk=mp-author-card\nAs the terminology in People Analytics is continuing to unfold and evolve, it is becoming increasing apparent that one of the best ways to understand People Analytics -both stringently and widely at the same time is to see it as:\n\n“Data Driven” HR and HR Decision Making\nWhen Data Science as a process and a field meets the HR context\n\nSeeing it this way helps prevent us from unnecessarily limiting the contribution that it can make, and yet at the same time help prevent proliferation of terminology to the point of meaninglessness. ‘Data driven’ must be how we conduct HR Management and decision making in the future. Data Science contributes to that goal by itself being a ‘data driven’ process.\nAnother way to not artificially limit the application of People Analytics, is to remind ourselves of the potential scope of the relevant HR context. People Analytics can be applied to:\n\ninformation on what is happening to employees in the organization over time. Typically this is thought of as HR metrics/demographics. Many still see this as the ‘extent’ of HR analytics.\nhow well the HR department conducts its business and operations. These are metrics related to process improvement. Right now this is more typically thought of in the ’quality improvement realm. But it’s still data driven decision making\n‘direct’ embedding of statistical algorithms in our HR methodologies- how we actually ‘do’ HR. There is huge application of People Analytics here. HR needs to get out of its traditional non-analytic methodologies paradigm where data science can be brought to bear.\n\nPeople Analytics in R- Job Classification is an example of embedding statistical algorithms in our HR methodologies.\nWith that in mind, I thought I would do a ‘revisit’ of job classification as an example of People Analytics in R. By revisit, I mean let’s restrict the tool to R, and let’s apply the data science process/framework to it. This would put into into a format similar to my last 2 blog articles. Additionally, in this article more time will be spent on describing how we generate the data in the first place.\nThe files for this currrent blog article can be found in this location:\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6433&authkey=!ABv-gHg5jVluYpc&ithint=folder%2cxlsx"
  },
  {
    "objectID": "posts/PAExampleClassification/PAExampleClassification.html#the-data-science-steps",
    "href": "posts/PAExampleClassification/PAExampleClassification.html#the-data-science-steps",
    "title": "People Analytics in R - Job Classification ‘Revisited’",
    "section": "The Data Science Steps",
    "text": "The Data Science Steps\n1. Define A Goal\n2. Collect And Manage Data\n3. Build the Model\n4. Evaluate And Critique Model\n5. Present Results and Document\n6. Deploy Model\n##1. Define A Goal\nOk - what is our goal here? Perhaps a quick primer on job classification would be in order to answer the question.\n\nA Quick Job Classification Primer\nJob Classification is at the heart of compensation and salary administration in HR. We desire to pay our employees fairly- both from an external and internal perspective. Salary surveys help us out on the external side. But job classification helps us out on the internal picture. We try to understand the similarities and differences between jobs in the organization.\nAt a base level, this process starts with documenting job descriptions. We document tasks, knowleges, and skills needed to complete the work of the organization as organized within our jobs. Usually as a result of job descriptions being documented, we design broader categories that the job descriptions fall into. We call these job classifications. They attempt to categorize like with like and distinguish between job descriptions that are different. We often are concerned with the characteristics of how responsibility, accountability, supervision, education level, experience etc vary between these classifications. And to tie into our compensations systems, we often have paygrades assigned to the classification.\nSo what is our goal in Job Classification? To properly categorize ‘job descriptions’ into appropriate job classifications. When job classifications are written they are, by definition, of a ‘known’, ‘intended’ population. Job descriptions until they are categorized are outside of that population. Once proper categorization is made of a job description, it becomes part of that ‘known’ population. It is ‘unknown’ by the population until then. When all the job descriptions are categorized into job classifications, both the job descriptions and the job classifications are part of the known population. Any new job description written in the future is unknown until it is classified as well.\nWhy is this significant to People Analytics? The above process indicates that we are trying to classify something, or find the right category, that we don’t know based on how it compares to a population we do know. HR job classification is the context here. It just so happens that in data science and statistics there are all sorts of algorithms designed to create categories or find the best fit among known categories.\nFor decades we have had job classification as a process in HR, and classification algorithms in statistics- but HR,in most organizations, is not recognizing this and the potential contribution it could/can make .\nSo in People Analytics in R -Job Classification:\n* our primary goal is to classify job descriptions into job classifications using the power of statistical algorithms to assist in prediction of best fit. * our secondary goal might be to help improve the design of our job classification system/framework."
  },
  {
    "objectID": "posts/PAExampleClassification/PAExampleClassification.html#collect-and-manage-data",
    "href": "posts/PAExampleClassification/PAExampleClassification.html#collect-and-manage-data",
    "title": "People Analytics in R - Job Classification ‘Revisited’",
    "section": "2.Collect And Manage Data",
    "text": "2.Collect And Manage Data\nFor purposes of this application of People Analytics, this step in the data science process will take the longest initially. This is because in almost every organization, the existing job classifications or categories, and the job descriptions themselves are not typically represented in numerical format suitable for statistical analysis. Sometimes, that which we are predicting- the pay grade is numeric because point methods are used in evaluation and different paygrades have different point ranges. But more often the job descriptions are narrative as are the job classification specs or summaries. For this blog article, we will assume that and delineate the steps required.\n\nCollecting The Data\nThe following are typical steps:\n\nGather together the entire set of narrative, written job classification specifications.\nReview all of them to determine what the common denominators are- what the organization is paying attention to , to differentiate them from each other.\nFor each of the common denominators, pay attention to descriptions of how much of that common denominator exists in each narrative, writing down the phrases that are used.\nFor each common denominator, develop an ordinal scale which assigns numbers and places them in a ‘less to more’ order\nCreate a datafile where each record (row) is one job classification, and where each column is either a common denominator or the job classification identifier or paygrade.\nCode each job classification narrative into the datafile recording their common denominator information and other pertinent categorical information.\n\n\nGather together the entire set of narrative, written job classification specifications.\nThis initially represents the ‘total’ population of what will be a ‘known’ population. Ones that by definition represent the prescribed intended categories and levels of paygrades. These are going to be used to compare an ‘unknown’ population- unclassified job descriptions, to determine best fit. But before this can happen, we should have confidence that the job classifications themselves are well designed- since they will be the standard against which all job descriptions will be compared.\n\n\nReview all of them to determine what the common denominators are\nTechnically speaking, anything that appears in the narrative could be considered a feature that is a common denominator including the tasks, knowledges described. But few organizations have that level of automation in their job descriptions. So generally broader features are used to describe common denominators. Often they may include the following:\n\nEducation Level\nExperience\nOrganizational Impact\nProblem Solving\nSupervision Received\nContact Level\nFinancial Budget Responsibility\n\nTo be a common denominator they need to be mentioned or discernable in every job classification specification\n\n\nPay attention to the descriptions of how much of that common denominator exists in each narrative\nFor each of the above common denominators ( if these are ones you use), go through each narrative identify where the common denominator is mentioned and write down the words used to describe how much of it exists. Go through you entire set of job classification specs and tabulate these for each common denominator and each class spec.\n\n\nFor each common denominator, develop an ordinal scale\nOrdinal means in order. You order the descriptions from less than to more than. Then apply a numerical indicator to it. 0 might mean it doesnt exist in any significant way, 1 might mean something at a low or introductory level, higher numbers meaning more of it. The scale should have as many numbers as distinguishable descriptions.(You may have to merge or collapse descriptions if it’s impossible to distinguish order)\n\n\nCreate a datafile\nThis might be a spreadsheet.\neach record(row) will be one job classification, and each column will be either a common denominator or the job classification identifier or paygrade or other categorical information.\n\n\nCode each job classification narrative into the datafile\nRecord their common denominator information and other pertinent categorical or identifying information. At the end of this task you will have as many records as you have written job classification specs.\nAt the end of this effort you will have something that looks like the data found at the following link:\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6435&authkey=!AL37Wt0sVLrsUYA&ithint=file%2ctxt\n###Manage The Data\nIn this step we check the data for errors, organize the data for model building, and take an initial look at what the data is telling us.\n\n\nCheck the data for errors\n\nlibrary(readr)\n#MYdataset &lt;- read.csv(\"jobclassinfo2.txt\")\nMYdataset&lt;- read_csv(\"jobclassinfo2.txt\", \n    col_types = cols(PG = col_factor(levels = c(\"PG01\", \n        \"PG02\", \"PG03\", \"PG04\", \"PG05\", \"PG06\", \n        \"PG07\", \"PG08\", \"PG09\", \"PG10\"))))\n\nstr(MYdataset,width=80,strict.width =\"wrap\")\n\nspc_tbl_ [66 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n$ ID : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...\n$ JobFamily : num [1:66] 1 1 1 1 2 2 2 2 2 3 ...\n$ JobFamilyDescription: chr [1:66] \"Accounting And Finance\" \"Accounting And\n   Finance\" \"Accounting And Finance\" \"Accounting And Finance\" ...\n$ JobClass : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...\n$ JobClassDescription : chr [1:66] \"Accountant I\" \"Accountant II\" \"Accountant\n   III\" \"Accountant IV\" ...\n$ PayGrade : num [1:66] 5 6 8 10 1 2 3 4 5 4 ...\n$ EducationLevel : num [1:66] 3 4 4 5 1 1 1 4 4 2 ...\n$ Experience : num [1:66] 1 1 2 5 0 1 2 0 0 0 ...\n$ OrgImpact : num [1:66] 3 5 6 6 1 1 1 1 4 1 ...\n$ ProblemSolving : num [1:66] 3 4 5 6 1 1 2 2 3 4 ...\n$ Supervision : num [1:66] 4 5 6 7 1 1 1 1 5 1 ...\n$ ContactLevel : num [1:66] 3 7 7 8 1 2 3 3 7 1 ...\n$ FinancialBudget : num [1:66] 5 7 10 11 1 3 3 5 7 2 ...\n$ PG : Factor w/ 10 levels \"PG01\",\"PG02\",..: 5 6 8 10 1 2 3 4 5 4 ...\n- attr(*, \"spec\")=\n.. cols(\n..  ID = col_double(),\n..  JobFamily = col_double(),\n..  JobFamilyDescription = col_character(),\n..  JobClass = col_double(),\n..  JobClassDescription = col_character(),\n..  PayGrade = col_double(),\n..  EducationLevel = col_double(),\n..  Experience = col_double(),\n..  OrgImpact = col_double(),\n..  ProblemSolving = col_double(),\n..  Supervision = col_double(),\n..  ContactLevel = col_double(),\n..  FinancialBudget = col_double(),\n..  PG = col_factor(levels = c(\"PG01\", \"PG02\", \"PG03\", \"PG04\", \"PG05\", \"PG06\",\n   \"PG07\", \"PG08\",\n..  \"PG09\", \"PG10\"), ordered = FALSE, include_na = FALSE)\n.. )\n- attr(*, \"problems\")=&lt;externalptr&gt;\n\nsummary(MYdataset)\n\n       ID          JobFamily      JobFamilyDescription    JobClass    \n Min.   : 1.00   Min.   : 1.000   Length:66            Min.   : 1.00  \n 1st Qu.:17.25   1st Qu.: 4.000   Class :character     1st Qu.:17.25  \n Median :33.50   Median : 7.000   Mode  :character     Median :33.50  \n Mean   :33.50   Mean   : 7.606                        Mean   :33.50  \n 3rd Qu.:49.75   3rd Qu.:11.000                        3rd Qu.:49.75  \n Max.   :66.00   Max.   :15.000                        Max.   :66.00  \n                                                                      \n JobClassDescription    PayGrade      EducationLevel    Experience    \n Length:66           Min.   : 1.000   Min.   :1.000   Min.   : 0.000  \n Class :character    1st Qu.: 4.000   1st Qu.:2.000   1st Qu.: 0.000  \n Mode  :character    Median : 5.000   Median :4.000   Median : 1.000  \n                     Mean   : 5.697   Mean   :3.167   Mean   : 1.758  \n                     3rd Qu.: 8.000   3rd Qu.:4.000   3rd Qu.: 2.750  \n                     Max.   :10.000   Max.   :6.000   Max.   :10.000  \n                                                                      \n   OrgImpact     ProblemSolving   Supervision     ContactLevel  \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:3.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :3.000   Median :4.000   Median :4.000   Median :6.000  \n Mean   :3.348   Mean   :3.606   Mean   :3.864   Mean   :4.758  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.750   3rd Qu.:7.000  \n Max.   :6.000   Max.   :6.000   Max.   :7.000   Max.   :8.000  \n                                                                \n FinancialBudget        PG    \n Min.   : 1.000   PG05   :15  \n 1st Qu.: 2.000   PG03   : 7  \n Median : 5.000   PG04   : 7  \n Mean   : 5.303   PG06   : 7  \n 3rd Qu.: 7.750   PG08   : 7  \n Max.   :11.000   PG09   : 6  \n                  (Other):17  \n\n\nOn the surface there doesn’t seem to be any issues with data. This gives a summary of the layout of the data and the likely values we can expect. PG is the category we will predict. It’s a categorical representation of the numeric paygrade. Education level through Financial Budgeting Responsibility will be the independent variables/measures we will use to predict. The other columns in file will be ignored.\n\n\nOrganize the data\nLets narrow down the information to just the data used in the model.\n\nMYnobs &lt;- nrow(MYdataset) # 66 observations \nMYsample &lt;- MYtrain &lt;- sample(nrow(MYdataset), 0.7*MYnobs) # 46 observations\nMYvalidate &lt;- sample(setdiff(seq_len(nrow(MYdataset)), MYtrain), 0.14*MYnobs) # 9 observations\nMYtest &lt;- setdiff(setdiff(seq_len(nrow(MYdataset)), MYtrain), MYvalidate) # 11 observations\n\n\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:43:48 x86_64-w64-mingw32 \n\n# Note the user selections. \n\n# The following variable selections have been noted.\n\nMYinput &lt;- c(\"EducationLevel\", \"Experience\", \"OrgImpact\", \"ProblemSolving\",\n     \"Supervision\", \"ContactLevel\", \"FinancialBudget\")\n\nMYnumeric &lt;- c(\"EducationLevel\", \"Experience\", \"OrgImpact\", \"ProblemSolving\",\n     \"Supervision\", \"ContactLevel\", \"FinancialBudget\")\n\nMYcategoric &lt;- NULL\n\nMYtarget  &lt;- \"PG\"\nMYrisk    &lt;- NULL\nMYident   &lt;- \"ID\"\nMYignore  &lt;- c(\"JobFamily\", \"JobFamilyDescription\", \"JobClass\", \"JobClassDescription\", \"PayGrade\")\nMYweights &lt;- NULL\n\nWe are predominantly interested in MYinput and MYtarget because they represent the predictors and what is to be predicted respectively. You will notice for the time being that we are not partitioning the data. This will be elaborated upon in model building.\n\n\n\nWhat the data is initially telling us\nLets use the caret library again for some graphical representations of this data.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nfeaturePlot(x=MYdataset[,7:13],y=MYdataset$PG,plot=\"density\",auto.key = list(columns = 2))\n\n\n\nfeaturePlot(x=MYdataset[,7:13],y=MYdataset$PG,plot=\"box\",auto.key = list(columns = 2))\n\n\n\n\nThe first set of charts show the distribution of the independent variable values(predictors) by PG.\nThe second set of charts show the range of values of the predictors by PG. PG is ordered left to right in ascending order from PG1 to PG10. In each of the predictors we would expect increasing levels as we move up the paygrades and from left to right (or at least not dropping from previous paygrade).\nThis is the first indication by a graphic ‘visual’ that we ‘may’ have problems in the data or the interpretation of the coding of the information. Then again the coding may be accurate based on our descriptions and our assumptions false. We will probably want to recheck our coding from the job description to make sure."
  },
  {
    "objectID": "posts/PAExampleClassification/PAExampleClassification.html#build-the-model",
    "href": "posts/PAExampleClassification/PAExampleClassification.html#build-the-model",
    "title": "People Analytics in R - Job Classification ‘Revisited’",
    "section": "3.Build The Model",
    "text": "3.Build The Model\nLets use the rattle library to efficiently generate the code to run the following classification algorithms against our data:\n\nDecision Tree\nRandom Forest\nSupport Vector Machines\nLinear\n\n\nDecision Tree\n\nlibrary(rattle)\n\nLoading required package: tibble\n\n\nLoading required package: bitops\n\n\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Decision Tree \n\n# The 'rpart' package provides the 'rpart' function.\n\nlibrary(rpart, quietly=TRUE)\n\n# Reset the random number seed to obtain the same results each time.\n#crv$seed &lt;- 42 \n#set.seed(crv$seed)\n\n# Build the Decision Tree model.\n\nMYrpart &lt;- rpart(PG ~ .,\n    data=MYdataset[, c(MYinput, MYtarget)],\n    method=\"class\",\n    parms=list(split=\"information\"),\n      control=rpart.control(minsplit=10,\n           minbucket=2,\n           maxdepth=10,\n        usesurrogate=0, \n        maxsurrogate=0))\n\n# Generate a textual view of the Decision Tree model.\n\nprint(MYrpart)\n\nn= 66 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 66 51 PG05 (0.03 0.076 0.11 0.11 0.23 0.11 0.061 0.11 0.091 0.091)  \n   2) ProblemSolving&lt; 4.5 47 32 PG05 (0.043 0.11 0.15 0.15 0.32 0.15 0.085 0 0 0)  \n     4) ContactLevel&lt; 5.5 32 21 PG05 (0.062 0.16 0.22 0.22 0.34 0 0 0 0 0)  \n       8) EducationLevel&lt; 1.5 15  9 PG03 (0.13 0.33 0.4 0.13 0 0 0 0 0 0)  \n        16) ProblemSolving&lt; 1.5 5  2 PG02 (0.4 0.6 0 0 0 0 0 0 0 0) *\n        17) ProblemSolving&gt;=1.5 10  4 PG03 (0 0.2 0.6 0.2 0 0 0 0 0 0)  \n          34) Experience&lt; 0.5 3  1 PG02 (0 0.67 0 0.33 0 0 0 0 0 0) *\n          35) Experience&gt;=0.5 7  1 PG03 (0 0 0.86 0.14 0 0 0 0 0 0) *\n       9) EducationLevel&gt;=1.5 17  6 PG05 (0 0 0.059 0.29 0.65 0 0 0 0 0)  \n        18) Experience&lt; 0.5 8  3 PG04 (0 0 0 0.62 0.37 0 0 0 0 0) *\n        19) Experience&gt;=0.5 9  1 PG05 (0 0 0.11 0 0.89 0 0 0 0 0) *\n     5) ContactLevel&gt;=5.5 15  8 PG06 (0 0 0 0 0.27 0.47 0.27 0 0 0)  \n      10) Experience&lt; 2.5 12  5 PG06 (0 0 0 0 0.33 0.58 0.083 0 0 0)  \n        20) ContactLevel&gt;=6.5 8  4 PG05 (0 0 0 0 0.5 0.38 0.13 0 0 0) *\n        21) ContactLevel&lt; 6.5 4  0 PG06 (0 0 0 0 0 1 0 0 0 0) *\n      11) Experience&gt;=2.5 3  0 PG07 (0 0 0 0 0 0 1 0 0 0) *\n   3) ProblemSolving&gt;=4.5 19 12 PG08 (0 0 0 0 0 0 0 0.37 0.32 0.32)  \n     6) ProblemSolving&lt; 5.5 13  6 PG08 (0 0 0 0 0 0 0 0.54 0.46 0)  \n      12) ContactLevel&gt;=6.5 10  3 PG08 (0 0 0 0 0 0 0 0.7 0.3 0) *\n      13) ContactLevel&lt; 6.5 3  0 PG09 (0 0 0 0 0 0 0 0 1 0) *\n     7) ProblemSolving&gt;=5.5 6  0 PG10 (0 0 0 0 0 0 0 0 0 1) *\n\nprintcp(MYrpart)\n\n\nClassification tree:\nrpart(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], \n    method = \"class\", parms = list(split = \"information\"), control = rpart.control(minsplit = 10, \n        minbucket = 2, maxdepth = 10, usesurrogate = 0, maxsurrogate = 0))\n\nVariables actually used in tree construction:\n[1] ContactLevel   EducationLevel Experience     ProblemSolving\n\nRoot node error: 51/66 = 0.77273\n\nn= 66 \n\n        CP nsplit rel error  xerror     xstd\n1 0.137255      0   1.00000 1.00000 0.066756\n2 0.117647      1   0.86275 0.98039 0.068266\n3 0.088235      2   0.74510 0.94118 0.070944\n4 0.058824      4   0.56863 0.78431 0.077835\n5 0.039216      7   0.39216 0.74510 0.078728\n6 0.019608      9   0.31373 0.72549 0.079060\n7 0.010000     10   0.29412 0.68627 0.079501\n\ncat(\"\\n\")\n# Time taken: 0.02 secs\n\n\n\nRandom Forest\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Random Forest \n\n# The 'randomForest' package provides the 'randomForest' function.\n\nlibrary(randomForest, quietly=TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n# Build the Random Forest model.\n\n#set.seed(crv$seed)\nMYrf &lt;- randomForest::randomForest(PG ~ .,\n      data=MYdataset[,c(MYinput, MYtarget)], \n      ntree=500,\n      mtry=2,\n      importance=TRUE,\n      na.action=randomForest::na.roughfix,\n      replace=FALSE)\n\n# Generate textual output of 'Random Forest' model.\n\nMYrf\n\n\nCall:\n randomForest(formula = PG ~ ., data = MYdataset[, c(MYinput,      MYtarget)], ntree = 500, mtry = 2, importance = TRUE, replace = FALSE,      na.action = randomForest::na.roughfix) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 37.88%\nConfusion matrix:\n     PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 class.error\nPG01    0    2    0    0    0    0    0    0    0    0   1.0000000\nPG02    0    4    1    0    0    0    0    0    0    0   0.2000000\nPG03    0    0    5    1    1    0    0    0    0    0   0.2857143\nPG04    0    0    1    3    3    0    0    0    0    0   0.5714286\nPG05    0    0    0    3   10    2    0    0    0    0   0.3333333\nPG06    0    0    0    0    1    6    0    0    0    0   0.1428571\nPG07    0    0    0    0    0    3    1    0    0    0   0.7500000\nPG08    0    0    0    0    0    0    0    4    2    1   0.4285714\nPG09    0    0    0    0    0    0    0    4    2    0   0.6666667\nPG10    0    0    0    0    0    0    0    0    0    6   0.0000000\n\n# List the importance of the variables.\n\nrn &lt;- round(randomForest::importance(MYrf), 2)\nrn[order(rn[,3], decreasing=TRUE),]\n\n                 PG01  PG02  PG03 PG04  PG05  PG06  PG07  PG08  PG09  PG10\nEducationLevel   2.85 15.84 12.83 4.29  6.42  2.03  5.47  6.35 -4.76  8.29\nProblemSolving   3.93  6.34  9.25 2.34  7.62 12.13  8.43 13.24 10.21 19.68\nExperience      -4.44 10.20  9.16 4.67  2.36  6.31  9.03 -5.78  4.93  3.32\nSupervision     -3.08  9.00  7.30 2.75  7.00  4.17  0.00  3.40  3.78 15.64\nContactLevel     2.46 11.88  3.88 3.89  2.77 10.43  3.70 10.34 -1.38 10.47\nOrgImpact       -2.52 10.59  3.67 2.58 10.19  1.78  5.61 -0.91  6.30 10.58\nFinancialBudget  2.25  6.28  3.59 1.65  8.66  2.95 -3.59 -5.06 11.90 15.03\n                MeanDecreaseAccuracy MeanDecreaseGini\nEducationLevel                 17.97             4.61\nProblemSolving                 23.60             6.30\nExperience                     13.86             4.42\nSupervision                    16.30             3.82\nContactLevel                   16.10             4.93\nOrgImpact                      14.17             3.38\nFinancialBudget                13.79             5.24\n\n# Time taken: 0.06 secs\n\n\n\nSupport Vector Machine\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:16 x86_64-w64-mingw32 \n\n# Support vector machine. \n\n# The 'kernlab' package provides the 'ksvm' function.\n\nlibrary(kernlab, quietly=TRUE)\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n# Build a Support Vector Machine model.\n\n#set.seed(crv$seed)\nMYksvm &lt;- ksvm(as.factor(PG) ~ .,\n      data=MYdataset[,c(MYinput, MYtarget)],\n      kernel=\"rbfdot\",\n      prob.model=TRUE)\n\n# Generate a textual view of the SVM model.\n\nMYksvm\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  0.151222236388729 \n\nNumber of Support Vectors : 64 \n\nObjective Function Value : -3.8437 -3.6794 -2.9812 -2.3382 -1.5472 -1.3242 -1.3688 -1.5097 -1.3689 -7.7967 -4.483 -2.6257 -1.6532 -1.3211 -1.3594 -1.5029 -1.3566 -8.6345 -6.5264 -3.1206 -2.3037 -1.8315 -2.088 -1.6889 -10.3209 -3.9815 -2.5211 -2.0744 -2.4871 -1.8299 -10.2577 -6.5316 -3.8955 -3.8986 -2.2741 -7.6954 -5.6307 -4.8631 -2.3234 -4.7859 -4.1069 -2.112 -9.9423 -6.2772 -6.586 \nTraining error : 0.30303 \nProbability model included. \n\n# Time taken: 0.43 secs\n\n\n\nLinear Model\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:17 x86_64-w64-mingw32 \n\n# Regression model \n\n# Build a multinomial model using the nnet package.\n\nlibrary(nnet, quietly=TRUE)\n\n# Summarise multinomial model using Anova from the car package.\n\nlibrary(car, quietly=TRUE)\n\n# Build a Regression model.\n\nMYglm &lt;- multinom(PG ~ ., data=MYdataset[,c(MYinput, MYtarget)], trace=FALSE, maxit=1000)\n\n# Generate a textual view of the Linear model.\n\nrattle.print.summary.multinom(summary(MYglm,\n                              Wald.ratios=TRUE))\n\nWarning in sqrt(diag(vc)): NaNs produced\n\n\nCall:\nmultinom(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], \n    trace = FALSE, maxit = 1000)\n\nn=66\n\nCoefficients:\n     (Intercept) EducationLevel  Experience  OrgImpact ProblemSolving\nPG02   31570.140     -29546.246 -10054.5402 -31742.949       22887.11\nPG03    5173.408     -13991.742  17611.5597 -23434.513       27580.11\nPG04  -12639.313       3742.145  -5763.6011 -11968.730       18528.68\nPG05  -29223.399       5164.052   1451.4069 -12579.427       21540.91\nPG06  -64936.850       5381.517   1332.3066 -10153.015       27846.30\nPG07  -55186.714       5379.689   1334.2282 -10155.643       25405.97\nPG08 -124914.252       6197.617    572.4667 -13900.558       42485.72\nPG09  -97759.116     -14428.508   8698.6998  -8681.908      -21901.43\nPG10 -201459.455       9187.762    765.1991 -15483.566       54919.25\n     Supervision ContactLevel FinancialBudget\nPG02   -3826.376   -3466.3364       14125.347\nPG03   -7979.596    4054.9295       -2131.931\nPG04   -1202.967   -4060.4297        6156.244\nPG05   -3137.087    -986.4325        5756.414\nPG06   -3540.354    -746.1415        6259.916\nPG07   -3539.699    -742.8932        6259.885\nPG08   -1862.503   -1725.1769        7267.505\nPG09   21068.207  -12739.5580       34119.536\nPG10   -2773.094    1152.7658        5960.684\n\nStd. Errors:\n      (Intercept) EducationLevel   Experience    OrgImpact ProblemSolving\nPG02 1.749631e-01   1.749631e-01 0.000000e+00 1.749631e-01   1.749631e-01\nPG03 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG04 0.000000e+00            NaN          NaN 1.776635e-14   7.258525e-15\nPG05 4.576029e-16   2.976903e-15 2.398744e-16 1.189550e-16   1.716121e-16\nPG06 3.859338e-01   1.397271e+00 6.650459e-01 1.366421e+00   1.543735e+00\nPG07 3.859338e-01   1.397271e+00 6.650459e-01 1.366421e+00   1.543735e+00\nPG08 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG09 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\nPG10 0.000000e+00   0.000000e+00 0.000000e+00 0.000000e+00   0.000000e+00\n      Supervision ContactLevel FinancialBudget\nPG02 1.749631e-01 1.749631e-01    1.749631e-01\nPG03 0.000000e+00 0.000000e+00    0.000000e+00\nPG04 4.074351e-15          NaN             NaN\nPG05 7.348316e-17 2.343827e-15    4.111322e-17\nPG06 8.809021e-01 1.298034e+00    3.216738e-01\nPG07 8.809021e-01 1.298034e+00    3.216738e-01\nPG08 0.000000e+00 0.000000e+00    0.000000e+00\nPG09 0.000000e+00 0.000000e+00    0.000000e+00\nPG10 0.000000e+00 0.000000e+00    0.000000e+00\n\nValue/SE (Wald statistics):\n       (Intercept) EducationLevel   Experience     OrgImpact ProblemSolving\nPG02  1.804388e+05  -1.688713e+05         -Inf -1.814265e+05   1.308111e+05\nPG03           Inf           -Inf          Inf          -Inf            Inf\nPG04          -Inf            NaN          NaN -6.736740e+17   2.552679e+18\nPG05 -6.386192e+19   1.734706e+18 6.050696e+18 -1.057495e+20   1.255209e+20\nPG06 -1.682590e+05   3.851448e+03 2.003330e+03 -7.430373e+03   1.803826e+04\nPG07 -1.429953e+05   3.850140e+03 2.006220e+03 -7.432296e+03   1.645746e+04\nPG08          -Inf            Inf          Inf          -Inf            Inf\nPG09          -Inf           -Inf          Inf          -Inf           -Inf\nPG10          -Inf            Inf          Inf          -Inf            Inf\n       Supervision  ContactLevel FinancialBudget\nPG02 -2.186961e+04 -1.981181e+04    8.073327e+04\nPG03          -Inf           Inf            -Inf\nPG04 -2.952536e+17           NaN             NaN\nPG05 -4.269123e+19 -4.208639e+17    1.400137e+20\nPG06 -4.019010e+03 -5.748241e+02    1.946045e+04\nPG07 -4.018266e+03 -5.723216e+02    1.946035e+04\nPG08          -Inf          -Inf             Inf\nPG09           Inf          -Inf             Inf\nPG10          -Inf           Inf             Inf\n\nResidual Deviance: 13.0907 \nAIC: 157.0907 \n\ncat(sprintf(\"Log likelihood: %.3f (%d df)\n\", logLik(MYglm)[1], attr(logLik(MYglm), \"df\")))\n\nLog likelihood: -6.545 (72 df)\n\nif (is.null(MYglm$na.action)) omitted &lt;- TRUE else omitted &lt;- -MYglm$na.action\ncat(sprintf(\"Pseudo R-Square: %.8f\n\n\",cor(apply(MYglm$fitted.values, 1, function(x) which(x == max(x))),\nas.integer(MYdataset[omitted,]$PG))))\n\nPseudo R-Square: 0.99516038\n\ncat('==== ANOVA ====\n')\n\n==== ANOVA ====\n\nprint(Anova(MYglm))\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: PG\n                LR Chisq Df Pr(&gt;Chisq)   \nEducationLevel   14.3433  9   0.110626   \nExperience       24.2064  9   0.003987 **\nOrgImpact         1.6140  9   0.996209   \nProblemSolving   21.1081  9   0.012179 * \nSupervision       2.9187  9   0.967430   \nContactLevel      4.8563  9   0.846653   \nFinancialBudget   5.5476  9   0.784206   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nprint(\"\n\")\n\n[1] \"\\n\"\n\n\nNow lets plot the Decision Tree\n\n\nDecision Tree Plot\n\n# Time taken: 0.16 secs\n\n#============================================================\n# Rattle timestamp: 2016-04-27 12:51:52 x86_64-w64-mingw32 \n\n# Plot the resulting Decision Tree. \n\n# We use the rpart.plot package.\n\nfancyRpartPlot(MYrpart, main=\"Decision Tree MYdataset $ PG\")\n\n\n\n\nA readable view of the decision tree can be found at the following pdf:\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6449&authkey=!ACgJAX951UZuo4s&ithint=file%2cpdf\n##4.Evaluate And Critique Model\n###Evaluate\nBecause we have multiple categories to be predicted, the only evaluation used is Error Matricies.\nLets see how well the models performed.\n\nDecision Tree\n\n#============================================================\n# Rattle timestamp: 2016-04-27 13:11:52 x86_64-w64-mingw32 \n\n# Evaluate model performance. \n\n# Generate an Error Matrix for the Decision Tree model.\n\n# Obtain the response from the Decision Tree model.\n\nMYpr &lt;- predict(MYrpart, newdata=MYdataset[,c(MYinput, MYtarget)], type=\"class\")\n\n# Generate the confusion matrix showing counts.\n\ntable(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    0    2    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    6    0    1    0    0    0    0    0\n  PG04    0    1    1    5    0    0    0    0    0    0\n  PG05    0    0    0    3   12    0    0    0    0    0\n  PG06    0    0    0    0    3    4    0    0    0    0\n  PG07    0    0    0    0    1    0    3    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    3    3    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr)\nround(per, 2)\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01    0 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  1.00\n  PG02    0 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG03    0 0.00 0.09 0.00 0.02 0.00 0.00 0.00 0.00 0.00  0.14\n  PG04    0 0.02 0.02 0.08 0.00 0.00 0.00 0.00 0.00 0.00  0.29\n  PG05    0 0.00 0.00 0.05 0.18 0.00 0.00 0.00 0.00 0.00  0.20\n  PG06    0 0.00 0.00 0.00 0.05 0.06 0.00 0.00 0.00 0.00  0.43\n  PG07    0 0.00 0.00 0.00 0.02 0.00 0.05 0.00 0.00 0.00  0.25\n  PG08    0 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00\n  PG09    0 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.05 0.00  0.50\n  PG10    0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n23\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n28\n\n\n\n\nRandom Forest\n\n# Generate an Error Matrix for the Random Forest model.\n\n# Obtain the response from the Random Forest model.\n\nMYpr &lt;- predict(MYrf, newdata=na.omit(MYdataset[,c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    1    1    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    7    0    0    0    0    0    0    0\n  PG04    0    0    0    7    0    0    0    0    0    0\n  PG05    0    0    0    0   15    0    0    0    0    0\n  PG06    0    0    0    0    0    7    0    0    0    0\n  PG07    0    0    0    0    0    0    4    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    0    6    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr)\nround(per, 2)\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.5\n  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00   0.0\n  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00   0.0\n  PG06 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00   0.0\n  PG07 0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00   0.0\n  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00   0.0\n  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00   0.0\n  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09   0.0\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n2\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n5\n\n\n\n\nSupport Vector Machine\n\n# Generate an Error Matrix for the SVM model.\n\n# Obtain the response from the SVM model.\n\nMYpr &lt;- kernlab::predict(MYksvm, newdata=na.omit(MYdataset[,c(MYinput, MYtarget)]))\n\n# Generate the confusion matrix showing counts.\n\ntable(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    0    1    1    0    0    0    0    0    0    0\n  PG02    0    4    1    0    0    0    0    0    0    0\n  PG03    0    0    5    0    2    0    0    0    0    0\n  PG04    0    0    0    6    1    0    0    0    0    0\n  PG05    0    0    0    1   12    2    0    0    0    0\n  PG06    0    0    0    0    2    5    0    0    0    0\n  PG07    0    0    0    0    0    4    0    0    0    0\n  PG08    0    0    0    0    0    1    0    6    0    0\n  PG09    0    0    0    0    0    0    0    4    2    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(na.omit(MYdataset[,c(MYinput, MYtarget)])$PG, MYpr)\nround(per, 2)\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01    0 0.02 0.02 0.00 0.00 0.00    0 0.00 0.00 0.00  1.00\n  PG02    0 0.06 0.02 0.00 0.00 0.00    0 0.00 0.00 0.00  0.20\n  PG03    0 0.00 0.08 0.00 0.03 0.00    0 0.00 0.00 0.00  0.29\n  PG04    0 0.00 0.00 0.09 0.02 0.00    0 0.00 0.00 0.00  0.14\n  PG05    0 0.00 0.00 0.02 0.18 0.03    0 0.00 0.00 0.00  0.20\n  PG06    0 0.00 0.00 0.00 0.03 0.08    0 0.00 0.00 0.00  0.29\n  PG07    0 0.00 0.00 0.00 0.00 0.06    0 0.00 0.00 0.00  1.00\n  PG08    0 0.00 0.00 0.00 0.00 0.02    0 0.09 0.00 0.00  0.14\n  PG09    0 0.00 0.00 0.00 0.00 0.00    0 0.06 0.03 0.00  0.67\n  PG10    0 0.00 0.00 0.00 0.00 0.00    0 0.00 0.00 0.09  0.00\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n30\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n39\n\n\n\n\nLinear Model\n\n# Generate an Error Matrix for the Linear model.\n\n# Obtain the response from the Linear model.\n\nMYpr &lt;- predict(MYglm, newdata=MYdataset[,c(MYinput, MYtarget)])\n\n# Generate the confusion matrix showing counts.\n\ntable(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr,\n        dnn=c(\"Actual\", \"Predicted\"))\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n  PG01    1    1    0    0    0    0    0    0    0    0\n  PG02    0    5    0    0    0    0    0    0    0    0\n  PG03    0    0    7    0    0    0    0    0    0    0\n  PG04    0    0    0    7    0    0    0    0    0    0\n  PG05    0    0    0    0   15    0    0    0    0    0\n  PG06    0    0    0    0    0    6    1    0    0    0\n  PG07    0    0    0    0    0    2    2    0    0    0\n  PG08    0    0    0    0    0    0    0    7    0    0\n  PG09    0    0    0    0    0    0    0    0    6    0\n  PG10    0    0    0    0    0    0    0    0    0    6\n\n# Generate the confusion matrix showing proportions.\n\npcme &lt;- function(actual, cl)\n{\n  x &lt;- table(actual, cl)\n  nc &lt;- nrow(x)\n  tbl &lt;- cbind(x/length(actual),\n               Error=sapply(1:nc,\n                 function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))\n  names(attr(tbl, \"dimnames\")) &lt;- c(\"Actual\", \"Predicted\")\n  return(tbl)\n}\nper &lt;- pcme(MYdataset[,c(MYinput, MYtarget)]$PG, MYpr)\nround(per, 2)\n\n      Predicted\nActual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error\n  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.50\n  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00  0.00\n  PG06 0.00 0.00 0.00 0.00 0.00 0.09 0.02 0.00 0.00 0.00  0.14\n  PG07 0.00 0.00 0.00 0.00 0.00 0.03 0.03 0.00 0.00 0.00  0.50\n  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00\n  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00  0.00\n  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00\n\n# Calculate the overall error percentage.\n\ncat(100*round(1-sum(diag(per), na.rm=TRUE), 2))\n\n6\n\n# Calculate the averaged class error percentage.\n\ncat(100*round(mean(per[,\"Error\"], na.rm=TRUE), 2))\n\n11\n\n\n\n\n\nCritique\nIt turns out that:\n\nThe model that performed best was Random Forests at 2% error\nThe linear model was next at 6% error.\n\nSupport Vector Machines performed less well at 18% error.\nAnd Decision trees, while being able to give us ‘visual’ on what rules are being used, performed worst of all at 23% error.\n\nEarlier I said ,that for purposes of this analysis, we would not have training and test datasets. This is because the total population of our data is only 66 records which are scattered among up to 10 categories. Unless we took special care, if we randomly created test and training datasets, we could not guarantee that each of these datasets would have all 10 groups represented. So we use all the data to predict itself.(usually not recommended if lots of data)\nIf as an organization , we were just starting out with this kind of endeavor, at this point you would only have the job classification specs coded into the dataset. it would be the only ‘known’ population. In some ways it is useful to do this here- to gauge how well designed our paygrades are ‘differentiated’ from each other. Even though we know that the known population predicting itself will give lower error rates, suffice to say that here, the hypothetical organization is seeing results that would make it worthwhile to expand the coding effort. If at this stage, the data could not reliably predict itself, we might to rethink our approach.\nIn most organizations who have written job descriptions and job classification specs, their job descriptions are already classified as well. So we arent necessarily restricted to just coding the job class specs. We could go ahead and code the job descriptions on the same common denominator features.( outside the context of this blog article) This would make the ‘known’ population quite a bit bigger. On a bigger population too we could also have the population predict itself but additionally do cross validation and have both training and test datasets.\nWhile the above results were found on just the job class specs, it would be wise to have a much larger population before deciding on which model is best to deploy in real life.\nOne other observation- you noticed in the results of the various models, that some model had predictions that were one or two paygrades off ‘higher or lower’ than the actual existing paygrade.\nIn a practical sense this might mean:\n\nthese might be candidates for determining whether criteria/features for these pay grades should be redefined\nand or whether there are ,in reality, fewer categories needed.\n\nWe could extend our analysis and modelling to ‘cluster’ analysis.This would create a newer grouping based on the existing characteristics, and then the classification algorithms could be rerun to see if there was any improvement.\nSome articles on People Analytics suggest that on a ‘maturity level’ basis, the step/stage beyond prediction is ‘experimental design’. If we are using our results to modify our design of our systems to predict better, that might be an example of this."
  },
  {
    "objectID": "posts/PAExampleClassification/PAExampleClassification.html#present-results-and-document",
    "href": "posts/PAExampleClassification/PAExampleClassification.html#present-results-and-document",
    "title": "People Analytics in R - Job Classification ‘Revisited’",
    "section": "5.Present Results And Document",
    "text": "5.Present Results And Document\nAs with previous blog articles, a good way of carrying out this step is this the .rmd file which is used to create this blog article. R Markdown language is used to create this narrative as well as allow for ‘inline’ inclusion of the R program and its output. The rmd file can be found here:\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6467&authkey=!AE4IyMNEaoLgqnw&ithint=file%2cRmd\n##6.Deploy The Model\nIn R the easiest form of deploying the model, is to run your unknown data against the model . Put the data in a separate dataset and run the following R commands:\nHere is dataset:\nhttps://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6478&authkey=!ALYidIIpaCrfnf4&ithint=file%2ccsv\n\n#DeployDataset &lt;- read.csv(\"~/Documents/OneDrive/Public/R Job Classification Example/Deploydata.csv\")\nDeployDataset &lt;- read.csv(\"Deploydata.csv\")\nDeployDataset\n\n  EducationLevel Experience OrgImpact ProblemSolving Supervision ContactLevel\n1              2          0         3              4           1            5\n  FinancialBudget\n1               4\n\nPredictedJobGrade &lt;- predict(MYrf, newdata=DeployDataset)\nPredictedJobGrade\n\n   1 \nPG05 \nLevels: PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10\n\n\nThe DeployDataset represemts the information coded from a single job description (paygrade not known). PredictedJobGrade compares the coded values against the MYrf (random forest model) and the prediction is determined. In this case - PG05."
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "",
    "text": "HR Analytics is still in its infancy in most organizations. One way to gauge that is to ask about and investigate the degree to which activity in this area is evident in HR practices in organizations. If you have either read some of my previous articles:\nhttps://www.linkedin.com/in/lyndon-sundmark-mba-59272a/detail/recent-activity/posts/\nor read the book I published:\nhttps://www.analyticsinhr.com/hr-analytics-books/\nhttps://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=Lyndon+Sundmark\nyou may remember that there are at least 3 broad categories that have the capacity generate HR-related information/data that can benefit from the use of HR Analytics:\n\nTraditional HR Metrics – information generated because of ‘onboarding, participation in, and offboarding’ of employees as they interact with organizations\n HR Operations themselves -when HR starts capturing information about every ‘HR request for service’ that comes into them, tracking its status from inception to completion to assist in HR process improvement.\n Embedding HR Analytics (Machine Learning and Artificial Intelligence) directly into HR methodologies themselves to improve the reliability and validity of the HR decisions we make on behalf of the organization\n\nI’ve mentioned in my book and in previous LinkedIn articles the rarity of this evidence - particularly of the last two. And in most organizations even the first one stagnates typically at the first level of analytics- ‘descriptive’ (reports and dashboards), rarely venturing into the even higher payoff areas of diagnostic, predictive, and prescriptive.\nI think that one way for HR to get out of its infancy, in this area of HR, is to continue as much as possible to immerse itself hands-on into their data wherever possible and experiment with HR analytics on their data (with appropriate rightful access assumed of course.)\nIt’s in that spirit, that I continue to have the passion to write in this field. The intent is to show examples of how HR analytics and some of the technologies out there can be brought to bear to make this field ‘more real’ to organizations and show that real gains can be made in the productivity in the organization as a result.\nIn this blog article, I wanted to explore further a previous example that I had given- Job Classification in R- and share a little bit further one example of what that ‘embedding’ might look like in a practical example, and how some of Microsoft’s technologies might help and be of use in that regard."
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#a-readers-digest-version",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#a-readers-digest-version",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "A Reader’s Digest version",
    "text": "A Reader’s Digest version\nThe previous example used statistical ‘classification’ algorithms to do ‘job’ classification in HR. A series of 66 ‘narrative’ job class specifications were reviewed - from a hypothetical organization for commonalities. After reviewing the narrative descriptions, it became evident that a common set of ‘factors’ were used in the traditional classification process to determine the pay grade. These were:\n\nEducation level\nExperience\nOrganizational Impact\nProblem Solving\nSupervision\nContact Level\nFinancial Budgetary Responsibility\n\nEach narrative description was quantified as to the level of each of these mentioned in each job classification description. For each description, a record of data was created in a spreadsheet with identifying information, and the current Pay Grade assigned.\nThe goal in HR Analytics in this example was to see whether this quantification of information from narrative sources and application of machine learning algorithms could provide a reasonable prediction of the paygrade - as compared to the actual paygrade.\nIn looking at the previous article, the accuracy of predicted versus actual was pretty good- so in the R script an example was given to test a ‘new’ job classification not in that existing population or records to determine where it would line up paygrade wise- given the ‘known’ population.\nThe example above showed the process used to validate which algorithm performed best on the data- with the accuracy of prediction against actual being the criteria. With the best algorithm chosen, new data was then applied to the model.\nIn a very loose, rudimentary, technical and ‘non-user-friendly’ way – that WAS ‘embedding’ - choosing to use the results of machine learning as part of the ‘decision making’ in a Job Classification process that already existed.\nGenerally, the more you find yourself able to trust the results (from your experience) of machine learning, the more willing you are to significantly alter your current practices.\nBut what if we wanted this ‘embedding’ to be more ’ user-friendly’?\nWhat if we wanted to encourage end users to use the power of machine learning and artificial intelligence in something they can interact with? One of the ways of doing that is to develop an end-user application that puts the complexities of ‘doing’ and the ‘how its done’ behind the scenes."
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#the-steps-involved",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#the-steps-involved",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "The Steps Involved",
    "text": "The Steps Involved\nThe following steps were carried out in this example:\n\nCreate a predictive model in R for use in embedding\n‘Embed’ the predictive model in SQL Server\nCreate some tables in SQL server to store new data records and the predictions\nCreate a quick ASP.NET MVC end-user web application to manage new data records and execute the predictive model and bring back results to the end user.\n\n\n1.Create the Predictive Model in R\nIn my previous blog articles and code examples, I use R ‘Rattle’ to generate much of the code to test various predictive models. Rattle was a GUI front end which generated code quickly- some of which you needed to know a lot of R to understand.\nOne of the nice things about R is that sometimes you have your choice of many packages to do the same things- some of which are far more readable than others. Since I had used R ‘Rattle’ before, I wanted to try some other packages. In Microsoft’s version of R, they had two libraries that they created which they used in their examples of how to embed in SQL Server. These libraries were RevoscaleR and MicrosoftML. I tried their examples to get a feel of how this would work in SQL Server. They worked, and I thought – ‘this is cool’. But then I began wondering whether I could use their pattern of code in SQL server using other R libraries.\nOne such R library was ‘mlr’ or ‘machine learning in R’.\nhttps://mlr-org.github.io/mlr-tutorial/release/html/\n I liked its syntax because it was more readable and understandable to me. But would it work when embedded into SQL Server? Happy to say it did.\nSo here is the R predictive model and code before it was embedded into SQL Server- so that you can see what we are embedding:\nThe data for creating the model is here:\nhttps://www.kaggle.com/HRAnalyticRepository/job-classification-dataset\nHere is R code\n\nYou would have to change MYdataset to wherever you saved the csv file. Also, note that only possibly predictive variables and target are included the training and testing data – not identifying information.\nThe prediction generates data for all 66 records (I’m showing only first 14)\n\nThe truth is the ‘actual’ and the response is the ’predicted”.\nLet’s see if we can combine original data with prediction:\n\nYup.\nOk let’s calculate accuracy of the model and create confusion matrix:\n\n98.4% accuracy!\n\n\n2.Embed this model in SQL Server\nNow that we have a good model, let’s proceed to the next major step of embedding it in SQL Server. To do this we will construct several tables and stored procedures in SQL Server. We need:\nTables:\n\nA table for the training data itself\nA table to serialize and store the model itself within SQL Server\n\nStored Procedures and SQL:\n\n Create, and serialize the model (do this once)- dbo.generate_jobclass_model\n SQL to save the model (do this once)\n\n\n\nUnserialize the model and run it against new data to get predictions (as often as you need predictions)- dbo.predict_joblass_new\nHave the capacity to run that prediction from outside of SQL Server (as often as you need predictions)- dbo.generatepgpred – called later by our application to run prediction\n\nI have created two tables in the jobclass database- jobclassinfo2 and jobclass_modelsJobclassinfo2 mirrors the csv file contents we showed previously. Jobclass_models has only 2 fields- the name and the serialized model.\nThe contents look like:\n\n\nHere is the stored procedure to create and serialize model and the sql to save it:\n\n\nBut we are not quite finished. We still need to create a stored procedure to use the model to predict paygrades on new data and the ability to run that stored procedure.\nThis SQL creates the prediction stored procedure:\n\nThis is the SQL to run it:\n\nYou will notice in the last one we tell it in a SQL Select statement where to read the new data from- a table called JobClassInformation. And from the former, we tell it to store the predictions in the JobClassPredictions table. Also, in the former, we tell SQL server to update the JobClassInformation table with the results of the predictions.\nFor those familiar with SQL- you can see the interplay necessary to embed R Code within SQL.\nMost of the above SQL above is patterned after the code that Microsoft provides itself for the examples it provides. I thank them for their tutorials on this.\n\n\n3.Create some tables in SQL server to store new data records and the predictions\nThe last step is to be able to store new data that needs predictions and the predictions. This was hinted at in the code in the last section.\nWe will create two new SQL tables – one for new data and one for predictions. These look very similar to the training data table:\nThe difference is the ‘response’ variable which is the prediction. It’s initially empty in the first, filled in the second one, and then updated in the first.\nThe reason for that is that in the final step- the ASP.NET MVC application we build – will be built around the JobClassInfomation table. That is the one end users will interact with. So we will want to see the results there.\n\n\n4. Create a quick ASP.NET MVC end-user web application to manage new data records and execute the predictive model and bring back results to the end user.\nThis is the final step that I did. I wanted to get to the point where the end user could make use of the power of machine learning and artificial intelligence with something that was user-friendly.\nThis required a few steps:\n1.Create an ASP.NET MVC application in Visual Studio 2017\n2.Add a data entity model based on the previously mentioned JobClassInformation table\n3.Scaffold ( generate code) from that table through the data entity model to create web pages that would list the records in the table and allow for the CRUD ( create,edit, update, and deletion) of records by the end user.\n4.Add to the list page a ‘link’ to the listing page to actually run the R predictive model behind the scenes and return to the results to the listing page.\nFor those familiar with ASP.NET MVC, they know the first three of the steps are fairly straightforward and doesn’t require them to write much if any code. For the fourth step, I made changes to the JobClassInformationsController and to the index view for that controller.\nJobClassInformationsController\nI added an additional ActionResult ‘GeneratePayGradePrediction’ for running the SQL stored procedure that generates the prediction.\n\nIndex View\nI also made changes to the Index view for JobClassInformation - I added the second Html.ActionLink to run the controller and present the link on the user screen:"
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#initial-screen",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#initial-screen",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "Initial Screen",
    "text": "Initial Screen"
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#manage-job-records",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#manage-job-records",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "Manage Job Records",
    "text": "Manage Job Records\n\nNotice the response column is empty."
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#edit-screen",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#edit-screen",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "Edit Screen",
    "text": "Edit Screen"
  },
  {
    "objectID": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#generate-prediction",
    "href": "posts/EmbedHRAnalytics/EmbedHRAnalytics.html#generate-prediction",
    "title": "Using Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example",
    "section": "Generate Prediction",
    "text": "Generate Prediction\nClick the generate prediction link on the list screen and ‘voila’ – the predictions are generated in the response column.\nAlthough I had several records here, even if you had just one new record, as long the Education Level through Financial Budget scores were filled out, it would generate a prediction in the response column based on that provided information."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html",
    "href": "posts/Wakeupcall/WakeupCAll.html",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "Over the last 35 years or so, I have had the privilege of working jointly in the HR and IT fields and in the interaction of those fields over that time period. Coupled with the foregoing has been an interest and a passion for human resources, information technology, and statistics- applying statistical analysis to HR data.\nThe combination of these experiences has enabled me to witness many changes in each of the above areas over that period of time. HR practices have increased in robustness and variety. Application of technology to the HR function has changed dramatically. This includes having:\n\nvery powerful HR information systems that support the bulk of the transactional activity that HR engages in in most areas/functions of HR\nusage of word processing, spreadsheet, presentation, and social media software at the individual user level in HR to increase their productivity at the individual level\nthe preparation of HR Metrics (in some organizations)- sometimes for internal Executive reporting purposes, and sometimes for external benchmarking purposes.\n\nIn spite of all the above changes, the one thing I have not seen as much of ##is robust statistical analysis of HR data for decision making purposes##. That’s not saying use of statistical analysis on HR data hasn’t existed. The areas most likely to have made some use of some statistical analysis – is probably the Labor/Industrial Relations area for collective bargaining purposes, Human Resource Forecasting, and perhaps Salary Administration/Job Classification for salary survey etc. type of purposes.\nBut for much of HR decisions and practices in organizations, the decisions have often been far more subjective than they needed to be, and many aspects of those HR practices at their ‘essence’ have not changed over the last 35 years even if some of the tools have. To some HR practitioners, they see HR as a ‘soft’, ‘people’, ‘non-technical’ type of work.\nWhile as compared to other staff areas such as Finance, Supply Chain, and Legal etc.- HR is perhaps more ‘people related’ because HR concerns itself with the organizational people related issues- this view of HR as a ‘soft’ area is incomplete. HR has also always been very analytical and technical. On a personal level, I have always believed that statistical analysis of HR data and its application in HR decision making had the potential for a much wider application in HR practices\nI recently came across an article that seems to support the above belief titled ’How Google is Using People Analytics to Completely Reinvent HR\nhttp://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\nThe article profiles Google as a company with respect to some of its People practices and the role of ‘People Analytics’ in those practices. While the article uses the term ‘people’ analytics, if you have ‘googled’ on analytics related to HR, you will likely notice that other terms will also come up including workforce analytics and HR analytics.\nWhile I encourage a read of the above noted article directly, I wanted to synthesize from this and some other sources googled from the internet in order to provide a bit wider picture of analytics as it applies to Human Resources and why I think it ** is a ‘wakeup call’ for HR **.\n\n\n\nOne of the things that is often the case in ‘leading’, ‘bleeding’ edges of any field is that terminology is often still in flux and working itself out in terms of meaning for a period of time.\nAs I started to do some review of the terminology, I found a variety of definitions for Workforce Analytics, HR Analytics, and People Analytics. These definitions all seemed to try cover similar ground albeit sometimes with some minor differences in nuance. At the present point in time, I see them as synonymous in terms of intent- but someone out there could correct me on that.\nLet’s take a look at some of these definitions first and then consider their similarities. The definitions below are intended to be illustrative not exhaustive.\nWorkforce Analytics\nFrom http://www.gartner.com/it-glossary/workforce-analytics\n##“Workforce analytics## is an advanced set of data analysis tools and metrics for comprehensive workforce performance measurement and improvement. It analyzes recruitment, staffing, training and development, personnel, and compensation and benefits, as well as standard ratios that consist of time to fill, cost per hire, accession rate, retention rate, add rate, replacement rate, time to start and offer acceptance rate.”\nHR Analytics\nFrom http://www.techopedia.com/definition/28334/human-resources-analytics-hr-analytics\nHuman resource analytics (HR analytics) is an area in the field of analytics that refers to applying analytic processes to the human resource department of an organization in the hope of improving employee performance and therefore getting a better return on investment. HR analytics does not just deal with gathering data on employee efficiency. Instead, it aims to provide insight into each process by gathering data and then using it to make relevant decisions about how to improve these processes.\n*What HR analytics does is correlate business data and people data, which can help establish important connections later on. The key aspect of HR analytics is to conclusively show the impact the HR department has on the organization as a whole. Establishing a cause-and-effect relationship between what HR does and business outcomes - and then creating strategies based on that information - is what HR analytics is all about.\nHR has core functions that can be enhanced by applying processes in analytics. These are acquisition, optimization, paying and developing the workforce of the organization. HR analytics can help to dig problems and issues surrounding these requirements and using analytical workflow will guide the managers to answer questions and gain insights from information at hand, then make relevant decisions and take appropriate actions.”*\nPeople Analytics\nFrom Dr. Sullivan’s article: http://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\n“##People analytics## is a data-driven approach to managing people at work. Those working in people analytics strive to bring data and sophisticated analysis to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job and team design, and compensation …\nIt is just listening to what your data says about you in the area of human resources. Data says a lot about how to run a company, and people analytics isn’t as scary as it sounds. It is really just the continuation of current trends in management, supporting them with data and informing action by insights derived from the data. It simply requires decisions to be supported by data, which is what we ask of most decision makers in a company already…\nThe basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decisions.”\n\n\n\nAs one looks over these definitions a number of similarities emerge which I think get at the heart of what analytics are and why they are important:\n\nWorkforce/HR/People analytics are a ‘data driven’ approach to managing people at work. What would be implied in this is that it isn’t data just for the sake of data, but data for a purpose. It implies that when you make HR decisions, you need to be able to show from your own internal research and data, why you are taking the actions you do\nThe scope of a ‘data driven’ approach covers ALL HR functions and practices. The potential of Analytics to ‘reinvent’ HR can impact everything. The scope is not just HR Department improvement but organizational improvement through the correlation of HR data to business data. It is about what we do, how we do it, and what impact that is really having on the organization.\n‘Data driven’ implies ‘data’. It may seem trite to say that, but to have ‘data’ means that we are measuring. This typically means metrics – not just transactional metrics but performance metrics as well. (not just what are we doing , but how well we are doing it)\nAnalytics and metrics aren’t just about HR data but are also about their tie to business data. It’s not just about HR Department improvement but organizational improvement through the correlation of HR data to business data. It is about what we do, how we do it, and what impact we are really having on the organization, as HR professionals.\nAnalytics are not just armchair empiricism, but rather the application of statistical analysis tools to HR data and metrics. It goes beyond a sense or a gut feel. At best a gut feel or sense is a ‘hypothesis’ that should be subject to confirmation by data.\nThe ‘purpose’ of analytics is for performance improvement whether at individual. HR Department/Operations or at the organizational/business outcome level. This means more accurate, more proactive (predictive) and less reactive decisions, quicker and more productive and efficient HR decisions.\nIn statistical terms, the scope of analytics could cover prediction, correlation, and process quality improvement tools and possibly other areas as well.\n\n\n\n\nIf Workforce/HR/People analytics truly have the potential to both reinvent and transform HR practices and business outcomes, the wakeup call might end up being the impact of HR and the organization not benefiting from that potential.\nConsider the following:\n\nYour own performance as an HR department and as an organization may be limited - comment provided in the Google people practices article above :\n’The basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decision’\n\nIf you aren’t collecting the data, measuring, and doing the analysis – how do you know you are producing superior results?\n\nYour own competitiveness may suffer. One of the more annoying trends impacting HR these days, for some, is the ‘justifying your existence’ as HR in your organization. Annoying particularly because it often feels like an organization thinks it can get along without HR and HR practices. In reality of course it can’t (any more than it can do without finance and accounting etc.).But ##the reality may be that it can get its HR activity done without ‘You’##. Are ‘You’ doing it better than the alternatives (contracting it out, reorganizing to create’ shared services organizations’, etc.)? Do you know? How do you know? I suppose the only thing worse that ‘justifying your existence’ is not even being given the opportunity to ‘justify your existence’ before restructuring decisions contract it out. If part of the picture of People analytics is HR process improvement analyses, and as an organization this isn’t built into being part of your HR operations, it would be difficult to show your competitiveness. This isn’t just an issue for HR but all organizational operations. Some organizations have the view that if you are not in direct delivery of the organization’s overall product or service – you are not part of the core business- as if organizations could operate without organizational infrastructure that keeps the organization operating smoothly. The other mistake I think organizations make here is that they may demand ‘justify your existence’ without those who are demanding having any data to prove that the alternatives are any better either. The point is HR significantly helps its circumstances when it knows and can show its internal competitiveness through metrics and show it compares to other organizations by benchmarking. And proactively doing this, ahead of an executive ‘ask’.\nThe means to have a ‘data driven’ approach isn’t new- but the execution of it and visibility of it is likely more recent. The basics to have ‘data driven’ HR: the HR function and practices, the ability to create and store data, the existence of statistical packages for analysis has been around for over 35 years in some form. The willingness to recognize this and have it become part of the DNA of what we do in HR and how we do it is what is new. This is probably an area of HR that has needed recognition for quite some time. The above Google People practices described are one example of this and its recency.\nThe current provision and use of HR metrics where it does exist and occur can sometimes be ‘insufficient’. I am a strong believer in the provision AND use of metrics. But insufficiencies can exist in both provision and use. In fact sometimes the provision affects the use.\nThe technology exists and is used in many organizations to create the HR metrics – data warehousing and business intelligence tools. Dashboards are created. And yet often the data is simply reviewed and no targeted action taken. Sometimes it’s because the metrics are not generated in a way that allows for the slicing and dicing and further analysis ##inside ##the organization. When that happens, there is no means to analyze the problem internally and take some targeted ‘data driven’ action. We get the use for ‘benchmarking’ (external use) right-comparison to other organizations, but we don’t get the internal picture right.\nBeing able to ‘take action’ implies that you are able to ‘interact’ with your data. There are many powerful visual tools that are emerging for use in data analysis and slicing and dicing in real time on your metrics. This interaction is often critical for ‘what if’ types of questions and analyses. However, where there can sometimes be an insufficiency here is knowing ‘what to pay attention to’ in the data. Part of what to pay attention to is based on what questions you are asking. But part of it is also using statistics, statistical tools and statistical analysis to determine what is statistically significant. This is important because we can create and process so much data that it can be easy to drown in it. And if we drown in it, it’s unlikely we will be able take targeted action as well.\n\nThere may be other reasons for this being appropriately perceived as a wake-up call as well. The above are just a few thoughts that come to mind.\n\n\n\nI suppose part of the answer to this question is based on whether you accept the premise that the Google People Practices are based on and that you want a ‘data driven’ approach to managing people at work:\nThe basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decisions.”\nIf you don’t agree with the premise and that you don’t see the need for managing people in organizations as needing to be data driven, it’s unlikely you will see this as a wakeup call and feel a need to respond. Ultimately that is every organization’s and HR practitioner’s decision and choice.\nIf you do decide that this is relevant to you personally as an HR practitioner or as an HR function in an organization, or as an organization in general, then I think there are many things that are conducive to moving in the direction of leveraging analytics to make ‘data driven’ people management decisions.\nHere are some thoughts:\n\nStart reading, researching and staying on top of developments in this area. Understand how this might apply to your specific specialty with in HR if you are a practitioner. Understand how this might apply to your operations if you are an HR leader. Be aware that much of the terminology and ideas are still in flux- and grow as the discipline grows. My own personal sense is that this is still a ‘leading’, ‘bleeding’ edge of HR activity and practice. As such, the definitions and practices are likely to be in flux and evolve over a period of time. What impresses me about the Google example above, is that it is refreshing, exciting and challenging. It is a real life example of actually bringing visibility to ‘data driven’ people decisions. Google will eventually be ##one## ##of a number of examples## as more organizations see the potential and start taking steps in a ‘data driven’ people management direction.\nTake claim to HR analytics as part of the HR domain. This isn’t a situation of ‘no’ decision being made. You make a decision either way- to either see HR analytics as part of HR and taking ownership and control of it, or have the risk of someone else taking ownership of it at some point. (That may or may not be HR). Some organizations make an argument it should be in a Finance area. I personally find that ludicrous. Finance often knows no more about the context of HR than HR does of Finance. Ownership should be in the area that has the domain knowledge. Organizational politics sometimes dictate otherwise. Take claim regardless of where you are in implementation or robustness.\nSee Workforce/HR/People Analytics simultaneously as both a separate discipline within HR and also as a core HR knowledge requirement at a basic level for all of HR. Some portions of this might require extensive separate knowledge by a specialized group in HR, and yet at the same some parts require a base understanding by all HR practitioners of what it is, why it is important, and a general sense of how it could/should be operationalized in your specific discipline within HR.\nSee your ‘hands on’ use of the technology to do this as both an extension of who you are as an HR professional and as a tool for increasing your competitiveness. Understand that because it is data driven, the hands on use of technology in HR is not optional but rather a requirement. Data driven means interaction with and exploring your data. In seeing HR and IT evolve over the last 3 decades, I have come across many situations where some HR practitioners have seen HR as non-technical, non-quantitative, non-data related. Or in some cases, even when they see the ‘data’ aspects of it, they think if it involves data it’s a non-professional or non-managerial function. I believe this is a wrong view of data and technology. As an HR practitioner at any level, understand that your competitiveness is partly a function of applying technology wherever you can effectively to do what you do better in HR. If you want HR to move into being data driven- technology needs to have hands on use.\nBe smart in your use (or not) of prepackaged vendor analytics software solutions. Don’t assume that if your current HRIS package has an ‘analytics’ feature, that that necessarily is valuable or means anything. Depending on what is provided it may or may not. The concern with ‘pre-packaged’ solutions is that they by their nature presume to know the data driven people decisions that exist and the questions that need to be asked. That may not necessarily be the case. Hands on interaction with your data through statistical tools, with your knowledge of your business and HR issues is much closer to the intent of Workforce/HR/People Analytics.\nIncrease your knowledge of statistics and statistical analysis. Understand that to extent that analytics often makes use of and needs to make use of advanced analytical tools, this typically implies statistical tools and statistical analysis. This in turn implies that you have an understanding of statistics and the how to use statistical tools and programs to do advanced analysis. One of the difficulties, I saw over many years is that in business diplomas and degrees, if statistics was taught as part of the curriculum at all, it was usually an elective taught by a science faculty. Often then, the examples how statistical analysis could be used in a relevant way were sketchy at best, because the examples were often directly scientific. For me personally, I did not understand the relevancy of statistics and statistical analysis until it was taught as applied ‘Business’ statistical analysis. Then the applicability of it for use in solving business problems became visible for the first time. ‘Context’ is everything. Statistical analysis is not only relevant but imperative in improving how we do what we do in HR. As HR practitioners, if you don’t already have a background in statistics and the use of statistical packages, this is critical to having People Management decisions being conducive to being data driven. Access to statistical packages these days is no obstacle. The R statistics software package on the internet is ##free##, and is available for installation on Windows, Macintosh and Linux operating systems.\nPossibly change your paradigm of HR. For many HR practitioners, HR is often seen as strictly the set of practices and methodologies used in HR that form the HR function in an organization (a series of silos and a top down orientation). In that paradigm, practices and methodologies are what they are irrespective to any connection to organizational outcomes, and they don’t have to be justified. You often see examples of this on blogs and discussion groups where the merit of a particular approach is touted – but it is done without any discussion of its use being tied to evidence on wider organizational outcomes. What may need to change for some organizations is seeing the entirety of HR in a service process model paradigm. One way of understanding this is to put the ‘vertical’ silos of HR on their side horizontally. If we borrow from the quality improvement world, think of SIPOC ( Suppliers- Inputs-Process-Outputs-Customers) See the entirety of HR silos as really the provision of ‘services’ to customers. To the right are customers. Those customers have expectations of the outputs (services). Those services have processes that exist that allow the services to be provided. The processes in turn have inputs. These would be to the left. The inputs in turn come from suppliers. The processes may or may not be working well. They may not be producing quality service output. The efficacy of any specific HR approach, practice, or methodology is also partly a function of the degree to which organizational performance is enhanced by its use (customers more satisfied with services provided). We then cannot talk about the merits of one approach over another, without also tying it to organizational outcomes. Data driven? –YES.\n\n\n\n\nAs I have suggested in the above, I believe Workforce/ HR/ People analytics are still in their early stages of use and terminology in HR. Even with that being the case, what is at the heart of the use and terminology for this is ‘data driven’ decision making. ‘Data driven’ goes beyond simply producing metrics. It is:\n\nBeing able to take a HR issue or question, understand what data exists around and available for that decision, understand what statistical analyses or summaries are useful to answer that question, presenting that data as part of the decision and decision making process\nUnderstanding for our existing metrics, why we produce them in the first place and what questions they were intended to answer.\nBeing able to take overall metrics, measures, and data and be able to slice and dice understand underlying relationships.\nMaking a decision as HR professionals to be ‘data driven’. As I mentioned earlier in this article, the ability to generate datasets of HR data and availability of applying statistical tools has been around for at least 35 years. The technologies have changed and have become more robust, and the amount and scope of data has increased over that period of time. The tools and data needed to do this are not necessarily an obstacle.\n\nGoogle is indeed doing some tremendously exciting things in their People practices. They have taken the bold move to increase dramatically their ability to be ‘data driven’ in their HR practices. And they will likely continue to see payoffs for their decisions.\nI guess the key question is- how will Workforce, HR, People Analytics impact you and your organization?\n\n\n\n\n\n\nAbout Lyndon Sundmark, MBA\nLyndon is a retired HR Professional with over 40 years experience of applying a ‘data-driven’, ‘evidence based’ mindset to HR practices in organizations in a variety of roles and industries."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#introduction",
    "href": "posts/Wakeupcall/WakeupCAll.html#introduction",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "Over the last 35 years or so, I have had the privilege of working jointly in the HR and IT fields and in the interaction of those fields over that time period. Coupled with the foregoing has been an interest and a passion for human resources, information technology, and statistics- applying statistical analysis to HR data.\nThe combination of these experiences has enabled me to witness many changes in each of the above areas over that period of time. HR practices have increased in robustness and variety. Application of technology to the HR function has changed dramatically. This includes having:\n\nvery powerful HR information systems that support the bulk of the transactional activity that HR engages in in most areas/functions of HR\nusage of word processing, spreadsheet, presentation, and social media software at the individual user level in HR to increase their productivity at the individual level\nthe preparation of HR Metrics (in some organizations)- sometimes for internal Executive reporting purposes, and sometimes for external benchmarking purposes.\n\nIn spite of all the above changes, the one thing I have not seen as much of ##is robust statistical analysis of HR data for decision making purposes##. That’s not saying use of statistical analysis on HR data hasn’t existed. The areas most likely to have made some use of some statistical analysis – is probably the Labor/Industrial Relations area for collective bargaining purposes, Human Resource Forecasting, and perhaps Salary Administration/Job Classification for salary survey etc. type of purposes.\nBut for much of HR decisions and practices in organizations, the decisions have often been far more subjective than they needed to be, and many aspects of those HR practices at their ‘essence’ have not changed over the last 35 years even if some of the tools have. To some HR practitioners, they see HR as a ‘soft’, ‘people’, ‘non-technical’ type of work.\nWhile as compared to other staff areas such as Finance, Supply Chain, and Legal etc.- HR is perhaps more ‘people related’ because HR concerns itself with the organizational people related issues- this view of HR as a ‘soft’ area is incomplete. HR has also always been very analytical and technical. On a personal level, I have always believed that statistical analysis of HR data and its application in HR decision making had the potential for a much wider application in HR practices\nI recently came across an article that seems to support the above belief titled ’How Google is Using People Analytics to Completely Reinvent HR\nhttp://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\nThe article profiles Google as a company with respect to some of its People practices and the role of ‘People Analytics’ in those practices. While the article uses the term ‘people’ analytics, if you have ‘googled’ on analytics related to HR, you will likely notice that other terms will also come up including workforce analytics and HR analytics.\nWhile I encourage a read of the above noted article directly, I wanted to synthesize from this and some other sources googled from the internet in order to provide a bit wider picture of analytics as it applies to Human Resources and why I think it ** is a ‘wakeup call’ for HR **."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#what-exactly-is-workforcehrpeople-analytics",
    "href": "posts/Wakeupcall/WakeupCAll.html#what-exactly-is-workforcehrpeople-analytics",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "One of the things that is often the case in ‘leading’, ‘bleeding’ edges of any field is that terminology is often still in flux and working itself out in terms of meaning for a period of time.\nAs I started to do some review of the terminology, I found a variety of definitions for Workforce Analytics, HR Analytics, and People Analytics. These definitions all seemed to try cover similar ground albeit sometimes with some minor differences in nuance. At the present point in time, I see them as synonymous in terms of intent- but someone out there could correct me on that.\nLet’s take a look at some of these definitions first and then consider their similarities. The definitions below are intended to be illustrative not exhaustive.\nWorkforce Analytics\nFrom http://www.gartner.com/it-glossary/workforce-analytics\n##“Workforce analytics## is an advanced set of data analysis tools and metrics for comprehensive workforce performance measurement and improvement. It analyzes recruitment, staffing, training and development, personnel, and compensation and benefits, as well as standard ratios that consist of time to fill, cost per hire, accession rate, retention rate, add rate, replacement rate, time to start and offer acceptance rate.”\nHR Analytics\nFrom http://www.techopedia.com/definition/28334/human-resources-analytics-hr-analytics\nHuman resource analytics (HR analytics) is an area in the field of analytics that refers to applying analytic processes to the human resource department of an organization in the hope of improving employee performance and therefore getting a better return on investment. HR analytics does not just deal with gathering data on employee efficiency. Instead, it aims to provide insight into each process by gathering data and then using it to make relevant decisions about how to improve these processes.\n*What HR analytics does is correlate business data and people data, which can help establish important connections later on. The key aspect of HR analytics is to conclusively show the impact the HR department has on the organization as a whole. Establishing a cause-and-effect relationship between what HR does and business outcomes - and then creating strategies based on that information - is what HR analytics is all about.\nHR has core functions that can be enhanced by applying processes in analytics. These are acquisition, optimization, paying and developing the workforce of the organization. HR analytics can help to dig problems and issues surrounding these requirements and using analytical workflow will guide the managers to answer questions and gain insights from information at hand, then make relevant decisions and take appropriate actions.”*\nPeople Analytics\nFrom Dr. Sullivan’s article: http://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\n“##People analytics## is a data-driven approach to managing people at work. Those working in people analytics strive to bring data and sophisticated analysis to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job and team design, and compensation …\nIt is just listening to what your data says about you in the area of human resources. Data says a lot about how to run a company, and people analytics isn’t as scary as it sounds. It is really just the continuation of current trends in management, supporting them with data and informing action by insights derived from the data. It simply requires decisions to be supported by data, which is what we ask of most decision makers in a company already…\nThe basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decisions.”"
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#what-can-we-glean-from-these-definitions",
    "href": "posts/Wakeupcall/WakeupCAll.html#what-can-we-glean-from-these-definitions",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "As one looks over these definitions a number of similarities emerge which I think get at the heart of what analytics are and why they are important:\n\nWorkforce/HR/People analytics are a ‘data driven’ approach to managing people at work. What would be implied in this is that it isn’t data just for the sake of data, but data for a purpose. It implies that when you make HR decisions, you need to be able to show from your own internal research and data, why you are taking the actions you do\nThe scope of a ‘data driven’ approach covers ALL HR functions and practices. The potential of Analytics to ‘reinvent’ HR can impact everything. The scope is not just HR Department improvement but organizational improvement through the correlation of HR data to business data. It is about what we do, how we do it, and what impact that is really having on the organization.\n‘Data driven’ implies ‘data’. It may seem trite to say that, but to have ‘data’ means that we are measuring. This typically means metrics – not just transactional metrics but performance metrics as well. (not just what are we doing , but how well we are doing it)\nAnalytics and metrics aren’t just about HR data but are also about their tie to business data. It’s not just about HR Department improvement but organizational improvement through the correlation of HR data to business data. It is about what we do, how we do it, and what impact we are really having on the organization, as HR professionals.\nAnalytics are not just armchair empiricism, but rather the application of statistical analysis tools to HR data and metrics. It goes beyond a sense or a gut feel. At best a gut feel or sense is a ‘hypothesis’ that should be subject to confirmation by data.\nThe ‘purpose’ of analytics is for performance improvement whether at individual. HR Department/Operations or at the organizational/business outcome level. This means more accurate, more proactive (predictive) and less reactive decisions, quicker and more productive and efficient HR decisions.\nIn statistical terms, the scope of analytics could cover prediction, correlation, and process quality improvement tools and possibly other areas as well."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#why-might-this-be-a-wake-up-call-for-hr",
    "href": "posts/Wakeupcall/WakeupCAll.html#why-might-this-be-a-wake-up-call-for-hr",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "If Workforce/HR/People analytics truly have the potential to both reinvent and transform HR practices and business outcomes, the wakeup call might end up being the impact of HR and the organization not benefiting from that potential.\nConsider the following:\n\nYour own performance as an HR department and as an organization may be limited - comment provided in the Google people practices article above :\n’The basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decision’\n\nIf you aren’t collecting the data, measuring, and doing the analysis – how do you know you are producing superior results?\n\nYour own competitiveness may suffer. One of the more annoying trends impacting HR these days, for some, is the ‘justifying your existence’ as HR in your organization. Annoying particularly because it often feels like an organization thinks it can get along without HR and HR practices. In reality of course it can’t (any more than it can do without finance and accounting etc.).But ##the reality may be that it can get its HR activity done without ‘You’##. Are ‘You’ doing it better than the alternatives (contracting it out, reorganizing to create’ shared services organizations’, etc.)? Do you know? How do you know? I suppose the only thing worse that ‘justifying your existence’ is not even being given the opportunity to ‘justify your existence’ before restructuring decisions contract it out. If part of the picture of People analytics is HR process improvement analyses, and as an organization this isn’t built into being part of your HR operations, it would be difficult to show your competitiveness. This isn’t just an issue for HR but all organizational operations. Some organizations have the view that if you are not in direct delivery of the organization’s overall product or service – you are not part of the core business- as if organizations could operate without organizational infrastructure that keeps the organization operating smoothly. The other mistake I think organizations make here is that they may demand ‘justify your existence’ without those who are demanding having any data to prove that the alternatives are any better either. The point is HR significantly helps its circumstances when it knows and can show its internal competitiveness through metrics and show it compares to other organizations by benchmarking. And proactively doing this, ahead of an executive ‘ask’.\nThe means to have a ‘data driven’ approach isn’t new- but the execution of it and visibility of it is likely more recent. The basics to have ‘data driven’ HR: the HR function and practices, the ability to create and store data, the existence of statistical packages for analysis has been around for over 35 years in some form. The willingness to recognize this and have it become part of the DNA of what we do in HR and how we do it is what is new. This is probably an area of HR that has needed recognition for quite some time. The above Google People practices described are one example of this and its recency.\nThe current provision and use of HR metrics where it does exist and occur can sometimes be ‘insufficient’. I am a strong believer in the provision AND use of metrics. But insufficiencies can exist in both provision and use. In fact sometimes the provision affects the use.\nThe technology exists and is used in many organizations to create the HR metrics – data warehousing and business intelligence tools. Dashboards are created. And yet often the data is simply reviewed and no targeted action taken. Sometimes it’s because the metrics are not generated in a way that allows for the slicing and dicing and further analysis ##inside ##the organization. When that happens, there is no means to analyze the problem internally and take some targeted ‘data driven’ action. We get the use for ‘benchmarking’ (external use) right-comparison to other organizations, but we don’t get the internal picture right.\nBeing able to ‘take action’ implies that you are able to ‘interact’ with your data. There are many powerful visual tools that are emerging for use in data analysis and slicing and dicing in real time on your metrics. This interaction is often critical for ‘what if’ types of questions and analyses. However, where there can sometimes be an insufficiency here is knowing ‘what to pay attention to’ in the data. Part of what to pay attention to is based on what questions you are asking. But part of it is also using statistics, statistical tools and statistical analysis to determine what is statistically significant. This is important because we can create and process so much data that it can be easy to drown in it. And if we drown in it, it’s unlikely we will be able take targeted action as well.\n\nThere may be other reasons for this being appropriately perceived as a wake-up call as well. The above are just a few thoughts that come to mind."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#how-might-hr-respond-to-this",
    "href": "posts/Wakeupcall/WakeupCAll.html#how-might-hr-respond-to-this",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "I suppose part of the answer to this question is based on whether you accept the premise that the Google People Practices are based on and that you want a ‘data driven’ approach to managing people at work:\nThe basic premise of the “people analytics” approach is that accurate people management decisions are the most important and impactful decisions that a firm can make. You simply can’t produce superior business results unless your managers are making accurate people management decisions.”\nIf you don’t agree with the premise and that you don’t see the need for managing people in organizations as needing to be data driven, it’s unlikely you will see this as a wakeup call and feel a need to respond. Ultimately that is every organization’s and HR practitioner’s decision and choice.\nIf you do decide that this is relevant to you personally as an HR practitioner or as an HR function in an organization, or as an organization in general, then I think there are many things that are conducive to moving in the direction of leveraging analytics to make ‘data driven’ people management decisions.\nHere are some thoughts:\n\nStart reading, researching and staying on top of developments in this area. Understand how this might apply to your specific specialty with in HR if you are a practitioner. Understand how this might apply to your operations if you are an HR leader. Be aware that much of the terminology and ideas are still in flux- and grow as the discipline grows. My own personal sense is that this is still a ‘leading’, ‘bleeding’ edge of HR activity and practice. As such, the definitions and practices are likely to be in flux and evolve over a period of time. What impresses me about the Google example above, is that it is refreshing, exciting and challenging. It is a real life example of actually bringing visibility to ‘data driven’ people decisions. Google will eventually be ##one## ##of a number of examples## as more organizations see the potential and start taking steps in a ‘data driven’ people management direction.\nTake claim to HR analytics as part of the HR domain. This isn’t a situation of ‘no’ decision being made. You make a decision either way- to either see HR analytics as part of HR and taking ownership and control of it, or have the risk of someone else taking ownership of it at some point. (That may or may not be HR). Some organizations make an argument it should be in a Finance area. I personally find that ludicrous. Finance often knows no more about the context of HR than HR does of Finance. Ownership should be in the area that has the domain knowledge. Organizational politics sometimes dictate otherwise. Take claim regardless of where you are in implementation or robustness.\nSee Workforce/HR/People Analytics simultaneously as both a separate discipline within HR and also as a core HR knowledge requirement at a basic level for all of HR. Some portions of this might require extensive separate knowledge by a specialized group in HR, and yet at the same some parts require a base understanding by all HR practitioners of what it is, why it is important, and a general sense of how it could/should be operationalized in your specific discipline within HR.\nSee your ‘hands on’ use of the technology to do this as both an extension of who you are as an HR professional and as a tool for increasing your competitiveness. Understand that because it is data driven, the hands on use of technology in HR is not optional but rather a requirement. Data driven means interaction with and exploring your data. In seeing HR and IT evolve over the last 3 decades, I have come across many situations where some HR practitioners have seen HR as non-technical, non-quantitative, non-data related. Or in some cases, even when they see the ‘data’ aspects of it, they think if it involves data it’s a non-professional or non-managerial function. I believe this is a wrong view of data and technology. As an HR practitioner at any level, understand that your competitiveness is partly a function of applying technology wherever you can effectively to do what you do better in HR. If you want HR to move into being data driven- technology needs to have hands on use.\nBe smart in your use (or not) of prepackaged vendor analytics software solutions. Don’t assume that if your current HRIS package has an ‘analytics’ feature, that that necessarily is valuable or means anything. Depending on what is provided it may or may not. The concern with ‘pre-packaged’ solutions is that they by their nature presume to know the data driven people decisions that exist and the questions that need to be asked. That may not necessarily be the case. Hands on interaction with your data through statistical tools, with your knowledge of your business and HR issues is much closer to the intent of Workforce/HR/People Analytics.\nIncrease your knowledge of statistics and statistical analysis. Understand that to extent that analytics often makes use of and needs to make use of advanced analytical tools, this typically implies statistical tools and statistical analysis. This in turn implies that you have an understanding of statistics and the how to use statistical tools and programs to do advanced analysis. One of the difficulties, I saw over many years is that in business diplomas and degrees, if statistics was taught as part of the curriculum at all, it was usually an elective taught by a science faculty. Often then, the examples how statistical analysis could be used in a relevant way were sketchy at best, because the examples were often directly scientific. For me personally, I did not understand the relevancy of statistics and statistical analysis until it was taught as applied ‘Business’ statistical analysis. Then the applicability of it for use in solving business problems became visible for the first time. ‘Context’ is everything. Statistical analysis is not only relevant but imperative in improving how we do what we do in HR. As HR practitioners, if you don’t already have a background in statistics and the use of statistical packages, this is critical to having People Management decisions being conducive to being data driven. Access to statistical packages these days is no obstacle. The R statistics software package on the internet is ##free##, and is available for installation on Windows, Macintosh and Linux operating systems.\nPossibly change your paradigm of HR. For many HR practitioners, HR is often seen as strictly the set of practices and methodologies used in HR that form the HR function in an organization (a series of silos and a top down orientation). In that paradigm, practices and methodologies are what they are irrespective to any connection to organizational outcomes, and they don’t have to be justified. You often see examples of this on blogs and discussion groups where the merit of a particular approach is touted – but it is done without any discussion of its use being tied to evidence on wider organizational outcomes. What may need to change for some organizations is seeing the entirety of HR in a service process model paradigm. One way of understanding this is to put the ‘vertical’ silos of HR on their side horizontally. If we borrow from the quality improvement world, think of SIPOC ( Suppliers- Inputs-Process-Outputs-Customers) See the entirety of HR silos as really the provision of ‘services’ to customers. To the right are customers. Those customers have expectations of the outputs (services). Those services have processes that exist that allow the services to be provided. The processes in turn have inputs. These would be to the left. The inputs in turn come from suppliers. The processes may or may not be working well. They may not be producing quality service output. The efficacy of any specific HR approach, practice, or methodology is also partly a function of the degree to which organizational performance is enhanced by its use (customers more satisfied with services provided). We then cannot talk about the merits of one approach over another, without also tying it to organizational outcomes. Data driven? –YES."
  },
  {
    "objectID": "posts/Wakeupcall/WakeupCAll.html#final-thoughts",
    "href": "posts/Wakeupcall/WakeupCAll.html#final-thoughts",
    "title": "Workforce/HR/People Analytics- Is This a Wake-Up Call for HR?",
    "section": "",
    "text": "As I have suggested in the above, I believe Workforce/ HR/ People analytics are still in their early stages of use and terminology in HR. Even with that being the case, what is at the heart of the use and terminology for this is ‘data driven’ decision making. ‘Data driven’ goes beyond simply producing metrics. It is:\n\nBeing able to take a HR issue or question, understand what data exists around and available for that decision, understand what statistical analyses or summaries are useful to answer that question, presenting that data as part of the decision and decision making process\nUnderstanding for our existing metrics, why we produce them in the first place and what questions they were intended to answer.\nBeing able to take overall metrics, measures, and data and be able to slice and dice understand underlying relationships.\nMaking a decision as HR professionals to be ‘data driven’. As I mentioned earlier in this article, the ability to generate datasets of HR data and availability of applying statistical tools has been around for at least 35 years. The technologies have changed and have become more robust, and the amount and scope of data has increased over that period of time. The tools and data needed to do this are not necessarily an obstacle.\n\nGoogle is indeed doing some tremendously exciting things in their People practices. They have taken the bold move to increase dramatically their ability to be ‘data driven’ in their HR practices. And they will likely continue to see payoffs for their decisions.\nI guess the key question is- how will Workforce, HR, People Analytics impact you and your organization?\n\n\n\n\n\n\nAbout Lyndon Sundmark, MBA\nLyndon is a retired HR Professional with over 40 years experience of applying a ‘data-driven’, ‘evidence based’ mindset to HR practices in organizations in a variety of roles and industries."
  },
  {
    "objectID": "posts/Contrarian/Contrarian.html",
    "href": "posts/Contrarian/Contrarian.html",
    "title": "People/HR Analytics - What Is your Paradigm ? It Matters",
    "section": "",
    "text": "TL;DR; People/HR Analytics long term sustainability into the future does not rest on continuing to promote and treat it as an add-on to HR but on the fundamental retooling of HR practices, methodologies, and education within the HR profession itself so that it becomes the DNA of how we ‘do’ HR."
  },
  {
    "objectID": "posts/Contrarian/Contrarian.html#what-typifies-the-add-on-paradigm",
    "href": "posts/Contrarian/Contrarian.html#what-typifies-the-add-on-paradigm",
    "title": "People/HR Analytics - What Is your Paradigm ? It Matters",
    "section": "What Typifies The ‘Add-On’ Paradigm ?",
    "text": "What Typifies The ‘Add-On’ Paradigm ?\nIn the ‘add-on’ paradigm, the way in which people / HR analytics is defined usually as an activity separate from either existing HR functions or activities. It sees it as ‘gathering employee information and carrying out analysis and processing of that information with the hopes that this information will impact business outcomes’.\nThe way in which it is suggested to be operationalized in organizations is to create a specially functioning HR analytics team charged with the responsibility of making breakthrough discoveries that will impact business outcomes. This team is distinct from all other ‘traditional’ functional HR teams.\nTo operationalize this often requires a business case to be made for the resources to bring this team into existence- since it is perceived to be an ‘add-on’ function to the traditional functions that already exist in HR. In most cases-inherent in that business case (and the hopefully subsequently obtained resources) is an implied expectation that this investment of additional resources will pay off. In effect the analytics team needs to ‘produce’ and ‘keep on producing’ to retain its existence.\nSome of the first deliverables are the automation of HR metrics through the use of Business Intelligence technologies and tools- including data warehousing and dashboards. Often these initiatives make the provision and visibility of HR information to decision makers in organizations both efficient and timely for the first time. For some organizations they equate these initiatives as being the whole of People / HR Analytics. These initiatives can be terribly alluring because it is both a tangible and visible deliverable. Its the starting point of HR Analytics in many organizations and often it should be.\nOnce these are in place, the attention then often focuses then on what kinds of questions are we able to look at and answer with this information that would have been too cumbersome or impossible before.\nThe expectations are that this team will get the attention of executive decision makers with breakthrough insights that will fundamentally improve and optimize business performance, and that the team will keep on delivering these insights.\nIn fact the criteria for success in this paradigm will often be exactly that- continual delivery of new breakthrough insights. After all, scarce organizational resources have been committed and this team must keep on delivering and impressing or its ‘why did we do this in the first place?’ time. The team must earn its keep and keep on jusstifying its existence.\nYou can tell by the description above, that everything about this paradigm shouts ‘add-on’- from its definition, to its assumptions, operationalization , expectations, and measures of success.\n\nHow does this compare/contrast with the alternate paradigm?\nIn the alternate paradigm, People/ HR Analytics is not seen as a separate activity or function of HR.\nIn this paradigm, People/ HR Analytics is defined as ‘data-driven’, ‘evidence-based’ HR Management and Decision Making. This means several things:\n\nIts scope covers ANYTHING related to HR Management and Decision Making.\n\nHR management and decision making is impacted by the analysis of data generated by the interaction of people with the organization- their onboarding (applicants), their interactions while within the organization, and their exit (terminations). BUT it also includes the scope of ALL HRIS information. This is much more than just employee and applicant information.\nHR management and decision making is ALSO impacted by how well we conduct the operations of business of HR - the provision of HR Services. This means looking at ourselves as a business within a business -with our own internal customers and collectiing and analyzing information on the provision of those services.\nFinally, HR management and decision making is impacted by the degree to which we pay attention to improving the very practices and methodologies we use in HR injecting ‘data-driven, evidence based’ into these wherever possible to improve the accuracy, reliability of them and to enhance the explainability of and trust in their results.This too requires the generation and analysis of data. We concern ourselves not only with what we do and how well we do it, but on exactly how we do it and improve that. We seek to leverage AI and machine learning to assist and change how we do what we do.\n\nThis paradigm does not assume a separate ‘add-on’ function. In fact it recognizes /sees People /HR Analytics as a change in and a choice of how we conduct the business of HR we are already in- HR Management and Decision Making which are ‘data-driven’, ‘evidence-based’. Historically, HR decision making and management have been notoriously subjective and non data driven, with possibly the exception of information used in collective bargaining, salary surveys, and workforce forecasting/ planning where practiced. In most recruitment activities, performance appraisal, job descriptions -data drivenness evidence-based has been minimal or non-existant.\nThis paradigm does not automatically assume that extra resources will be necessary to engage in data-driven, evidence-based HR Management and Decision Making. Initially much of this paradigm can be operationalized within existing resources."
  },
  {
    "objectID": "posts/DDHROperations/DDHROperations.html",
    "href": "posts/DDHROperations/DDHROperations.html",
    "title": "Data-Driven’ HR Operations",
    "section": "",
    "text": "TL;DR;Data Driven HR Operations is an imperative if as HR we are ever going to get an accurate picture of our performance, our customers, and the continous improvement of HR Services\n\n\n\n\nIntroduction\nOver the last few articles that I have written, I have talked about the criticality of being ‘data driven’ in HR to achieve what Workforce/HR/People Analytics desires to do for organizations- that being to improve the performance of the organization through the improvement of HR practices and processes.\nI have also mentioned that if we want to get our heads around what the ‘totality’ of scope of ‘data driven’ for HR it can be helpful to think of HR metrics in the following way:\n\nMetrics covering HR activity- what is actually going on with our human resources- i.e. traditional HR metrics such as turnover, hiring rates, absenteeism to name a few.\nMetrics covering HR operations themselves- how good are we actually in how we operate\nMetrics covering HR practices and methodology itself-deeply embedding metrics in our practices- how we actually do what we do as HR The above would cover much if not most of the domain of ‘data driven’ in HR.\n\nThis article will expand on the middle bullet point –Data Driven HR operations. What does this look like? What is necessary in terms of measurement to be data driven here? Can the R Statistics software help us here too?\n\n\nDefinition\nFirst of all, it might be important to define, even if somewhat informally, what ‘data driven HR operations’ means. In keeping with the above intention of Workforce/HR/People analytics, it’s when we are capturing information about our HR operations with an intent to improve performance- in this case- specific to HR as a group/function within the organization. This is easy to put into words. Operationalization of it – not so much.\nMost of us get the HR operations/ group/ function part. This is how most of us traditionally see ourselves- as an HR group- the aggregate of HR functions to help manage people in our organization. These include recruitment, hiring, training, compensation, labor relations, and performance evaluation as examples. And we understand ‘performance’ if it’s meant in the context of performance appraisals or evaluation. If this is your perspective of yourselves as HR, it is the ‘traditional’ perspective of HR both by the organization (rightly or wrongly) and often by HR itself. That ‘traditional’ perspective however is not necessarily conducive to being ‘data driven’. Here’s why:\n\nIt may cause HR to see itself as a series of functions rather than as a business. If it doesn’t see itself as a business – it may not understand that it has customers that it is accountable to. And those customers have needs.\nThe primary focus, especially if resources are tight, may be to simply get ‘our job done’- in other words get the requested activity finished and move on to the next. We might have neither the time nor the interest in knowing how well we did in the provision of our activity.\nWe may be myopic – in that – even if we are concerned with how well we did, we may achieve what we are doing at the expense of other areas of HR that are related to our part of HR, but separate from our specific HR function.\nThe traditional view of ourselves simply has no ‘natural’ impetus to ‘measure’ how well we are doing built into itself. We might see performance appraisal as the only ‘needed’ measurements of ourselves- and we do that on ourselves and expect the rest of the organization to carry this out- regardless of how much we do or don’t enjoy that process. And interesting enough, performance appraisals are neither visible nor relevant to our customers.\n\nIf the above points have even a remote ring of ‘truth’ to them in our collective HR experience, we need to think differently about HR. How do we do this? It means a different set of assumptions and a different model of HR operations.\n\n\nSome Foundational HR Assumptions\n\nWe should stop seeing HR itself as a function, and more as a business.\nA business has a reason to exist. It exists to provide a product or service.\nThe product or service has customers.\nAs HR we don’t ‘manufacture a product’- so we’re in business to ‘provide a service’.\nCustomers may or may not be happy with our service. If they are not satisfied with the quality of our service, they may cease to request it and look elsewhere. If enough of this is the case, we can go out of business (contracting out, outsourcing, offshoring).\nThe quality of our service is actually directly a function of the design of processes that exist(s) to provide/perform the service. Whether those processes are documented anywhere or understood- the end result of the provision of a service is not independent of a process that provides the service. The quality of our process impacts the quality of the service provided.\nWe can’t know what the quality of our service is unless we are asking the customer.\nChanging the quality of our service means we need to change our process(es)\nWe can’t ask the customer if we don’t know who they are\nWe don’t know who they are unless we are recording that somewhere\nWe can’t remember their feedback unless we are recording it\nEvery request we receive in HR whether it be by phone or email is a Service Request for provision of an HR service.\nEvery HR Service Request is a moment of truth with our customer- we have either both satisfied and delighted our customer or we haven’t.\nOur performing/completion of a request takes time. That time is categorized into wait time and actual performance time. The first is how long from the time we received it, that we actually started it. The performance time is from when we actually started it till actually completing it.\nThe total time it takes from receipt to completion is a function of the amount of resources available to provide the service.\nCustomers don’t appreciate wait time.\nWe can’t know what the total time is unless we are measuring it.\nWe can’t really improve anything if we aren’t measuring it.\n\nDo we agree?\nIf so, we can’t read through the above list and not conclude that it screams a need for ‘data’. We can’t achieve any of the above without data, and any improvements to our HR Services will be ‘driven’ by that ‘data’. Being driven by that data means knowing what to collect, collecting it, and then knowing how to analyze it to improve out HR operations.\nThe rest of this blog article cover those areas. Like many of my previous articles, I will touch briefly on these areas- for purposes of whetting your appetite for ‘Data Driven’ HR. One could write books on the detail of this. Indeed the ‘analysis part’ of our HR operations data is covered by the field of quality improvement- for which many books have been written over the few decades. My intent is to describe the basics- as a beginning to understanding the applicability of this area to data driven HR operations.\nTo understand the ‘what to collect’ and ‘how to collect it’ we need to think about what requirements seem to be implied by the above assumptions, the suggested minimum data content, and consider what approaches are possible to collect the data.\n\n\nData Requirements\n\nAs much as is feasible and possible, we need to capture and record every interaction with our customers.\nWe need to know the details of the request for our service\nWe need to know who the customer was that requested it\nWe need to know exactly when the request came in\nWe need to know when the request was actually started in terms of being worked on\nWe need to know when the request was completed\nWe need to know who worked on it.\nWe need to know how satisfied the customer was with the service\nWe need to have the means to calculate the total time it took from when we received the request till when it was completed.\nWe need to have the means to calculate how much of that time was wait time and how much was actual performing of the services.\n\n\n\nSuggested Minimum Data Content\nThe above data requirements would seem to suggest the following data content that needs to be captured:\n\nA transaction id that helps differentiate each customer’s request from each other\nA request heading or title – for quick reference\nThe request details- or detailed information\nThe requestor’s identity\nThe requestor’s contact information ( not so much for analysis- but for managing the request through to completion)\nThe category of the service (i.e. was it a recruitment request, a job classification request, a training request, a collective agreement interpretation etc.). Different types of services being provided might take different lengths of time to complete depending on the complexity of the request or the service provided. We will likely need to know this to make improvements later\nThe identity of the person who worked on the request.\nThe DateTime the request was received\nThe DateTime the request was started\nThe DateTime the request was completed.\nThe elapsed total time of the requested (derived from DateTime request completed – DateTime request received.\nThe wait time (derived from DateTime Started- DateTime Received)\nThe performance time (derived from DateTime completed- DateTime- started)\nCustomer satisfaction scale(at a minimum possibly a 5 point scale from extremely dissatisfied to extremely satisfied/delighted)\n\nAt an absolute minimum, the capturing of this information would allow us to know:\n\nwhat interactions we are having with our customer\nwho are customers are\nhow well we are performing at least in terms of time\nmanaging the services requests through their lifetime\nwhat our customers think of the service we provided.\n\n\n\nApproaches to Collect Data\nUltimately, I suppose, how you collect that data is up to you. Having said that, at least in my own experience, the best approaches are ones where the data needed is collected as a natural part of our HR business processes. This is likely always to be better than assigning ‘someone’ and ‘some process’ to capture this information after the fact for analytical purposes. It provides an ongoing real time picture to HR and it’s not onerous- so it is likely to collect this information reliably and efficiently.\nBut how can this ‘as part of the process’ come about? Well, it’s helpful to think in terms of customer relationship management (CRM) systems and Service Ticket categories of software. The idea behind these software approaches is that ALL interactions between service provider and customer is through these systems. All interaction is logged. When statuses of the service requests (tickets) have changed, these are all logged. As requests are received, processed and completed- they are managed through systems such as these. If you consider these, you still need to do your due diligence to determine whether they will work for you, and whether they capture the necessary information you need.\nI used the above categories of software as illustrations of a way of thinking. As HR we too are a ‘Service Desk’. You don’t necessarily have to ‘buy’ the above categories of software. If your own organization is savvy- you can roll your own. Technologies such as SQL Server, with web based front ends that:\n\nserve up to your customers only their requests\n\nserve up to HR only requests by category to various teams (i.e. recruitment requests to recruitment, job classification to compensation etc.)\n\nallow automatically logging of DateTime when submitted through web browser other DateTime stamps as the status changes logging who submitted the requests generally can accomplish much of both the data requirements and data content described above. The point is- it is doable and in a way that doesn’t get in the way of what we do in HR, but rather becomes the way by which we do it. As HR, we are a service, we have customers, and we are often seen as a service desk by our customers whether we see ourselves that way or not.\n\nWhat do we gain by incorporating the above as the way we do HR business?\n\nWe record in one place, most if not all, business conducted by HR.\nWe can show Executive Management in real time on an ongoing basis, the volume of work that comes into our area.\nWe can see that volume by customer ( i.e. area of the organization)- when we do -we then see who are customers really are\nWe can see the volume of services requested by area of HR\nWe get an understanding and picture of the length of time it takes to provide our services\nWe get a view of how much ‘wait’ time is introduced into our business of HR due to insufficient resources\nWe get some basic feedback about our provision of HR Services that isn’t terribly cumbersome or invasive.\n\nWithout the intention of being disrespectful, I am amazed that ,in this day and age, more HR ERP software companies don’t build this HR ‘Service Desk’ type of functionality directly into their products, and why this type of software ( whether provided by ERP vendors or not) isn’t more commonly in use by HR groups. How do we show our worth to Executive Management without showing them a picture of the demands on the HR part of the organization? How do we as HR have a full and complete picture of our HR business without a picture of or view on this? And how do we think we can improve HR operations if we are not capturing this information and thereby ‘measuring’ our current performance and customer satisfaction to see if we are improving our service as time goes by?\nThe last question is the one that I would like to expand on next.\nIf we accept the premise that the purpose of workforce/HR/People Analytics is to improve the performance of the organization through the improvement of HR practices and processes –how exactly can we do this and how do statistical packages such as R assist in this?\n\n\nQuality Improvement-SixSigma\nThe reason for highlighting above some foundational assumptions is that the relevance of quality improvement, and discussing one form of quality improvement tools such as SixSigma, might not be as well understood.\nIf you don’t see HR as:\n\nBeing a business\nHaving customers\nProviding services\nSeeing those services as ‘outputs’ of HR processes\nUnderstanding HR processes as requiring inputs\nUnderstanding that the quality of HR services/outputs are based on the quality of HR processes and HR inputs\nUnderstanding that changing the quality of outputs requires changes to the quality of inputs and processes\n\nthen this section will likely be irrelevant to you.\nQuality Improvement methodologies such as SixSigma have as their intention the improvement of product or service .This is done by applying the scientific method to process improvement. Again, I won’t get into detail on this subject in this article- because room would not allow for it. But at its heart it includes the following steps known by the acronym (DMAIC):\n\nDefine\nMeasure\nAnalyze I\nmprove\nControl\n\nA wealth of books have been written on this subject. I will mention a couple of these books below shortly.\nSince we understand from the above that the quality of the service is a function of the quality of the process and the inputs- we are trying to understand the interaction of these. One of the first questions we need to ask is ‘what is the ’measure’ of quality we use for the service being provided’. There can and should be several (the voice of the customer), but one that is often present in most cases is the time it takes to provide a service. Take too long, and customer is either disappointed or pissed off. Do it sooner than expected and the customer is either satisfied or even better-delighted. Provide for customer satisfaction at a ‘delighted’ level consistently over time is even better. Having this consistency and continuing to ‘shorten’ the time required to provide the service is even better yet. These are the purposes of the application of quality improvement methodologies to HR processes.\nTake a look at the charts below of some hypothetical service requests and the elapsed time in hours it takes to complete them (total time- from receipt to completion):\nWhat do these charts tell us?\n\n\n\n100 requests have been simulated in both the above examples\nIn any HR business process, there will be an average length of time it takes to complete a service request.\nBoth show processes that have an average time (center) of elapsed time hours to complete of 367 hours\nThere will also be standard deviations around the calculations of an average. The charts show different standard deviations (StdDev)\nQuality improvement methodologies will typically have ‘control’ limits around the ‘average’- i.e. how much variation.\nWhen the length of time it takes to complete a request is within an upper control limit (UCL) and a lower control limit (LCL), the process is said to be ‘in control’. Any request where the length of time is outside of those bounds, and the process is typically deemed to be ‘out of control’. The first diagram shows a process ‘in control’. The second shows a process ‘out of control’\nThe intent of quality improvement is to design business processes that firstly bring the process into control- so that the performance of the service is stable. Then the intent is to improve the quality of the performance by improving the process. Improving the process above would mean lowering the CL line (average) and tightening the UCL and LCL lines around the lowered CL line (reducing the variation).\nThe more the variation is reduced the higher the consistency of the provision of service ( reliability)\n\nWhen you think about it, the one type of diagram above can provide us a bucket load of useful information, and give us a direct meaningful view of our operations.\nSixSigma is one of the most well-known quality improvement methodologies out there. The methodology goes way beyond just the provision of charts like the above. It is an entire methodology around process ‘improvement’. My example above barely scratches the surface.\nIn my own ‘reading’ travels I have come across a couple of good books on SixSigma:\nThe Six Sigma Handbook\nhttp://www.amazon.ca/The-Sigma-Handbook-Fourth-Edition/dp/0071840532\nAchieving HR Excellence through Six Sigma\nhttp://www.amazon.ca/Achieving-Excellence-through-Six-Sigma/dp/146658646X/ref=sr_1_1?s=books&ie=UTF8&qid=1428509342&sr=1-1&keywords=hr+and+six+sigma\nThe point of sharing all the above- is to understand that being ‘data driven’ in HR also includes being ‘data driven’ with respect to our HR operations. Some might consider this area less interesting and less sexy than some of the typical HR analytics that might get more visibility- HR ROI, prediction of high performers that leave the organization etc. The fact is concentration on our business processes and improvement of them makes us more productive, by shortening time and eliminating poor quality. Presumably being more productive would allow for a better ROI. HR ROI calculations may indicate the organization has a problem (or not) - but in and of themselves are likely incapable of suggesting any solutions for improvement.\n\n\nHow Does the R Statistics Software Contribute To Being Data Driven in Our HR Operations?\nIf you have read any of my previous blog articles, you will know that R is one of a number of statistical packages that exist out there. Its benefit as compared to some of the alternatives is that it’s free and it has a huge scope in terms of statistical procedures that it includes. And even within that you can have several different procedures that are variations on the same thing- more than one way to do things.\nSuch is the case with quality improvement in R as well. There are a number of procedures or libraries within R that are available for quality improvement. Two of these libraries that I am familiar with are qcc and SixSigma. (Yes, R does have SixSigma library that provides certain statistical procedures that assist in the SixSigma quality improvement initiatives that an organization may choose to undertake.)\nThe previous graphic made use of the qcc library with the following R commands\nlibrary(qcc)\nqcc(“the length of time field in your data”,“xbar.one”)\n“xbar.one” is a specific type of quality control run chart.\nThe SixSigma library concentrates on statistical and graphical tools that center around the above DMAIC steps. If my memory serves me correctly, some of the things the R library helps with are process maps, control charts, and cause effect diagrams. There is actually a book that has been written by the authors of the SixSigma library in R called:\nSix Sigma With R: Statistical Engineering for Process Improvement written by Cano, Moguerza, and Redchuk.\nhttp://www.amazon.ca/Six-Sigma-Statistical-Engineering-Improvement/dp/1461436516/ref=sr_1_3?s=books&ie=UTF8&qid=1428509470&sr=1-3&keywords=six+sigma+and+R\n\n\nConclusion\n‘Data Driven’ is central to our ability as HR professionals and organizations to achieve improvement in organization performance through the capturing and use of Workforce/HR/People analytics. The scope of ‘data driven’ does include HR operations information as well. The possibility of HR and their organizations to benefit from this however is based on their ability and willingness to:\nHR seeing itself as a business Start logging all interaction with their customers Measuring that interaction both in terms of time taken to perform services and the feedback from their customers Understanding the role Quality Improvement plays in HR operations measurement and the methodologies that assist them in this ( ie SixSigma) Significantly increasing both knowledge and application of statistical analysis packages, procedures and tools to fully realize ‘data driven’ operations Often as HR professionals we are left with the impression from Executive Management and other parts of the organization that HR needs to justify its existence. On a personal level I don’t support or agree with that mindset. Organizations require HR Services regardless of HOW they are provided- whether from inside or outside the organization. HR Services DO need to exist. The real question is HOW they are to be provided and by WHOM. We are in a much better position to make the argument that it should be us, when we are measuring the performance of our HR Services in the above ways, and require any thought of outsourcing these services to provide verifiable evidence that they can do better with the same level of resources.\n\n\n\n\n\n\nAbout Lyndon Sundmark, MBA\nLyndon is a retired HR Professional with over 40 years experience of applying a ‘data-driven’, ‘evidence based’ mindset to HR practices in organizations in a variety of roles and industries."
  },
  {
    "objectID": "posts/kaggle/kaggle.html",
    "href": "posts/kaggle/kaggle.html",
    "title": "Why Kaggle Might Be Important To People Analytics",
    "section": "",
    "text": "Background\nFor any of you who have been following my blog articles on LinkedIn for the last couple of years, one of the thoughts /themes I have been sharing in probably all of them is that:\n‘People Analytics is what happens when Data Science meets Human Resources.’\nThe intersection of these two fields is People Analytics. When we:\n\nAsk critical HR business questions\nSearch /review what information sources and data we have that may be relevant to answer those questions\nPay attention to the quality of that data\nConduct statistical analyses that are appropriate to the questions asked\nInterpret and document our findings\nMake recommendations based on our findings for either actions or decisions themselves that need to be made- because of a better grasp /understanding of the issues, or because the statistical analyses itself was predictive in its purpose\n\nWe are engaging in Data Science with the context / domain of human resources management and decision making. The recognition of the ‘data science’ part of ‘that’ is critical to understanding the potential contribution Kaggle can make to People Analytics.\n\n\nWhat Is Kaggle?\nSo, what is Kaggle and why is that important to People Analytics?\nOne good description comes from Wikipedia:\nhttps://en.wikipedia.org/wiki/Kaggle\nThis sums it up as follows:\n‘In 2010, Kaggle was founded as a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models.’\nAnd\n‘In April 2015, Kaggle released the first version of their Scripts product onto their platform. Scripts allows users to write, run, and publicly share their code on Kaggle.’\nIn effect Kaggle is a site on the internet at kaggle.com where users can share datasets of information. Once posted, the contributing user of the dataset or anyone else who uses Kaggle can post their code with respect to how they approached the analytics problem they were try to solve, and show their results. The datasets are uploaded as csv formatted files to Kaggle. The code uploaded and used can be code in:\n\nR\nPython\nJulia\nSQLite\n\nR and Python are often the most used languages for Data Science.\nThe benefits and strengths of this functionality on Kaggle should be obvious:\n\nYou have a platform where data can be shared reasonably easily\nYou have a platform where many more eyes- beyond your own or your organization, and the expertise there - can be brought to bear on your data\n\nThe previous Wikipedia description indicates that Kaggle was founded as a platform for predictive modelling and analytics competitions. Indeed, the vast majority of datasets, code, and activity on Kaggle seems to revolve around those ‘founding’ roots. Many of these have requirements for being formally part of the analytics ‘competition’ and some have paid prizes.\nHowever, in the last several months, Kaggle introduced the concept of ‘organization(s)’ in their platform. With this functionality, you don’t necessarily have to submit data as part of an organized competition, for money, or with many restrictions- you can submit datasets for which your intent is to have others, informally and voluntarily, share their ideas of analysis freely through code examples of their own. And with this functionality you can group datasets you have submitted to be visible in one place. You can also add ‘people’ to your ‘defined’ organization who have the rights to submit other datasets to be grouped under that ‘organization’. In effect, at its simplest, an organization is just a name you create to group ‘your’ datasets -so that they are all in one place.\nWhile you use this functionality to ‘group’ them, the datasets themselves are public to all Kaggle users for download, viewing, and usage. When statisticians, data miners, data scientists look at your data, and look at the problem you are trying to solve, they can submit a ‘kernel’ tied to your dataset which is R code or Python code that attempts to solve the problem or answer the question. That ‘kernel’ too is publicly visible.\n\n\nWhy is this potentially important to People Analytics?\nBecause People Analytics is at the intersection of Data Science and HR, it requires knowledge of both domains. HR people know the context of HR and in depth, but often know very little about data science. Data Scientists often know data science and statistics in depth, but know very little, if anything, about the context of HR and HR information. You are ‘fortunate’ if you have both of those skills in your organization, and more fortunate still if they both exist in the same individual(s)\nThat really is a ‘Catch 22’ for People Analytics to take root and gain a proper legitimate foothold in HR in organizations. You may have lots of HR professionals who see the potential of People Analytics- but don’t know how to approach their data analytically, and need the help of data science professionals. And you may have a lot of data science professionals who would like to help apply their skills to HR data to help answer / address HR questions /issues -but don’t have access to HR data and to the minds of HR professionals and their knowledge of the HR context.\nAdditionally, another big ‘Catch 22’ is that HR information, like many other types of information, is subject to confidentiality and privacy laws that are ‘country specific’. The ‘emerging’ question is how can data be ‘shared’ while maintaining the appropriate confidentiality and compliance with privacy laws?\nI think it’s important that these critical questions be answered correctly. How quickly, successfully, and effectively People Analytics emerges as a discipline within HR and organizations, I think, is dependent on this. If they can be answered successfully and in compliance with privacy and confidentiality laws- Kaggle, especially with its ‘organization(s)’ feature can be an excellent platform to have additional analytics skills be brought to bear on your data.\n\n\nA Personal Example\nI had become aware of Kaggle about a year ago, and wanted to experience it firsthand. But I didn’t want to hold any formal competition per se. I just wanted to try it out as a platform. Within the last year, the ‘organization’ feature was announced and I inquired with Kaggle about it. And they seem to indicate that ‘yes’- creating my own organization would allow me to submit my own datasets without there needing to be an ‘intent’ of any formal competition.\nAs I mentioned at the beginning of this blog article, if you have been following my blog articles over the last couple of years, you will have come across several People Analytics in R examples. These examples included both links to datasets on my Microsoft Onedrive space as well as showing R code and output.\nI thought the best way to try out and experience Kaggle would be to try to upload those datasets and include my code as Kaggle kernels. To do that I needed to create an organization first. The name of my organization on Kaggle is\nPeople HR Analytics Repository found at\nhttps://www.kaggle.com/HRAnalyticRepository\n\nYou will notice that I have 4 datasets included there. One of the things that Kaggle does is keep you apprised of activity with respect to your datasets- how many times it’s been downloaded etc. When you click on the dataset listed and scroll down the page presented you can see a sample of the data:\n\nAt the top of the page you can see among other things ‘kernels’ for that dataset listed and you can click on those lists to see the kernel and its output:\n\nWhile my initial intention was to just experience Kaggle with datasets that I was familiar with, and try running my R code as kernels- It wasn’t long before I started receiving the above-mentioned notifications that others were interested and downloading my datasets to try out their own hand at analysis of my data.\nEventually – on one of them- another Kaggle user used a different data science algorithm on my data and came with higher accuracy of predicted results than I had. The 5 algorithms I had used had varying levels of accuracy of prediction, but his beat all of them.\nAnd the bonus was that I hadn’t explicitly asked anyone to take a look at my data to see if they could ‘better’ my prediction. They did this voluntarily for the ‘challenge’ on their part. And I learned a new algorithm which could be brought to bear on my data. Bonus!\nSuch is the spirit and intent of Kaggle as was described in the opening Wikipedia description.\n\n\nCaveat\nOne thing that can’t be stressed enough is that in submitting any dataset to Kaggle, that you have\n· the authority to do so\n· that you are complying with all privacy and confidentiality laws in your jurisdiction.\nOne thing that I asked Kaggle is – ‘is it possible to delete a dataset once you have submitted it?’. The answer a few months ago, was ‘No’. At present, you can’t delete it once uploaded. You can update it however. So, know the answers to the above bullet points, before you upload anything.\nIn my case, I didn’t have to worry about the above bullet points because all my datasets are fictitious, and I created them.\n\n\nClosing Comments\nWhat my own personal experience with Kaggle illustrated for me was the power of having ‘additional minds’ and ‘eyes’ on my data. While I know/knew several the appropriate algorithms to use on my data to answer the questions I had, you can never know ‘everything’. The extra eyes on this was rewarding.\nTo the extent that many organizations don’t necessarily have both the data science and HR skills in their organization, Kaggle as a platform can be very useful to People Analytics depending on your situation, and providing that you can be fully compliant with the privacy /confidentiality laws in your jurisdiction."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "",
    "text": "From\nhttp://www.merriam-webster.com/dictionary/reinvent\nReinvent:\nTo make major changes or improvements to something To present something in a different or new way To make as if for the first time something already invented Recently, I wrote a blog article on whether HR / Workforce / People analytics was a wake-up call for HR.\nhttps://www.linkedin.com/pulse/workforcehrpeople-analytics-hr-lyndon?trk=prof-post\nIn that article, I referred to another article from the internet that was profiling Googles ‘data driven’ HR practices, and the degree to which that was allowing Google to ‘reinvent’ HR practices for itself.\nhttp://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\nThat article got me quite excited (in a good way) because it started to give me a peek into the future, of what the future for HR could look like and become IF it chooses to become data driven. As important as that future state could be for HR, I have my doubts that HR in most organizations will see that future anytime soon. And it will be because HR will find it very difficult to reinvent itself.\nIn the rest of this blog article, I would like to explain why I think this will be the case."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#introduction",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#introduction",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "",
    "text": "From\nhttp://www.merriam-webster.com/dictionary/reinvent\nReinvent:\nTo make major changes or improvements to something To present something in a different or new way To make as if for the first time something already invented Recently, I wrote a blog article on whether HR / Workforce / People analytics was a wake-up call for HR.\nhttps://www.linkedin.com/pulse/workforcehrpeople-analytics-hr-lyndon?trk=prof-post\nIn that article, I referred to another article from the internet that was profiling Googles ‘data driven’ HR practices, and the degree to which that was allowing Google to ‘reinvent’ HR practices for itself.\nhttp://www.tlnt.com/2013/02/26/how-google-is-using-people-analytics-to-completely-reinvent-hr/\nThat article got me quite excited (in a good way) because it started to give me a peek into the future, of what the future for HR could look like and become IF it chooses to become data driven. As important as that future state could be for HR, I have my doubts that HR in most organizations will see that future anytime soon. And it will be because HR will find it very difficult to reinvent itself.\nIn the rest of this blog article, I would like to explain why I think this will be the case."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#the-argument-in-summary",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#the-argument-in-summary",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "The Argument In Summary",
    "text": "The Argument In Summary\n\nReinvention (by the above definition) requires major changes or improvements to something.\nReinvention (what I infer from the Google People Analytics article above) requires an organization to be data driven in its HR practices\nBeing data driven in HR practices is not something that most organizations are characterized by at the present time. (The idea to be ‘data driven’ in HR seems relatively recent in HR literature- at least to the point of being able to operationalize it into organizational reality)\nHR practices themselves do not exist in a vacuum. What HR practices that are in evidence in an organization, are usually related to, based on, and consistent with the wider model of management and management practices within the organization itself. (In other words I accept the premise that current HR practices are not here by accident)\nThe choice of management model or management practices within an organization limit the choices of HR practices, which in turn may limit designing HR practices which are ‘data driven’ capable or friendly, which then may affect the ability of the organization to make major changes or improvements in its HR practices. This then limits the ability of the organization to reinvent itself.\n\nTo help understand the logic behind this argument, I would like to start with the bottom bullet point first and work my way up"
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#the-choice-of-management-model-and-practices--what-is-the-dna-model-or-paradigm-of-our-hr-practices",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#the-choice-of-management-model-and-practices--what-is-the-dna-model-or-paradigm-of-our-hr-practices",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "The Choice of Management Model And Practices -What is the DNA, Model or Paradigm of our HR Practices?",
    "text": "The Choice of Management Model And Practices -What is the DNA, Model or Paradigm of our HR Practices?\nGenerally, I think there are typically two choices that exist for HR practices in most organizations:\n\nA top down orientation of HR, divided into typical recognizable HR functions, traditional HR methodologies and practices. Often communication is one way. Directives are driven ’ top down, through the hierarchy and structure of the HR functions, while concurrently customers of HR are clamoring for service to be provided by these HR functions. (This is the choice, most organizations make - that I have seen over the last 35 years)\nA process orientation to HR, where each traditionally recognizable function with HR is instead seen as a process or set of processes for which the outputs are the services provided to the customers. In this choice the traditional HR functions are not top down silos but are horizontal processes. And these horizontal processes are not silos because of the interdependency on other processes in meeting customer requirements of the service they are providing. (This is seen much less often in organizations)\n\nGiven these two choices, one might ask the question ‘why do these choices exist’? Why the preponderance of one over the other? What is the DNA, model or paradigm behind these choices for HR?\nI have always believed that HR practices do not exist in a vacuum. Rather I believe that HR practices in turn reflect the wider organizational management philosophy, culture, and practices that they are within. We tend to make choices in our HR practices which reflect, and are consistent with, how we manage our organizations.\nWhat are these wider choices of how we manage our organizations? Generally in management literature, it is the choice between MBO/MBR (Management by Objectives/Management by Results) and Quality Improvement.\nThe form and choice of our HR practices will generally line up with which model of management the organization has chosen to operate under. The awareness of an organization that they have made a choice or not may or may not be there. The above management model examples are used because they tend to reflect the choices organizations make.\nIn any case, these management choices not only can affect the HR practices choices we make, but also the degree to which ‘reinvention’ may end up being possible. To understand this, I would like to first identify some of the differences between the MBO/MBR and QI models of management. Then I would like to share how that impacts what our HR practices look like. Finally, I would like to show how that might impact the ability to reinvent ourselves as HR."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#what-is-at-the-heart-of-mbrmbo",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#what-is-at-the-heart-of-mbrmbo",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "What is at the heart of MBR/MBO?",
    "text": "What is at the heart of MBR/MBO?"
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#section",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#section",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "",
    "text": "Wikipedia defines Management By Results/Objectives as:\nThe process of defining objectives within an organization so that management and employees agree to the objectives and understand what they need to do in the organization in order to achieve them.\nhttp://en.wikipedia.org/wiki/Management_by_objectives\nIt consists of 5 steps:\n\nReview organizational objectives\nSet worker objectives\nMonitor progress\nEvaluate performance\nGive reward\n\nHow is this management model operationalized in most organizations?\n\nIt generally is a top down approach. The executive often sets the overall organizational objectives, and these overall objectives are to be cascaded down through each area and of the organization where it is expected cascaded goals and objectives are in some way tied to the level above. (In reality, often this doesn’t occur. Various levels and areas may generate their own goals and objectives, especially if it is hard to tie what they do to the overall organizational objectives)\nIt is based on goals and objectives and plans to carry them out\nWhen progress is monitored, the monitoring really consists of how far along are we on the achievement of those objectives. And it’s usually tied to time intervals – i.e. quarterly reviews. Are we 25% underway after the first quarter? Etc.\nWhen performance is evaluated, it’s often at the level of ’were all the goals and objectives fully and successfully completed?’ This type of measurement implies little tolerance for ‘variation’. You are expected to meet your goals.\nIt is measured at an individual level. Every employee has goals and objectives set for them and is evaluated against the achievement of their goals and objectives.\nIt is dependent on organizational structure- the organizational hierarchy, the organization chart. The person above you that you report to is the person that evaluates your performance.\nRewards of handed out often at the end of the year after the organization knows how it performed overall."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#what-is-at-the-heart-of-quality-improvement-management-approaches",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#what-is-at-the-heart-of-quality-improvement-management-approaches",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "What is at the heart of Quality Improvement Management Approaches?",
    "text": "What is at the heart of Quality Improvement Management Approaches?\nOk – how does a Quality Improvement Management Approaches (Six Sigma and others as examples) compare? Actually it stands in stark contrast to MBO/MBR.\nConsider the following about Quality Improvement:\n\nProcess Model - Suppliers-Inputs-Process-Outputs-Customers. The business in not understood in terms of goals and objectives in the abstract or the organizational hierarchy or structure per se. The organization is defined and understood by who its customers are, what the products are services are, what processes exist to provide those products or services to its customers, what inputs are required for those processes and who are providing those inputs.\nDefinition of the business-The business is seen as the entirety of all of its business processes that are necessary to produce the products or services that meet the customer’s requirements and the ‘interdependency’ of those processes.\nGoals and Objectives are specifically tied to products, services and customers – Goals and objectives reflect decisions of getting into or out of products or services, or are related to improvement in the quality of the products or services that will continue to be provided.\nQuality improvement management approaches understand that the quality of the end product or service is very much related to the quality of the processes themselves and the quality of the inputs. Poor quality in inputs or processes will results in poor quality of output. It’s not just how much services or product did we produce (i.e. meeting a quota) or did we complete the product or service, but how good these are. And also understand why they are at the level of quality they are.\nThe only way to know the level of quality of inputs, processes and product and service outputs is by measurement of all of these things on a regular basis. The only way to understand the interrelationships between inputs process and product or service output quality is through ongoing analysis of these measurements. No measurement- no ability to monitor quality.\nQuality improvement approaches to management recognize that variation will be inherent in these measurement outputs- as a natural phenomenon. In fact it’s often variation in the measures that helps us understand the interrelationships. Having said that, it is understood that minimization of variability in our products and services provided, the processes and inputs generally leads to higher quality."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#how-does-our-choice-of-management-model-and-practices-affect-our-hr-practices-choices",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#how-does-our-choice-of-management-model-and-practices-affect-our-hr-practices-choices",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "How Does our Choice of Management Model ANd Practices Affect our HR Practices choices?",
    "text": "How Does our Choice of Management Model ANd Practices Affect our HR Practices choices?\nMBO/MBR\nWhen the above are the overarching characteristics of how we manage organizations, their outcomes, and performance, it has an impact on the choices we make in our HR practices (the list is intended to be illustrative not exhaustive):\n\nPerformance appraisals take the form of traditional performance appraisals. These include the setting of the individual objectives, monitoring and overall evaluation by the person the employee reports to. These are annual for sure, and sometimes interim review of achievement of goals and objectives will be encouraged and occur as well\nOrganization Charts become the de facto way of understanding and describing the organization to others. Who is at the top? Who are the executive? How are the business functions (LOB and staff) designed within the organization to achieve the overall organization objectives? How many levels top to bottom? And here often one of the most important characteristics and questions- to what degree is the organization decentralized or centralized? Decentralization often more prevalent when organization assumes that will keep business closer to customers. Centralization often being more in evidence when there is an assumption that will control costs.\nJob Descriptions- take the form of traditional job descriptions. Someone from HR goes out to a supervisor of a position or job, and asks them what the overall purpose of the job in question is, what its duties and responsibilities are and what the proportion of time is spent on each. It will also typically capture what skills, knowledge, and years of experience are required to be successful in positions. Each job or position description is its own entity and is often written up from scratch independent of how it compares to other positions- because its purpose is to accurately describe ‘that’\nCareer Planning and Succession Planning will often be completely dependent on and consistent with the job descriptions and organization charts\nTraining will often be focused around the job itself- either for good performance in the current position or in preparation for being able to respond to the demands of future positions- career or succession related training\nRecruitment too will often center on the job description as well. Job specs and job advertisements will generally describe what the job requires and candidates.\nCompensation will typically reflect both the external market (based on your occupation) AND where a person is in the organizational hierarchy. The higher you are, the more your pay will be as compared to those below you in the same line of work. Organization hierarchy will have a big impact to on what is perceived as equitable compensation in the organization as compared to others\nPerformance Rewards will typically be based on numerical rating scale, and often a bell curve will attempt to be applied. Additionally there will often be an overall cap will be applied to total salary increase funds or bonuses to be applied\n\nAs you can see from the above examples, it’s not difficult to see that these types of traditional HR practices are consistent with an MBO/MBR management philosophy and culture within the organization. Dependency on traditional organization charts and goals and objectives are an example here That ‘consistency’ with the basis of management is what allows the traditional HR practices to ‘live’ comfortably within the organization. It will likely be rare to find HR approaches inconsistent with MBO/MBR lasting very long in an organization based on a management approach of MBO/MBR.\nIf we accept the idea that generally HR practices being ‘practiced’ cannot be inconsistent with the overall management philosophy or model that exists for an organization, it can be reasonable to assume that if the management model is different it’s likely that our HR practices will reflect that as well, and they might be different under a different management model. Let’s contrast the above management model and HR practices with what a Quality Improvement approach and resultant HR practices might look like.\nQuality Improvement\nWhat impact is there on HR practices, when the model of management is based on quality improvement?\n\nPerformance appraisals – The terminology here almost becomes an inconsistency itself with the quality improvement management model. Quality improvement models of management pay predominant attention to ‘process’ performance. The prevailing philosophy here is that it isn’t individual performance aggregated across the organization and its improvement at the individual level that results in better overall organizational performance. Rather it is improvement in processes which people work within that results in better overall organizational performance. In general, only about 15% of organizational performance can be tied to an individual. The remaining 85% is attributed to the design of systems and processes that people work within. In quality improvement management models, attempting to have a performance evaluation/measurement system based on the individual in the form of traditional performance appraisals (100% individually based) simply is not consistent with the above awareness that only 15% of organizational performance can be attributed to people themselves. Improve the processes people are working within, and organization performance improves.\nOrganization charts – go from being top down to left to right in their orientation. In top down, employees pay attention to the people above them first because that is where they get their rewards from. To attempt to please the customer can sometimes be at odds with pleasing ones boss. In a quality improvement model of management, the organization chart is more accurately seen as where are my inputs coming from and where my outputs are going to. To the extreme right (which would in top down would be the bottom of the organization chart) you have the employees that are the direct front line serving the customers. Their outputs are what keep your organization in business. To the extreme left (which would be top in a top down model) is the individual who is ultimately responsible for the provision of resources and inputs, and the processes that result in the ability of the front line to provide products and services to their customers. In between the extremes, are all those employees who have the responsibility of distributing those resources and inputs through processes highly designed to provide good quality products and services delivered through the front line. With respect to what drives organizational design, the degree of centralization and decentralization is based not on the latest person brought in (sometimes with a big ego) to solve a problem- but by what design has the appropriate effect on the best performance of processes and satisfaction of the customer on the product or services provided.\nJob Descriptions- In a quality improvement, process oriented model of management, the focus doesn’t start and end individual by individual for job descriptions. It starts with the descriptions of inputs processes and outputs. Individuals work within these processes using inputs and providing outputs. So a job description for an individual becomes ‘what part of these processes’ does the individual perform. What inputs do they use? What outputs do they produce? Instead of being written one by one independent of any other job description, they are documented based on what portion of the overall process and products or services they are providing. (Think the elephant and blind men example. The elephant is the overall process being described. The blind men describing what they experience is the equivalent of what part of the elephant they are dealing with). What is the significance of this approach to job description creation? You can actually compare job to job directly based on what they have and don’t have of the larger picture. This also means you can’t document job descriptions without documenting the processes first.\nCareer Planning and Succession Planning – what changes here is that there is less focus on the organizational ladder and more focus on process knowledge and expertise. If your organization chart is left to right in orientation what exactly are you climbing? Since traditional HR practices would typically look at differences in positions or jobs and their content for career planning and succession planning purposes, the equivalent in quality improvement model of management HR practices is gaining more understanding and knowledge of the processes, inputs and outputs themselves. Since other job descriptions within an area will often be other parts of the same process (picture), understanding the differences between positions or jobs is simply really gaining a bigger understanding of the business processes you are in. What you are ‘climbing’ is process knowledge, quality knowledge, knowledge of product/service, knowledge of customer, better organizational performance. If you are still determined to climb structure, you are moving left in a ‘left to right’ organization chart. The further left you go, the more you must be intimately familiar with the above- to distribute resources and inputs effectively. Promotion more likely to be based on what you know than who you know. The idea of ‘high fliers’ in the ‘abstract’ (outside of the context of ‘high flier for what’) – doesn’t fit. If you want to be increasingly recognized in the organization, ability to improve processes and process knowledge becomes the appropriate currency.\nTraining in a quality improvement management model goes from being a ‘soft’, ‘best guess’, ‘nice to have’, ‘hope the time spent achieves some useful organizational outcome’, ‘first to go when times get tough’ to being a fundamental part of the success of the organization. Quality improvement models of management understand that improvement is never ending. Learning about the business we are in, our customers, our processes, their quality is also never ending. So training is never ending.\nRecruitment- while the focus may still be on job requirements based on job descriptions, since the job descriptions themselves are really subset descriptions of wider business processes, the focus of selection is likely to be directed towards the wider processes which the job descriptions reflect than just the individual job descriptions. The impact may also be on the personality types that are hired. Quality improvement models of management have greater dependency on processes than individuals, and have the left to right organization chart mind set. Personality types that seek personal recognition as a result of process improvement leading to better organizational performance, generally would be more conducive to this style of management and HR practices\nCompensation – will still be ultimately based on the level of authority you have on resources, inputs, outputs, design, supervision etc.\nRewards- in a quality improvement model of management will be based far more on quality improvement of the products and services and market share, often with the entire team receiving whatever the rewards are because ( as mentioned earlier) 85% of the performance of any system is based on the design of the system and process itself. In the quality improvement world, there is no room for bell curves, subjective ratings, forced ranking"
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#how-do-our-hr-practice-choices-impact-our-ability-to-be-data-driven-and-to-reinvent-ourselves",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#how-do-our-hr-practice-choices-impact-our-ability-to-be-data-driven-and-to-reinvent-ourselves",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "How Do Our HR Practice Choices Impact Our Ability to Be Data Driven and To Reinvent Ourselves?",
    "text": "How Do Our HR Practice Choices Impact Our Ability to Be Data Driven and To Reinvent Ourselves?\nI think the answer to this question is in part based on what it means to be ‘data driven’. Organizations have a propensity to generate mountains of data. But are they driven by that data as the basis upon which they make decisions? In many areas of an organization –yes. The Financial and Accounting area of organizations most quickly comes to mind. HR – not so much.\nWhen you consider many of the ERP systems in use out there these days by organizations, their design reflects intention for transaction reporting and recording. There is very little in the design of these systems that encourages analysis or decision making. It’s like we make our decisions outside of the system and then record them in the system.\nThese systems reflect traditional HR practices, which in turn reflect ‘traditional’ MBO/MBR management practices. In many of these, beyond the transactions being recorded, their design has done little more than automate what used to be a paper process. Same old processes- but speeded up to use computers instead of paper. Job descriptions rarely are ‘databasized’. Rather they are often in the form of Word documents on a network drive. Performance Appraisals, where automated, capture text and ratings and store them in a database. But this still results in little more than what their former paper processes were based on. (Little other than the scores can be summarized or analyzed.) Succession Planning/Career Planning, where automated, does little more than document names of replacements and text- nothing in the form that can be used for human resource forecasting. As mentioned before they are ‘transactional’ – capturing data after events have occurred and decisions made. . Do we see this as ‘robust’ data driven automation that fundamentally reinvents HR? Traditional HR practices being automated often does very little to ‘measure’ anything and therefore limits the ability to be data driven ( data collected that actually assists in decisions being made on the basis of the data collected). Goals and objectives completion and rating tell you what happened but not why it happened or necessarily how or how well it happened.\nCurrent HR systems do not lend themselves well to allowing an organization to be data driven, because they are based on traditional HR practices. And that is because traditional HR practices continue to be so pervasive- as pervasive as the management practices they, in turn, are based on.\nQuality improvement models of management and the resultant HR practices lend themselves much more to being data driven. These require much more data and measures to be gathered on an ongoing basis. When we measure -we can see whether change is occurring and whether in the right direction. Continuous data gathering and analysis are fundamental to how a quality improvement approach to management and HR practices work.\nOne other major missing piece in many current HR ERP systems is automation of HR service requests themselves. When you think about it, almost everything we do in HR is a ‘request’ from someone and what we do in HR is provide HR Services. We are a service business. How well we provide those services determines how satisfied our customers are and how much we cost the organization. Few HR ERP systems have this capability. Nor do they then link these requests to the actual transactions that are recorded in the HR systems. And yet, these types of features would be totally consistent a quality improvement management model impact on HR practices and with being data driven.\nI think it’s also important to re-remind ourselves of what reinvention means and how it is defined. Per the definition provided at the beginning of this article, reinvention means to make major changes or improvements to something. In this definition. Improvements usually mean the attention is ‘constant’ to the entities we are paying attention to. In a quality improvement management model and related HR practices, this occurs because by design the entities we are paying attention to are processes, inputs, outputs, customer satisfaction and the improvement of these. Reinvention means that we are making changes or improvements in these. MBO/MBR has no guarantee of this ‘constancy of attention’ to our object of attention. In MBO/MBR the attention is on, for the most part, the goals and objectives and achievement of them. These goals and objectives and what they are paying attention to often change each year. These goals and objectives are not required to reflect existing customers, products and services. They might- but there is nothing in the MBO/MBR design itself that forces or guarantees this. From my own experience I can share with you that over many years in many organizations typical MBO/MBR goals and objectives are agnostic in awareness of the existing customers, products and services we are already in (especially if you are a staff area) So in effect, you not only have to provide all the existing services you currently are, but new projects goals and objectives on top of that. Often this is without any additional resources being provided to complete these, or an awareness of what resources are required to provide for the existing products and services. The mindset is ‘we don’t care how you get it done, just get it done’. How do you know you are ‘improving’ anything if what you focus your attention on isn’t constant? And if you are not changing for the purpose of improving how are you reinventing anything?\nThere is one final thought we might want to consider with respect to being data driven. Does our management model and the subsequent HR practices in our organizations cause the issue of incongruity or lack of linkage between our HR activities/metrics and organizational outcomes in the first place? IF MBO/MBR is predominantly dependent on the setting of goals and objectives and the achieving of them by a deadline, there is little requirement for measurement of HR activity as it is occurring. Quality improvement approaches, which fundamentally assume the quality of the end service to the customer is tied to the quality of the processes and inputs and measurement of all these, depend heavily on measurement. One approach tries to understand linkages between HR activity and organization outcomes after the fact. The other pays attention to these in real time as the basis upon which management practices and HR practices actually occur.\nOur HR practices do impact our HR systems and vice versa and our ability to be data driven."
  },
  {
    "objectID": "posts/WhyHRMightNotReinvent/NotReinvent.html#conclusion",
    "href": "posts/WhyHRMightNotReinvent/NotReinvent.html#conclusion",
    "title": "WHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF",
    "section": "Conclusion",
    "text": "Conclusion\nHR Reinvention – Is it possible? Absolutely. Any time soon for many organizations? Less likely.\nThis is because I think there are still many obstacles that organizations are facing in an attempt to become data driven:\n\nData driven means that data for making decisions being collected and analyzed is a fundamental part on ongoing organizational activity and HR practices. If being data driven means that HR, Workforce, or People analytics are in evidence- these are the ‘new kids’ on the block in the HR field in terms of them being recognized. Traditional data collection in HR has predominantly been for transactional reporting purposes not reinvention.\nReinvention requires improvement. Improvement requires change. Noticing change requires measurement and making changes in the right direction. Measurement requires that our attention on what we pay attention to remains relatively constant.\nHR ERP systems will need to change to enhance the ability of an organization to use its data to make data driven decisions.\nHR practices themselves will need to change to become more data driven in decision making\nManagement practices will need to change to support the needed changes in HR practices. Can organizations change their management model?\n\n\nCan they give their HR group the permission to reinvent itself? Is HR willing to reinvent itself? IS HR willing to become far more analytic and data driven? The challenges here are significant for HR, but necessary. What do you think?"
  },
  {
    "objectID": "posts/CalculatingEmployeeTurnover/CalculatingEmployeeTurnover.html",
    "href": "posts/CalculatingEmployeeTurnover/CalculatingEmployeeTurnover.html",
    "title": "Is it Time To Take Another Look at How to Calculate Employee Turnover?",
    "section": "",
    "text": "TL;DR; In a word- YES. Current formulas for calculating turnover often use denominators which are an average. These averages really are attempting to indirectly combine inflows with the outflows. In my opinion inflow rates and outflow rates are separate entities\n\n\n\n\nIntroduction\nEmployee turnover is a very common HR metric of interest to many organizations.\nOne of the things I have found, over the years, in HR metrics related projects - is that clarity, definitiveness, and consistency of definitions are all strong ingredients to usable HR metrics. HR ‘metrics’ are of course measures. And for measures to be useful they must be ‘valid’ and ‘reliable’. ‘Valid’ means that they measure what they are supposed to measure. ‘Reliable’ means that it does it consistently. Take away clarity, consistency etc. and metrics lose their utility.\nRecently I decided to do a quick Google search on how employee turnover was defined and how to calculate it. Admittedly this isn’t overly scientific or deep. And it wasn’t intended to be. But I was interested in seeing how consistent the recommended definitions and suggested formulas for calculating turnover were.\nThe good news was that there seemed to be relatively common definitions for turnover and for formulas as to how to calculate it.\nThe bad news was that even though the formula itself was relatively common- how to calculate portions of that formula were not.\nWhat exactly did I find? Consider the following definition and formula:\nDefinition\nGenerally the definitions from the traditional typical dictionaries (Merriam Webster, Cambridge, and Oxford) centered on something like the following:\nTurnover is the rate at which people leave a company.\nA rate, in turn, is generally concerned with the frequency or speed of something. Often it is expressed over the measurement of time. We are not only concerned with how many people leave an organization, but over what period of time.\nAnd for context it is often of less concern what the absolute number of people were that left but really what proportion of people left the organization out of the total number of people in the organization. So really what we are concerned with is the proportion in any given time period.\nFormula\nGiven the above definition, it is not surprising that a common formula for turnover was:\nEmployee Turnover = the number of people who left the organization / the number of employees\nWhat’s implied in this formula is that it still must be within the context of ‘time’ to have any meaning. It’s the number of employees who left over a period of time. So the formula is relatively consistent with the definition.\nWhat was not common in the Google search was how to calculate the number of employees.\nI came across at least 3 unique (non-repeating) suggestions for the denominator in my Google search:\nThe ‘average’ number of employees for the time period The number of employees at the end of the time period The number of employees at the beginning of the time period The ‘average’ in first suggestion above was calculated by taking the opening and closing balance for the time period in question (often a month), and dividing by 2.\nAs mentioned previously, for any of you who have been charged with responsibility of providing HR metrics in your organization - having absolute clarity and consistency in your definitions (turnover or any HR metric) is essential if you are going to provide meaningful HR metrics information. In fact when getting into an organization project of creating HR metrics, often the time take to define them and determining formulas for them and getting agreement on these can take as long as the technical development of these in a data warehouse itself.\nIf a basic Google search indicates that there are at least 3 possible ways of calculating the denominator, that doesn’t exactly provide ‘desired’ consistency. And if different organizations make different choices as to how to calculate number of employees, you can probably appreciate the danger of asking another organization the question of ‘what is their employee turnover rate?’ without also asking them ‘how exactly do you calculate that?’ As importantly, for our own metrics and calculations- we have to also ask –‘how exactly do we calculate that?’ Given the options, how do we choose?\nAs I try to answer that question, I have no intent or desire of ‘absolving’ you from the responsibility of making your own choice. How you calculate it is based on your needs in your organization. Sometimes your choice will be based on regulatory requirements. Sometimes the choice is open for you. Sometimes it’s both- if you consider having more than one metric for turnover. What I do desire to do is to assist you in understanding further what you are really measuring, given these 3 choices, and perhaps suggest some criteria for your choice, when the choice is not regulatory. Let’s look at each of the calculation options further:\n\n\nTurnover Calculations Based On an Average for the Denominator\nEmployee Turnover = the number of people who left the organization /Number of Employees (Average)\nThe idea behind taking an ‘average’ is that for the time period of concern –the denominator may change. Even in an often typical smallest time period of interest for HR metrics- a ‘month’- if people leave and come into the organization, ‘number of people’ is not constant for the whole month. Indeed, typically the following is the case for any given time period:\nAverage Employee Count= (Opening Count of Employees for the Period + Ending Count of Employees for the Period) /2\nThe argument here is that since the number of employees is not constant during the month- we can or should ‘negate’ the variation by taking an average employee count. The same principle would apply whether we are dealing with month, quarter, or years as the time period under consideration. On the surface this has an appealing logic. Since the count of employees will change during the period of time in question- we feel this would be a more ‘representative’ figure for the time period in question.\nBut let’s dig a little deeper. ‘What exactly are we measuring and what do we really have when we calculate an average for the denominator?’\nWe definitely have an ‘arithmetic’ construct- the average. But for this metric we also have the inclusion of hiring information and termination information in the denominator. Why? Because to get the ending balance for a time period (in the formula above), we must base it on the opening balance and then the changes during the time period. The changes to an employee population in any time period are reflected in the hires AND terminates within that same time period.\nEnding Balance= Opening Balance – Terminates + Hires\n205= 200-25+30\nSince the ending balance for that time period is part of the denominator, it means we have hires count and terminates count in the denominator for turnover. ( Per the above formula for average employee count)\nAverage Employee Count= (opening count + (opening count – terminates + hires))/2\nAnd it means that the turnover calculation using an average of opening and closing balances is really comparing the count of terminates for the period over both the original balance and the changes (hires and terminates) for that time period. Aside from hires now being introduced into the denominator, we are actually also negating terminates from the opening balance in the denominator.\nSo if it weren’t for the ‘hires’ being included, terminates are not based on just the opening balance but rather the opening balance less the terminates. And then this averaged with the opening balance of the denominator too. This hardly makes sense to me personally. Especially if we consider a scenario where there were no hires in the time period. Your equation for the denominator then becomes:\n(Opening balance + (opening balance- terminates+hires))/2\nExample denominator: (200 + (200-25+0))/2\nYou are taking the opening balance of 200 above and adding that number again and subtracting the terminates of 25). Then you are dividing that by 2 to get an average. And THEN you put the numerator of the number of terminates over that. Let’s see- turnover in this calculation means that we are dividing the number of those who terminated by an average of something that includes the hires and also negates the terminates in the denominator as well. Confused? I AM.\nArithmetic construct? Yes. Justifiable logic as to meaningfulness of the denominator? What do you think?\n\n\nTurnover Calculations Based on the End of the Period Count of Employees for Denominator\nThis approach for calculating the contents of the denominator suffers from many of the same issues as the previous suggestion. The only thing that is better here is that at least it’s not an average for the sake of having an average.\nEnding balance always is the opening balance minus the terminates plus the hires for a time period.\nLet’s look again what happens if there were no hires in a given time period for a closing balance.\n(Opening balance – terminates + hires)\nExample denominator: (200-25+0)\nSo once again you have hires in a turnover (terminates) calculation. And if there were no hires in that time period your turnover formula is really doing a calculation of terminates over the opening balance minus those terminates. You are actually basing your terminates over a denominator that has already been reduced by the terminates.\nExample turnover: 25/ (200-25+0)\nDoes this make sense?\n\n\nTurnover Calculations Based on the Beginning of the Period Count of Employees\nThe previous two options for denominator introduce the ‘terminates’ and the ‘hires’ in the denominator. It’s like double counting the impact of the ‘terminates’ and adding ‘hires’ into a calculation that should be about ‘terminates’. Can this be avoided? Yes – if we agree that hires have nothing to do with turnover and that terminates have no place in the denominator either- because it is double counting. How? By having the count of employees based the opening or beginning balance of the employee count for the time period.\nWhy is this the case?\nFirst of all let’s look more closely at what an opening count is. An opening count in this case means the count of something before anything else can occur that would change it. What could change it? Hires and terminates for the current period. For time purposes, a new time period is often demarcated by midnight. Any transactions recorded before midnight are categorized to be the previous time period, any at or after are categorized into the current time period. This is true whether the time period is a day, month, quarter, semester, or year –when we are at the end of any of these. It also might be useful to think in terms of:\nOpening Balance of Current Time Period = Closing Balance for previous time period.\nThe closing balance for the previous time period would be just before midnight where no other transactions logged could be possible either by time itself or by organizational intention and design.\nIf the terminates occur anywhere in the present time period, they must be compared or based against the previous period’s closing balance/current period opening balance to NOT be double counted in the formula.\nIf we can see how this makes sense on a day by day time period calculation, we can understand that when we are concerned with longer time period like a month, the last ‘month’ population that the terminates belonged to is not the current month (end of this month) but rather were part of the previous months employee population. By definition, they are terminates in the ‘current’ time period. So quite aside from the interaction of terminates and hires in the denominator with the previous two options, this option makes sense intuitively even without that interaction impact.\n\n\nFinal Thoughts\nAs mentioned earlier- every organization has to make its own decision as to how it will calculate turnover (and the number of employees in the denominator). Sometimes the decision is made for you because it is a regulatory requirement for some purpose. Other times it is not.\nNone of the discussion in this article is here to suggest or imply that you as the reader need to change your current definitions or calculations. That’s really your decision and responsibility based on your needs.\nBut perhaps I can leave you with some closing thoughts or suggested criteria for your choice, whether it be for regulatory or other purposes:\nIs there a reasoning behind why you are calculating it the way you are calculating it? If a regulatory reason requires one of the above 3 choices in your denominator, then that is reason enough for ‘that’ purpose. Can you have multiple turnover measures for different purposes? Some may be for regulatory purposes, some for benchmarking with other organizations. These may require different calculations. If the purpose for the calculation is for comparison to other organizations, how do they calculate the denominator? Is their entire formula the same? If so, your purposes are met. If not, another formula may be required for that specific purpose or intent. If there is no regulatory required definition for your employee turnover metric, does your metric really measure what you think it measures? And if you have more than one calculation for turnover, are your definitions and terminology allowing you to distinguish between them and still have clarity, meaningful dialogue, and a reaching of all the purposes intended? What do you think?\n\n\n\n\n\n\nAbout Lyndon Sundmark, MBA\nLyndon is a retired HR Professional with over 40 years experience of applying a ‘data-driven’, ‘evidence based’ mindset to HR practices in organizations in a variety of roles and industries."
  },
  {
    "objectID": "posts/Doing HRA1/DoingHRA1.html",
    "href": "posts/Doing HRA1/DoingHRA1.html",
    "title": "Announcing “Doing HR Analytics – A Practitioner’s Handbook With R Examples",
    "section": "",
    "text": "For those of you who have been following my HR analytics blog articles to date -thank you! Your interest has been appreciated. These articles have been written with the intent of encouraging ‘data-driven’ HR management and decision making within organizations.\nAs a result of the feedback from those articles, as well as feedback from some local presentations, I decided to write an e-book on the subject over the late spring and summer 2017:\n“Doing HR Analytics – A Practitioner’s Handbook With R Examples”\nIt is available from Analytics in HR at\nhttps://www.analyticsinhr.com/hr-analytics-books/\nchanged after first year to Amazon\nhttps://www.amazon.ca/Doing-HR-Analytics-Practitioners-Handbook-ebook/dp/B07JGGD8M7/ref=tmm_kin_swatch_0?_encoding=UTF8&qid=1704922909&sr=8-1\nMost of the books that have been written in this field talk about the significance and strategic impact of People / HR Analytics to organizations. This is important still -because very few organizations yet have a firm grasp on the competitive advantage such activity can bring. However, I think it’s exceedingly important that organizations also move from the ‘talking about it’ to the ‘hands-on doing of it’. It has been that spirit behind many of the blog articles I have written. So too with this book.\nThe book covers:\n\nBasic concepts\nA possible scope and breadth\nSeeing HR from an informational and analytical perspective\nData Science tools\nA Data Science framework\n‘Hands on’ HR examples\nSuggestions for how you start\n\nThe first five of the bullet points above are geared towards helping to structure your thinking and efforts. The HR examples draw on a number that I have shared in my previous blog articles, but with some minor tweaks to the code (datasets and code are included with e-book). The final bullet point attempts to address the ‘where you do go from here?’ question.\nThe book is intended for 3 audiences because skills are required from each of these in a typical People /HR Analytics effort:\n\nHR Professionals- to get a better understanding of the informational side of discipline and why both IT and Data Science/ Statistics are important for successful HR analytics efforts\nIT Professionals- in general to get a much better understanding the HR context, and help them realize that metrics and data warehousing is only part of the analytics picture.\nData Science Professionals- to understand that HR, like sales and marketing, is another content area of business needing Data Science as a support to it to move towards ‘data-driven’ and that IT is an important partner in that.\n\nTwo thoughts go a long way to understanding People / HR Analytics:\n\nit’s what happens when you are ‘data-driven’ in HR management and decision-making.\nit’s what happens when you apply Data Science and Statistics to the HR field and the HR information that is generated by organizations.\n\nIt is my hope that content of this book will aid in that understanding\nRegards…"
  },
  {
    "objectID": "posts/Automl/automl.html",
    "href": "posts/Automl/automl.html",
    "title": "Automated Machine Learning –Does This Benefit HR Analytics?",
    "section": "",
    "text": "TL;DR; In a word- YES"
  },
  {
    "objectID": "posts/Automl/automl.html#r-automl",
    "href": "posts/Automl/automl.html#r-automl",
    "title": "Automated Machine Learning –Does This Benefit HR Analytics?",
    "section": "R autoML",
    "text": "R autoML\nR autoML is as the name suggests an R library and therefore uses R syntax. Information on it can be found at:\nhttps://github.com/XanderHorn/autoML/wiki\nOn that page, are links to a few tutorial examples of how to use it.\nhttps://github.com/XanderHorn/autoML/wiki/Tutorial:-autoML-supervised-learning\nhttps://github.com/XanderHorn/autoML/wiki/Tutorial:-autoML-unsupervised-learning\nAs implied earlier, it simplifies things, by having you indicate what the dataset is, what it the target to be predicted in that dataset ,and you can either specify the models you want to run, or you can specify ‘ALL’ to run all that have been built into autoML that are relevant to the target you are predicting. It then runs all the machine learning models and provides a ‘summary’ of each of the models tested in a tabular form so that you can easily and conveniently determine which is the best performing model. While it is running and comparing models you can go grab a cup of coffee and allow it to do the ‘grunt’ work for you.\nThe various algorithms it uses are\n\nHere is some sample R autoML code:\n\nThe models could have been changed to be “All” to run multiple models.\nThe thing to pay attention to is that in the above syntax all you had to specify was the target- that which was to be predicted, an ID field if there, and the dataset name- it knows then that all other fields are to be treated as potential predictors.\nWhen you look at the R autoML example links above, it shows the nature of the results produced- including a summary leaderboard of the results of the models run."
  },
  {
    "objectID": "posts/Automl/automl.html#h2o.ai",
    "href": "posts/Automl/automl.html#h2o.ai",
    "title": "Automated Machine Learning –Does This Benefit HR Analytics?",
    "section": "H2o.ai",
    "text": "H2o.ai\nH2o.ai seems to be a very polished, powerful and flexible solution at this point. It is somewhat unique as a product because it can be run from R, Python, or from a Java web front end. The library backend itself is written in Java. You can even run your models from R or Python review the output there and then switch to the web-based front end to look at even more output in h2o from the same run you just ran in R or Python.\nIt can be found at:\nhttps://www.h2o.ai/products/h2o/\nSome of the algorithms it used are:\nSupervised\n\nCox Proportional Hazards (CoxPH)\n Deep Learning (Neural Networks)\nDistributed Random Forest (DRF)\n Generalized Linear Model (GLM)\nGradient Boosting Machine (GBM)\nNaïve Bayes Classifier\nStacked Ensembles\n XGBoost\n\nUnsupervised\n\nAggregator\n[Generalized Low Rank Models (GLRM)\n Isolation Forest\nK-Means Clustering\nPrincipal Component Analysis (PCA)\n\nThere are others as well in their documentation.\nThis is another one of the automated machine learning solutions that produces a ‘leaderboard’ showing a summary of all the models it ran and their performance ranking them from highest to lowest. This allows you to zero in on the best predictive model very quickly.\nJust as an example, I took a file that had just short of 50,000 records and ran it through all applicable algorithms that h2o has built in. It took about 45 minutes to number crunch through the dataset and all the models (20 or more). Manually it would have taken days to accomplish this coding up one model/algorithm at a time.\nSome example R h2o code;\n\nAnd here is  a sample of its ‘Leaderboard’ output:\n\nI had mentioned earlier that you could also run h2o from a web-based front end. This is what it looks like- and you will notice the runAutoML choice among others:It presents you with a GUI interface to run automated machine learning:\n\nAnd as I mentioned previously, it also allows you to view the results you originally generated from R:\n\nYou will notice that the top model in the RStudio console window is listed the h2o window beneath it. In the h2o window, you can click on the model link to get much more details."
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "",
    "text": "A little over a year ago, Hendrik Feddersen (https://www.linkedin.com/in/hendrikfeddersen)\nand I co-authored a short article on how to start a HR Analytics project within your organization.\nhttps://www.linkedin.com/pulse/helpful-tips-how-start-hr-analytics-project-hendrik-feddersen?trk=prof-post\nThat article and a number of my previous blog articles on People Analytics in R, have been generating a fair amount of interest on LinkedIn, also when some have been published again on http://AnalyticsInHR.com . (And I’m honored by that).\nSome of that interest has resulted in requests for informal mentoring as well along with questions of ‘how do I get started’ in the field of People/HR Analytics?- or how do I prepare myself?. I have shared my thoughts and my own experience in response to those requests. So I thought I would share some of that. I would like to follow up that article with another one- this one- on how do I start preparing myself for the field of People/HR Analytics?\nThis has a little different scope than the article that Hendrik and I co-authored. That one was intended to have the context of ‘within your organization’. This one intends to have the context of the field in general.\nI would preface all that share below with an acknowledgement up front that- I consider myself but one voice is this field. Others may have had different journeys/ experiences that me that have led to their work in this area. So I leave it up to your judgement as to whether you find my comments useful in your situation and experience.\nI think there are several things that help in preparing oneself for this field. They are listed in summary below, and are expanded on in the rest of this blog article.\n\nStart with a good foundational definition.\nBecome familiar with the basic building blocks of People/HR Analytics.\nRead, network, and get hands on practice."
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-hr-and-hr-functions-and-business-processes",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-hr-and-hr-functions-and-business-processes",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "Knowledge of HR and HR functions and business processes",
    "text": "Knowledge of HR and HR functions and business processes\nAs was mentioned previously- ‘analytics’ itself, regardless of context, is about being ‘data driven’ – data analysis with a purpose. People/HR Analytics is really ‘that’ but within the HR context.\nTo understand the HR context means to understand HR. And to understand HR, it means understanding HR functions and business processes. And if I can be permitted to play around with the words -it means understanding and seeing HR ‘functions’ as the HR ‘business processes’ that they really are.\nToo much ‘traditional’ thinking has seen HR as only a series of business ‘functions’. Every major function in HR is a business process- complete with inputs, steps in the process, and outputs. Why is this important?\nTo be ‘data driven’ in HR means that we are collecting data for the purpose of ‘measuring’. Seeing HR as inter-related business processes gets us focused on at least one major a category of HR measurement. (This will be elaborated upon later.) As well, you can’t really understand ‘data driven’ HR unless you live and breathe an understanding of HR itself. HR ‘IS’ the context."
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-information-technology-with-respect-to-data-warehousing-data-retrieval-and-human-resource-information-systems-typical-content",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-information-technology-with-respect-to-data-warehousing-data-retrieval-and-human-resource-information-systems-typical-content",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "Knowledge of Information technology with respect to data warehousing, data retrieval, and human resource information systems typical content",
    "text": "Knowledge of Information technology with respect to data warehousing, data retrieval, and human resource information systems typical content\nPeople/HR Analytics is absolutely ‘dependent’ on HR information as its retrieval and its processing. As the risk of stating the obvious, with today’s technology the existence of HR information isn’t the problem. The problem is that we are drowning in HR data, because there is so much. So a knowledge of getting at the data and getting at it efficiently and processing it efficiently is paramount. This is where a knowledge of data warehousing and retrieval is important.\nIf you are lucky, in your own organization, you may be able to depend on others in IT to do this and be the experts here. Even if that is the case, you are expected to be the HR expert and be able to talk with IT in their language to get the job done. This means that to be effective in People/HR analytics you need to know as much as you can of this area which is ‘relevant’ to your role in HR Analytics. Usually, initial efforts in organizations in HR Analytics are ‘proof of concept’ or ‘skunkworks’ projects that are ‘informal’ and ‘off the books’ because no resources have been allocated for this at an early stage. In that case you may have to be in this role at a rudimentary level yourself. The rudimentary level will often be at ‘at least’ a SQL commands level for retrieval of information.\nAs well, you have to be familiar enough with you Human Resource Information System and its contents to know what kinds of HR questions can be answered by your HR data.\nIn any case, being able to ‘talk the talk’ and ‘walk the walk’ with the information technology dimensions of People/HR analytics, requires the building block of knowledge in this area."
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-business-statistical-analysis-particularly-with-respect-to-hr-data-hr-questions-hr-problems-and-hr-measurement-and-metrics.",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-business-statistical-analysis-particularly-with-respect-to-hr-data-hr-questions-hr-problems-and-hr-measurement-and-metrics.",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "Knowledge of business statistical analysis – particularly with respect to HR data, HR questions, HR problems, and HR measurement and metrics.",
    "text": "Knowledge of business statistical analysis – particularly with respect to HR data, HR questions, HR problems, and HR measurement and metrics.\nAnother ‘at the heart of analytics’ is analysis, measurement, and statistics. This isn’t optional. If you want rigorous, robust, and informative ‘data driven’ decision making this is an essential part of the picture. (If you hate statistical analysis- get over it. This skill is absolutely essential to data mining and analytics.)\nToo much of ‘analytics’ has the propensity for organizations to be satisfied with ‘cool graphics’ especially interactive ones. The same goes for drilldown and slicing and dicing of information. Don’t get me wrong. These ARE part of the analytics picture, but by themselves are ‘superficial’.\nTo really understand what is going on in your data, you must delve into statistical analysis. And I tend to recommend learning it with business examples.\n\nYou need to understand your data, your world, your HR in terms of Independent variables and dependent variables- what you are trying to predict, and what you are predicting with.\nYou need to know your data in terms of measurement- what are categorical, ordinal, and continuous variables, and how that affects the particular statistical analyses and algorithms you use.\nYou literally need to see all of HR and HR information as measurement- seeing HR as business processes is ‘part’ of this.\nYou need to understand that different statistical analyses, algorithms, and procedures are designed to answer different HR questions. So you must understand the lay of the landscape from what to choose from. You must also have the HR questions to begin with to drive the effort. And you need to know what types of statistical procedures answer what types of questions\nYou must understand that statistical analysis and HR analytics isn’t JUST about ‘Predictive’ analytics but ‘descriptive’ analytics as well. Too much of what it written on analytics these days would give you the impression that ‘analytics’ is about predicting. That’s because its ‘cool’ and gets people’s attention. And yes IT IS COOL and IT GETS PEOPLES ATTENTION. But again, if you limit your understanding to just this view- its superficial. Statistical analysis and the resulting being ‘data driven’ can be as a result of descriptive analytics as well- descriptive statistics. And with regard to prediction, another misunderstanding is to assume that statistical prediction is all about predict the future only. Wrong. Statistical prediction for categorical ‘dependent variables’ is to predict a best fit category for some new observation in a population. That’s current time not future. So prediction can be present and future.\nInteractive graphics have critical role to play in communicating to others what is going in the data, and in an effective way. But it’s the statistical analysis that helps separate the wheat from the chaff, separate what is statistically significant in the data and what isn’t. Graphics by themselves don’t do this.\n\nTo live and breathe statistical analysis you have to be familiar with statistical packages. There are many out there. SPSS and SAS are commercial packages that have been around for decades. They are good- but they cost. R has also been around for decades and its free- although its more command based and has a higher learning curve for some. I tend these days to prefer R because it’s free. Free helps, when you want to be empowered to learn analytics."
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-the-scope-of-whats-measureable-in-the-hr-context.",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-the-scope-of-whats-measureable-in-the-hr-context.",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "Knowledge of the scope of what’s measureable in the HR context.",
    "text": "Knowledge of the scope of what’s measureable in the HR context.\nThere is a tendency because of the historical roots of HR analytics being partly steeped in HR metrics and HR scorecards, to assume that ‘that’ is it with respect to HR analytics and what we measure. But again if we limit our picture to just that, then once again its superficial and only part of the picture.\nIf we step back and try to get a more extensive lay of the land, we find that there are at least 3 major areas of measurement in HR, that can form the basis of analytics and being data driven. These are:\n\nWhat is going on with the employees in our organization? Some of our HR decision making will be based on the employee picture.\nWhat is going on in our HR operations? Some of our HR decision making will be based on how efficiently and effectively our HR processes. ```\nWhat is going on in our HR decision making and policies itself? How do we make HR decisions? How good or bad are our decisions and actions? Some of our HR decision making will be based on this – if we want to be extensive data driven.\n\nWhat is going on with the employees in our organization?\nThe best way to describe this category is that it is the traditional HR Metrics for the most part. Another way to think of HR metrics, is that it concerns itself with measurement of various aspect of the employee life cycle in organizations including but not limited to:\n\nApplicant counts\nInterview counts\nHires\nEmployee counts\nTraining activity\nGrievance activity\nHealth and Safety activity/injury\nEmployee engagement surveys\nTerminations\nExit Interviews\n\nI have shared this link before, but a good starting point for understanding the kinds of things that can be part of traditional HR metrics is the following:\nhttp://www.hrma.ca/wp-content/uploads/2012/05/HR-Metrics-Standards-and-Glossary-v7.3.pdf\nWhen the above types of metrics are calculated (often with data warehousing technology to make it efficient so that we don’t spend inordinate amounts of time preparing the data), then both predictive and descriptive analytic techniques can be applied to these metrics to answer all sorts of business questions. We want to be data driven, by letting what our data is telling us through proper statistical analysis, help guide the decisions and actions we make.\nThis is what most organization think of first when they think HR analytics. It is a good starting point. But just don’t end it there.\nWhat’s going on in our HR operations?\nBeing ‘data driven’ should mean that we are data driven in our HR operations as well. All that we do in People/HR Analytics should add value to the organizations. How efficient and effective we are in conducting our HR activities - is one part of this.\nTraditionally, and alternatively for that matter, this could be thought of as quality improvement/continuous improvement of HR operations- the voice of the customer. This is a whole other area of study even on its own. But we can be ‘data driven’ in how we carry out our HR business operations and activities. So this could be perceived as a legitimate part of the People/HR analytics part of the picture.\nThe primary ingredient or enabler of gathering this kind of information – is HR Request Tracking systems. You need to monitor every single transaction from inception to completion, cradle to grave, that comes into HR. and you need to monitor where it came from, who worked on it, when it came in and when it was completed.\nThink of your HR business as being like some the global delivery businesses, who can track any package in real time all the time, both for its needs and the customer needs. You get the idea. Tracking every request – helps you to be ‘data driven’ in improving your HR services to your employee who are your internal customers. In fact, you cannot have ‘data driven’ operation without the ‘data’. That should be painfully obvious. Does your organization do this?\nWhat is going on in our HR decision making and policies itself?\nWhat this is getting at is the ‘infusing’ of People/HR analytics into the actual carrying out of our HR activities themselves in real time. Being data driven in real time. The previous two categories might tend to have the flavor of after the fact. And that’s ok. But being able to use analytics in real time is even better.\nSome examples:\n\nTaking features and characteristics of our known job descriptions and job classifications to predict the best fit of a new job description to a best fit job classification using People/HR analytics\nTaking our existing HR information on existing employees and terminates, along with engagement survey data, and exit interview data to see if there are any patterns in the data with respect to who leaves or who stays, and if so, predict likely future terminates before they occur. Or better yet through identification of patterns in the data, make changes in HR policies to prevent the turnover in the first place.\nTaking our existing HR information on absenteeism, and see if there are any patterns in the data. If so, adjust our policies and decisions where possible to discourage absenteeism more effectively.\n\nIn each case, its recognizing that infusing analytics in our decision making in real time is another potential part of the ‘total’ landscape of People/HR analytics.\nThe important lesson here, is that the application of People/HR analytics in many ways is only limited by your imagination. Don’t limit it, by thinking that its only traditional HR metrics (as important as these are).\nI guess a final message in this section, is to review why ‘data driven’ is so important. In its absence, organizations still take action. They always have. The issue is ‘informed’ action. Sometime the problem is taking action when no action is needed. Other times it’s not taking action when action is needed. If we aren’t ‘data driven’ how do we KNOW?"
  },
  {
    "objectID": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-the-data-science-framework-and-how-to-apply-it.",
    "href": "posts/HRAnalyticsPrepare/HRAnalyticsPrepare.html#knowledge-of-the-data-science-framework-and-how-to-apply-it.",
    "title": "How Do I Start Preparing Myself for the Field of People/HR Analytics?",
    "section": "Knowledge of the Data Science framework, and how to apply it.",
    "text": "Knowledge of the Data Science framework, and how to apply it.\nSo far we have been hitting on the principle of being ‘data driven’ very hard in this blog article- for good reason. It goes right to the DNA of what analytics is about.\nWith that in mind, knowledge of one other area is helpful, and I mean REALLY helpful. That area is the Data Science Framework.\nOne of the challenges in People/HR analytics is the issue of how do I both structure my thinking and my efforts/endeavors for maximum likelihood of success? I think the answer to that is part is to have a good framework to help guide the process.\nData Science, as a process, is that kind of helpful framework- because its intent is enable being ‘data driven’ in a structured way.\nThere are a number of books out there on the subject. Many are not necessarily written for the context of HR. You have to apply the framework to an HR problem.\nSome of my previous blog articles illustrate this:\nhttps://www.linkedin.com/pulse/people-analytics-example-using-r-lyndon-sundmark-mba?trk=mp-author-card\nhttps://www.linkedin.com/pulse/people-analytics-using-r-employee-churn-example-sundmark-mba?trk=mp-author-card\nhttps://www.linkedin.com/pulse/people-analytics-r-job-classification-revisited-sundmark-mba?trk=mp-author-card\nOne book that I came across that I found useful regarding R and Data Science was:\nPractical Data Science with R by Nina Zumel and John Mount.\nhttps://www.manning.com/books/practical-data-science-with-r\nThe basic steps in the framework from that source are:\n\nDefine a goal.\nCollect and manage data\nBuild the model\nEvaluate and critique model\nPresent results and document\nDeploy model\n\nThe following is an elaboration of that from one of my previous blog articles:\n1.Define a goal\nAs mentioned above, this means identifying first what the HR management business problem is you are trying to solve. Without a problem/issue we don’t have a goal.\n2.Collect and Manage data.\nAt its simplest, you want a ‘dataset’ of information perceived to be relevanto the problem. The collection and management of data could be a simple extract from the corporate Human Resource Information System, or an output from an elaborate Data Warehousing/Business Intelligence tool used on HR information. For purpose of this blog article illustration we will use a simple CSV file. It also involves exploring the data both for data quality issues, and for an initial look at what the data may be telling you\n3.Build The Model.\nThis step really means, after you have defined the HR business problem or goal you are trying to achieve; you pick a data mining approach/tool that is designed to address that type of problem. With absenteeism as an HR issue, are you trying to predict employee with propensity to high absenteeism from those who aren’t? Are you trying to predict future absenteeism rates? Are you trying to define what is normal absenteeism from that which is atypical or an anomaly? The business problem/goal determine the appropriate data mining tools to consider. Not exhaustive as a list, but common data mining approaches used in modelling are classification, regression, anomaly detection, time series, clustering, association analyses to name a few. These approaches take information/data as inputs, run them through statistical algorithms, and produce output.\n4.Evaluate and Critique Model.\nEach data mining approach can have many different statistical algorithms to bring to bear on the data. The evaluation is both what algorithms provide the most consistent accurate predictions on new data, and do we have all the relevant data or do we need more types of data to increase predictive accuracy of model on new data. This can be necessarily repetitive and circular activity over time to improve the model\n5.Present Results and Document.\nWhen we have gotten out model to an acceptable, useful predictive level, we document our activity and present results. The definition of acceptable and useful is really relative to the organization, but in all cases would mean, results show improvement over what would have been otherwise. The principle behind data ‘science’ like any science, is that with the same data,people should be able to reproduce our findings/ results.\n6.Deploy Model\nThe framework presented goes a long way in structuring your thinking and activities and I have found it quite useful personally. At the end of the day you want to increase the likelihood that your People/HR analytics activities provides value to your organization. This framework can help."
  },
  {
    "objectID": "posts/Rattle/Rattle.html",
    "href": "posts/Rattle/Rattle.html",
    "title": "A Good Article On the R ‘Rattle’ GUI",
    "section": "",
    "text": "Hi Everyone,\nThis is just a short post.\nFor those of you who have read my previous LinkedIn blog articles and also for those who purchased my book- you will know that I referred to the use of R Rattle library as a GUI to do an immense amount of machine learning and data science analyses quickly on HR Data you may have authorization to analyze.\nBecause its a GUI it allows for a lot of point and click to do your initial analyses and also to generate the R core necessary behind the scenes for you -very powerful.\nI just came across this article on R Bloggers and r4stats.com on describing the GUI part of this and its use- and found it to be a very good one:\nhttp://r4stats.com/articles/software-reviews/rattle/\nJust thought I would share by forwarding it on.\nThanks"
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "“Job Analysis and Job Descriptions in their current form, for the most part, are NOT data-driven.”\n\n\nIf you have read any of my previous blog articles- but particularly the last one- you will get the clear impression that I don’t see HR Analytics as an ‘add-on’ to HR. I see it fundamentally as a means to change the way ‘we do’ HR.\nIf you accept HR Analytics as ‘data-driven HR management and decision making’, then it’s all about making ‘traditional’ HR Management and Decision Making- ‘data-driven’. It means ‘data-driven’ is infused into all of HR in some fashion and at some level.\nThis includes Job Analysis and its end results as well.\nIn this article I therefore wanted to focus on Job Analysis and Job Descriptions concerning ‘data-driven’.\nThe reason for this is because Job Analysis impacts the many other HR practices that are dependent on it. Being data-driven here enables many ‘other’ HR practices to start becoming more data-driven.\nThese HR practices include:\n\nsalary administration\nclassification\nrecruitment /talent acquisition\nlabor relations/ collective bargaining\ntraining\ncareer planning\nhuman resource planning\noccupational health and safety\nand others…\n\nYou can probably regard this article again as an ‘opinion’ piece. Its intent is to get you to ‘think differently’ about job analysis and job descriptions.\nThis blog article will:\n\nRevisit what ‘data-driven’ requires and what this might mean when it is applied to this area of HR.\nTake a brief look at what makes up much of traditional job analysis and its methodologies, and job descriptions content over the last 50 or more years and how those approaches reflect the larger management systems that HR finds itself in.\nEvaluate the degree to which ‘traditional’ job analysis methodologies and job descriptions lend themselves to the requirements of ‘data-driven’.\nExplore in practical terms why we should care about ‘data-driven’ in job analysis\nExplore an alternative way of thinking and methodology for job analysis and descriptions that is ‘data-driven’.\nShow what new possibilities ‘data-driven’ Job Analysis and Descriptions give us in terms of truly transforming the work of HR in the organization.\n\n\n\n\n“Data-Driven” is at the heart of what HR Analytics is all about - ‘Data-driven’ HR management and decision making . We are interested in ‘evidence-based’ management and decision-making regarding people.\nThis means at least three things:\n\ncommitting to making more HR decisions based on objective information and more rigorous analysis.\norganizing ourselves around making ‘that’ objective information more visible and accessible in our HR practices\nkeeping that information in a form that readily lends itself to automation and analysis. This requires the information to:\n\nexist\nbe standardized\nbe structured\nbe quantifiable or categorizable\nbe stored in a database\nallow for the summary, comparison, and contrast readily and quickly across the entire set of data.\nallow for appropriate statistical analyses and reporting to answer questions posed of that data.\n\n\n\n\n\nTo answer this question, it is always helpful to look at a variety of definitions from a variety of different sources, to get a sense of the commonalities that emerge.\nA quick and cursory Google search turned up many definitions.\nJob Analysis Definition Google Search\nOne of them that came up in my Google search was from Wikipedia:\nWikipedia\nhttps://en.wikipedia.org/wiki/Job_analysis\n“Job analysis (also known as work analysis[1]) is a family of procedures to identify the content of a job in terms of activities involved and attributes or job requirements needed to perform the activities.\nJob analysis provides information of organizations which helps to determine which employees are best fit for specific jobs. Through job analysis, the analyst needs to understand what the important tasks of the job are, how they are carried out, and the necessary human qualities needed to complete the job successfully.\nThe process of job analysis involves the analyst describing the duties of the incumbent, then the nature and conditions of work, and finally some basic qualifications. After this, the job analyst has completed a form called a job psychograph, which displays the mental requirements of the job.[2] The measure of a sound job analysis is a valid task list. This list contains the functional or duty areas of a position, the related tasks, and the basic training recommendations. Subject matter experts (incumbents) and supervisors for the position being analyzed need to validate this final list in order to validate the job analysis.[3]”\nI wont profile all the google hits above in this blog article, but if you look at many of them they are identical or very similar- and have many commonalities between them.\nWhat are some of these commonalities?\nThey:\n\nstudy the job to identify its content which can include:\n\nactivities\nresponsibilities\nqualifications\nworking conditions\ntasks\nduties\nknowledge\nskills\nabilities\nimportance of tasks\nhuman qualities\nhow often tasks are performed\n\nhave multiple purposes in mind for the usage of the gathered information.\nhave a job description often as the most immediate end result. Typically this written narrative describes tasks, duties and responsibilities of a position. This relationship between job analysis and job description is described as well in Wikipedia:\n\n\n-   [Wikipedia definition of a job description](https://en.wikipedia.org/wiki/Job_description)\n\n    -   *\"According to Torrington, a job description is usually developed by conducting a [job analysis](https://en.wikipedia.org/wiki/Job_analysis \"Job analysis\"), which includes examining the tasks and sequences of tasks necessary to perform the job. The analysis considers the areas of [knowledge](https://en.wikipedia.org/wiki/Knowledge \"Knowledge\"), [skills](https://en.wikipedia.org/wiki/Skills \"Skills\"), and [abilities](https://en.wiktionary.org/wiki/abilities \"wikt:abilities\") needed to perform the job. Job analysis generally involves the following steps: collecting and recording job information; checking the job information for accuracy; writing job descriptions based on the information; using the information to determine what skills, abilities, and knowledge are required to perform the job; updating the information from time to time.\"*\n\n-   Historically-once produced- these job descriptions would be stored in filing cabinets for retrieval when needed.\n    In the 1980s and later these would be 'Word' documents stored on a network file server somewhere.\n    Some organizations took it a step further and put them in HTML format on web server.\n    Regardless though, these job descriptions are still 'narrative documents' in whatever technology form they're in.\nThe above methods and formats have been in the literature, textbooks and professional HR education for 40-50 years with very little change. Two key questions at this point might be:\n\nwhy is this the past and current paradigm for job analysis?\nDoes this paradigm lend itself to being data-driven?\n\nWe will explore the first of these questions in this section immediately below and the second in the next.\nWhy is this the past and current paradigm for job analysis?\nTo answer this, we have to step back a bit and realize that the job and/or the position is and has almost always been the focal starting point of all job analysis.\nFrom above:\n“Job analysis (also known as work analysis[1]) is a family of procedures to identify the content of a job in terms of activities involved and attributes or job requirements needed to perform the activities.\nJob analysis provides information of organizations which helps to determine which employees are best fit for specific jobs. Through job analysis, the analyst needs to understand what the important tasks of the job are, how they are carried out, and the necessary human qualities needed to complete the job successfully.”\nNotice that the focus is the JOB .\nIn this paradigm, if you wanted to understand the organization and document its work, you study and document the jobs -one by one. The assumption is that the ’work of the entire organization is really seen as the aggregate of all of these together. You start at the individual level and you ‘aggregate’ to get the entire picture of an organization’s work. This is a ‘proxy’ for understanding the business that the organization is in. The assumption here is aggregating from individual jobs and positions will synthesize into an overall ‘coherent’ picture of the work of the organization.\nThis ‘job by job’ analysis paradigm is not independent of the other organization management paradigms they are within. To map out and to know when you have the work of the entire organization covered, you traditionally have depended on organization charts. When you have covered every position/job in the organization chart you supposedly have the work of the organization documented.\nThat got me doing a little detective work on how organization charts originated and why they are so pervasive. This would be a whole other topic in itself which I won’t go into detail here. But that minor detective work led me to the following book:\nThe Leaders Handbook -Peter Scholtes\nIn that book, Peter indicates that organization charts came from ‘train-wreck’ charts (an apt metaphor? ;) tongue in cheek). The intent was to prevent train wrecks that occurred in the 1800s from happening in the future. These ‘train-wreck’ charts among other things illustrated:\n\nchains of commands\nfunctional divisions\nclear descriptions of responsibility\n\nIn his book, Peter goes through a chronological analysis of how the above eventually resulted in Management By Objectives (MBO) as a system of management.\nOut of those larger management systems such as MBO would have likely come the HR methodologies for job analysis. (i.e. clear descriptions of responsibility sound an awful lot like job descriptions to me)\nThe point I am raising is that our HR practices didn’t emerge in a vacuum. They were responses intended to be consistent with the larger management systems they found themselves within.\nThese larger management systems aren’t ‘without’ impact or consequences.\nIn MBO, the consequence is that if you want to ‘understand’ a business, you look at an organization chart. You look at the chain of commands, functional divisions, and descriptions of responsibility. For what it’s worth, personally I never found that to be true. Looking at the above helped me understand how the business was organized or structured - but that isn’t the same as understanding the business.\nIn HR, the impact was ‘job descriptions’ and a methodology for job analysis that was ‘one by one’ and with the assumption that aggregating from the ‘bottom up’, you would then have a picture of the work of the organization and the business it was in. The ‘one by one’ ultimately weaves itself into traditional performance appraisal systems. We evaluate the performance of employees ‘one by one’. MBO requires it, and traditional job analysis and descriptions support it -‘one by one’.\nThe ‘bottom up’, ‘one by one’ paradigm rarely leads to a coherent picture of the work of the organization and the business it is in- more often than not it is an incoherent, fractured picture.\nLet me illustrate why.\nPicture Puzzle Analogy\nMost of you are familiar with picture puzzles (a picture that has been carved up into several hundred pieces). You probably grew up with them as kids. The reason why these puzzles work is that they start with a ‘whole picture’ and carve it up into pieces. When you put those pieces together they from the whole original picture.\nNow imagine instead that you have half a dozen people independently who are attempting to create puzzle pieces from scratch. Even if they know that the end result is ‘a picture’ (hopefully a coherent business purpose), their differences (as human beings) will result in a variety of puzzle pieces which are unlikely to fit together well or at all.\nWhen you start with the pieces, rather than the entire picture, those pieces will find it almost impossible to form an entirely coherent picture. There is very little commonality between those pieces.\nI would suggest that traditional job analysis methods are a lot like that. You may have all the pieces, but you have very little way of linking them together- finding pieces that fit and join together. We are so focused on the ‘job’ as the entity under study that we cant see the larger picture under this paradigm. (It’s also noteworthy in the above wikipedia definition it said ‘also known as work analysis’. I think we have forgotten that.)\nWe need a paradigm with job analysis methodologies and end results (job descriptions) that start with the overall picture and deliver puzzle pieces that stand on their own - AND still fit back together as a unified whole.\nBeing ‘data-driven’ in our job analysis methodologies and job description results is a potential enabler in this direction.\n\n\n\nLet’s look at the characteristics of ‘data-driven’ and evaluate the degree to which traditional job analyses and descriptions meet these requirements.\n\n\nLoosely speaking- even traditional job analyses and descriptions meet this criterion. Traditional job analyses are a manual process for ‘generating’ information about the contents of a job. And as the methods mentioned above indicate- there are quite a few ways of generating that information. PASS\n\n\n\nLoosely speaking we could say that this requirement is also met (at least in part). Our study of jobs through job analysis focuses on standardizing the content. This is reflected above in what the typical content areas are:\n\nactivities\nresponsibilities\nqualifications\nworking conditions\ntasks\nduties\nknowledge\nskills\nabilities\nimportance of tasks\nhuman qualities\nhow often tasks are performed\n\nIf you cover some or all of these ‘consistently’ in every job analysis and resulting description you are ‘standardizing’ your ‘approach’ to the content. (but maybe not the expression of that content)\nAt least at a ‘narrative’ level then most organizations achieve standardization through guidelines and procedures. Undoubtedly though, some variation will occur in the standardization due to different people with different writing styles. PASS\n\n\n\nThis means:\n\nnot only are the contents standardized but they also\nare expressed in a way that we can easily see and find the equivalent information from job to job.\n\nThis will be a PASS most of the time if the narrative descriptions are standardized and well written. But in my opinion -a bare pass because there can be a lot of subjectivity and judgment around the equivalence around the job content and characteristics.\nThose of you who have done job classification by comparing job descriptions to ‘narrative’ job class specifications could probably attest to the fact that even when policies, procedures, and guidelines are in place for writing job descriptions- the ease of slotting it into the most appropriate job class is not easy. It is definitely partly a function of how well the job description was written in terms of structure and standardization and the job class specifications as well.\nIn other words, even with the best of intentions and guidelines for writing, the use of traditional narrative job descriptions can be very difficult.\n\n\n\nBecause most traditional job analyses and descriptions end up being narrative documents derived from narrative information gathering, this requirement is typically not met. At best there may be some quantification of time spent (i.e. proportion of time spent) on major responsibilities. But for such things as skills, knowledge, and abilities required, these may be vague statements of whether there is more of something or less of something required, and these are usually ‘narrative’ -NOT quantified.\nEven if minimum information is quantified, if the structuring of job information or the standardization of it is weak, quantification in these circumstances doesn’t help us much. We really don’t know if X% time spent on Y responsibility is really comparable to Z% time on the same responsibility in another job.\nWithout strict attention to standardization- we may or may not be comparing the same responsibility. True standardization means that we capture the same types of common data on EVERY JOB. Narrative job descriptions (unless heavily standardized, structured, and written with an eye to eventual quantification of that information) rarely meet this criteria. FAIL\n\n\n\nRight from the ‘get-go’ this requirement in traditional job analysis and descriptions is rarely, if ever, met. Combing and scouring narrative sources of written information, verbal content from interviewing Subject Matter Experts (SMEs), etc. is not in a format needed for storage in a database. Automating job descriptions to the point of being either Word documents on a file server or HTML documents on a web server for easy access isn’t ‘being stored in a database’.\nFor job analysis information and resulting descriptions to be stored in databases:\n\nEach job must be stored as a single entity (often a record) in the database. (often somewhat comparable to thinking of a row in Excel)\nAll the information used to describe that job and its characteristics must be represented by data elements or fields for that record or sub-records. (often comparable to thinking columns in Excel for a single row)\nThe information must be structured and standardized in the database. (In Excel, all columns appear in every row)\nIf information is to be used to compare one job to another, all information to be used for comparison must be categorized into standardized qualitative information if textual (ie non-numeric) or quantified into measures if numeric. The reason for this is to have common denominators on which to compare and contrast different jobs.\nThe database must hold the information on all documented jobs.\n\nThe verdict on ‘traditional’ job analysis and descriptions: FAIL\nNothing is preventing an organization from thinking non-traditionally here. The idea of storing job information in a structured, quantifiable, measurable way is NOT NEW. I am not the originator of this idea. Early attempts to use the technology to do this are at least 30 years old if not older. Even 30 years ago, it gave me some sense of the future possibilities and how things could change if we wanted them to.\nSo why no huge uptake?\nA lot of the answer to that goes back to the difference that HR Analytics requires. ‘Data-driven’ HR Management and Decision Making absolutely requires structured and standardized information and measurement. Traditional HR doesn’t. It’s why the literature and methods around this part of HR haven’t changed much for 50 years. I’m not sure that’s a good thing.\n\n\n\nThis too is an immediate FAIL on the criteria of ‘data-driven’.\nWord documents on a file server or HTML documents on a web server do not allow for any quick comparison or contrast across all jobs. At best Word documents would be a manual effort of perusal, and HTML documents would offer keyword search capabilities. But there is no ability to quantitatively compare information across all jobs.\nSo, on balance, how do traditional job analysis and job descriptions stack up against ‘Data-Driven’ HR?\nPoorly.\n\n\n\n\nI think the answer to this comes down to a few major reasons.\n\nTraditional job analysis and descriptions and their methodologies make the process of doing this far more manual than it needs to be and far longer than it needs to be.\nSome things in HR are way more difficult to do or do efficiently and well, or do at all -with traditional job analysis and descriptions.\nAs mentioned earlier there are many uses made of job information in HR, so by making improvements here, we improve many areas of HR.\n\nLet’s look at each of these reasons in turn.\n\n\nThose of you whose job is job analysis can probably attest to this. Traditional Job Analysis and Job Descriptions are a laborious, manual, and slow processes- whether its in the creation of these or usage of them when job descriptions are done.\nFor creation:\n\na request for a new job is created.\nyou will likely be required to interview the person to who this job will report to.\nyou will likely need to interview SMEs if there are any\nif it’s an existing job that is changing- you will likely have to interview an incumbent\nyou will likely have to review your organizational standards for job analyses and descriptions\nyou will likely have to review other jobs to understand how this is similar or different from them\nyou will prepare new job description from all your analysis\n\nBest guess-a few days to organize all of this into an end result.\nFor HR usage:\nLet’s use job classification (using narrative job class specs) as an example:\n\nyou will need to read the job description in question- getting some sense of its content.\nif you are a season classification officer, you will form an initial impression of the job family it falls into.\nyou will then need to review at least a few job classification specs, and perhaps many other comparable positions/jobs to ballpark your decision on classification.\nyou meet with the requester and present findings and hopefully justify the decision made to their satisfaction.\n\nBest guess-a few days to organize all of this into a completed decision.\nWhat if being ‘data-driven’ in our approach resulted in job analysis and descriptions taking no more than a couple of hours? What if upon obtaining the information- classification took only minutes?\nDo we in HR actually care about increasing the efficiency of our methods, procedures, and techniques and resulting customer response time?\n\n\n\nThere are at least a few examples here that come to mind.\nConsider the following:\n\n\n\n\nStrategic planning, career planning, and succession planning are all dependent on good job information and to be able to use that information in a specific way. The common denominator in all these planning efforts is the the successful ‘defining and bridging of gaps’.\n\nFor strategic planning it’s defining the gap between who, where, and what you are now as a business and the desired future for your business- desired by you as an organization and by your customers - and bridging that gap with plans and activities. ( strategic planning is really career planning at a ‘business entity’ level).\nFor career planning, it’s defining the gap between who you are and where you are right now and what you desire to be in the future and then bridging that gap with plans and activities. (career planning is really strategic planning at an ‘individual’ level)\nFor succession planning, it’s defining the gap between job requirements needed for the future, and succession candidates current capabilities and coming up with developmental plans and activities to reduce or bridge that gap.\n\n\nDoing any/all of these, at some point, requires us to bring this down to a job content, job analysis, job description level. To truly get down to the nuts and bolts of defining gaps- you have to define differences. To define differences, you must be expressing things in a standardized way that allows you to say for instance:\n\nthese items belong to job 1 and only job 1\nthese items belong to job 2 and only job 2\nthese items belong to both job 1 and job 2\nto get from job 1 to job 2, the person must keep all things common to both and learn items in job 2\n\nThe same goes for strategic planning:\n\nThese activities/products/services are current and only current\nThese activities/products/services are what we desire for future and are only future\nSome activities/products/services will be both current and continue into future\nTo get from current to future successfully we will need to drop some activities /products /services, understand that some will continue to need to be maintained, and some new initiatives will need to be introduced and plan for all of these.\n\nEven strategic planning eventually has to be expressed in terms of how it affects jobs, their contents, and differences.\nIf your job analysis methodologies and job descriptions don’t quantify, standardize, structure job information and stored in a database - allowing for the summary, comparison, and contrast readily and quickly across the entire set of data- the quality of the above planning processes and their implementation may be limited.\nAre your existing traditional job analysis methods and traditional job description format and contents capable of this and up to the challenge? ‘Data-driven’ in this area of HR is likely to be an enabler for this.\n\n\n\nIf you wanted to extend the strategic planning example:\nAt some point in strategic planning, as per the above, you should;\n\nidentify the products and services you are no longer engaging in\nidentify the products and services that you are going to continue to engage in\nidentify the new initiatives, products, and services that will come on stream\n\nEach of these steps requires understanding how this will affect your current jobs and the employees that are in them.\nDo we know how many FTEs will be saved by those products and services no longer engaged in and exactly where?\nDo we know how many FTEs will need to continue to support continued products and services and exactly where?\nDo we know how many FTEs will be needed to properly support new initiatives and exactly where?\nWithout good, robust, and data-driven job analysis methodologies and job description content and storage it’s very difficult for HR to answer these questions and to do it promptly.\n\n\n\nThe primary goal of analytics, and for our purposes HR analytics- is to improve performance in the organization whether it be through\n\nHR metrics- so that the organization knows what is happening to its human resources over time and can be more proactive\nHR Services- being much more aware of the level and quality of services we provide to our organizational customers and making improvements as a result of that\nHR methodologies- efficiency and effectiveness- to do what we do better and quicker (without sacrificing better)\n\n\nJob content is interwoven and related to all of these.\nIn short- THIS IS WHY WE SHOULD CARE ABOUT DATA-DRIVEN JOB ANALYSIS AND DESCRIPTIONS.\n\n\n\n\nThe short answer is to change our paradigm and its associated methods, increase our use of technology, and quantify our job information.\nHere are some practical ways to change our ‘50-year-old’ thinking in this area and move on from historical and current approaches.\n\nOur job analysis methods need to start with the entire picture and work back from there.\n\nThe concentration/focus is not on ‘job’ analysis but ‘work’ analysis.\nJobs just happen to be the way we segment out that work into understandable and manageable chunks for a position or person.\nany one job is just a subset or snapshot of the much larger picture of the work of the organization.\nAs a subset of a larger whole- differences and similarities are more readily discernible\n\nUnderstand that starting point of understanding the entire picture of the work of the organization can’t be the organization chart (aka train wreck chart). An organization chart tells you nothing about the work of the organization, only how you are structured. The starting point is:\n\nConfirm what business the organization is in.\nThen identify and enumerate all of the products and services the organization produces/performs in being in that business\nThen the identification and enumeration of the business processes that produce those products and services\nThen identification and enumeration of the tasks, knowledge, skills, qualifications necessary to perform those business processes.\nFinally determine how to apportion those tasks, knowledge, skills, and qualifications into individual jobs and positions. A ‘job’ description then is the documentation of assigning tasks and knowledges within the business processes that lead to the provision of products and services to those individual jobs and positions. That way you can go both directions (up and down) in job analysis/work analysis for the organization. You ‘could’ represent these via an organization chart- but you would probably find it equally and possibly more useful to represent your organization via business process charts and identify (in some manner) how the positions are integrated into them. Each job is a ‘subset’ of those tasks and knowledges.\n\nUnderstand what measurement means in relation to job information and job analyses and realize much is measurable and quantifying that job information. This includes:\n\nunderstanding the presence or absence of tasks, knowledge, and skills, qualifications in any ‘one’ job out of the total picture the tasks knowledge, skills and qualifications needed is a form of measurement\nunderstanding that capturing of ‘levels’ of knowledge, skills, and qualifications on some sort of scale is in fact -measurement.\nunderstanding that time spent on various tasks is measurement\n\nUnderstand that when you standardize the job information and quantify it, you are enabling that information to be more readily stored in a database.\nUnderstand that when job information is structured and standardized in a database, this is what enables much more extensive and robust use of that information.\nUnderstand that when job information is structured, standardized, and quantified in a database that web applications can be developed to serve out much of the functionality expected from the traditional approaches but also ‘much more’. It’s actually a two way street here- if we change to a new paradigm for job analysis- the tools supporting it must change. Traditional job analysis fundamentally requires job descriptions, paper documents, word docs on a file server, or HTML docs on a web server. Data-driven requires databases and computer applications to manage, house, process an analyze the job information.\n\n\n\n\n\n\n\nJob Descriptions that are dynamically kept up to date. Storage in a database means they are as current as the last time they were update and from a central source\nJob Descriptions that are often available in a variety of formats- HTML, Word, PDF etc\nJob Descriptions that are keyword searchable\nJob Descriptions that can be created from online wizards that guide the process or printed questionnaires to select job content from.\nSecurity separation from those who are consumers of job information from those responsible for creating the job content in web applications.\n\n\n\n\n\nBetter organizational HR planning processes - through detailed defining gaps/differences/ similarities between positions for planning purposes including career planning and succession planning\nBetter organizational strategic planning processes - by helping the organization understand the effect on existing FTEs of dropping, maintaining, and adding new product and service expectations will have on the organization.\nMuch more robust HR Management and Decision-Making processes - by putting job information more in the form necessary to be able to be used in machine learning and artificial intelligence to complement the current strengths of this in the organization.\nBetter HR and organization performance through recognizing that organizing job information around products/services and their corresponding business process provides a paradigm for naturally gravitating the organization in the direction of ongoing continuous improvement.\n\n\n\n\n\nAs mentioned earlier, my intent in this blog article is to stimulate ‘thinking differently’ as to how we understand this area of HR and give some thought as to whether we should operationalize it differently in our HR practices. Are we motivated as HR professionals to do that?\nI have often had the thought that many of us might unintentionally be doing a ‘Rip Van Winkle’ when it comes to some of our HR practices and methodologies. We could go to sleep for 50 years, wake up, and still do HR work with our skill sets because so little changes.\nThe changes that ‘do occur’ are often externally imposed on the organization and HR profession in the form of meeting changes in legislative regulatory requirements. It often isn’t driven with an eye of stepping back and asking fundamental questions about our practices and methodologies on how we could do them better.\n\nHow often do we go to annual HR conferences with an intent of and an eye toward truly learning new skills and applying them to change the way we conduct the business of HR vis-a-vis seeing the latest vendor options that promise to do things for us but require no changes in our skill sets?\nHow often does HR complain about not being taken seriously or being represented at the executive table?\n\nAs much as I believe that HR representation is essential at the executive level, it’s much more likely that representation will be seen as more credible when we actually exhibit leadership- through continuous improvement of our HR practices and methodologies, and enable the whole organization to perform better as a result. Continuous improvement is intentional and proactive. Too many traditional HR practices are reactive, and demonstrate little, if any, innovation.\nTechnology can support innovation in our HR practices. In fact it’s increasingly fundamental to it. Change in our paradigm will require changes in methodologies and supporting tools-technological or otherwise. The biggest obstacle to innovation is often simply being willing to ‘think differently’.\n‘Data-driven’, ‘evidence-based’ HR management and decision making (HR Analytics) helps to mitigate complacency with the status quo. And changing our paradigm of Job Analysis and Descriptions is also very much part of that picture.\nFood for thought …\n\n\n\nAs I mentioned above job analysis and description rely on methodologies and tools regardless of traditional or new paradigms.\nI have been prototyping a ‘proof-of-concept’ web application and database for data-driven job analysis as a hobby. (Part of my software development interests and skills). The prototype is being written using Microsoft’s .Net technologies in C#. If and when I get most of the base functionality working, I might put it out in the public domain under the GPL license on Github.com. The intent would be to illustrate one way of how a new paradigm could actually be operationalized for use in job analysis and descriptions.\nIf I succeed in that, the documentation of that ‘proof of concept’ would be the subject of a possible future blog article."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#introduction",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#introduction",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "If you have read any of my previous blog articles- but particularly the last one- you will get the clear impression that I don’t see HR Analytics as an ‘add-on’ to HR. I see it fundamentally as a means to change the way ‘we do’ HR.\nIf you accept HR Analytics as ‘data-driven HR management and decision making’, then it’s all about making ‘traditional’ HR Management and Decision Making- ‘data-driven’. It means ‘data-driven’ is infused into all of HR in some fashion and at some level.\nThis includes Job Analysis and its end results as well.\nIn this article I therefore wanted to focus on Job Analysis and Job Descriptions concerning ‘data-driven’.\nThe reason for this is because Job Analysis impacts the many other HR practices that are dependent on it. Being data-driven here enables many ‘other’ HR practices to start becoming more data-driven.\nThese HR practices include:\n\nsalary administration\nclassification\nrecruitment /talent acquisition\nlabor relations/ collective bargaining\ntraining\ncareer planning\nhuman resource planning\noccupational health and safety\nand others…\n\nYou can probably regard this article again as an ‘opinion’ piece. Its intent is to get you to ‘think differently’ about job analysis and job descriptions.\nThis blog article will:\n\nRevisit what ‘data-driven’ requires and what this might mean when it is applied to this area of HR.\nTake a brief look at what makes up much of traditional job analysis and its methodologies, and job descriptions content over the last 50 or more years and how those approaches reflect the larger management systems that HR finds itself in.\nEvaluate the degree to which ‘traditional’ job analysis methodologies and job descriptions lend themselves to the requirements of ‘data-driven’.\nExplore in practical terms why we should care about ‘data-driven’ in job analysis\nExplore an alternative way of thinking and methodology for job analysis and descriptions that is ‘data-driven’.\nShow what new possibilities ‘data-driven’ Job Analysis and Descriptions give us in terms of truly transforming the work of HR in the organization."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#revisiting-what-data-driven-means",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#revisiting-what-data-driven-means",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "“Data-Driven” is at the heart of what HR Analytics is all about - ‘Data-driven’ HR management and decision making . We are interested in ‘evidence-based’ management and decision-making regarding people.\nThis means at least three things:\n\ncommitting to making more HR decisions based on objective information and more rigorous analysis.\norganizing ourselves around making ‘that’ objective information more visible and accessible in our HR practices\nkeeping that information in a form that readily lends itself to automation and analysis. This requires the information to:\n\nexist\nbe standardized\nbe structured\nbe quantifiable or categorizable\nbe stored in a database\nallow for the summary, comparison, and contrast readily and quickly across the entire set of data.\nallow for appropriate statistical analyses and reporting to answer questions posed of that data."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#revisiting-traditional-job-analysis-and-job-description-approaches",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#revisiting-traditional-job-analysis-and-job-description-approaches",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "To answer this question, it is always helpful to look at a variety of definitions from a variety of different sources, to get a sense of the commonalities that emerge.\nA quick and cursory Google search turned up many definitions.\nJob Analysis Definition Google Search\nOne of them that came up in my Google search was from Wikipedia:\nWikipedia\nhttps://en.wikipedia.org/wiki/Job_analysis\n“Job analysis (also known as work analysis[1]) is a family of procedures to identify the content of a job in terms of activities involved and attributes or job requirements needed to perform the activities.\nJob analysis provides information of organizations which helps to determine which employees are best fit for specific jobs. Through job analysis, the analyst needs to understand what the important tasks of the job are, how they are carried out, and the necessary human qualities needed to complete the job successfully.\nThe process of job analysis involves the analyst describing the duties of the incumbent, then the nature and conditions of work, and finally some basic qualifications. After this, the job analyst has completed a form called a job psychograph, which displays the mental requirements of the job.[2] The measure of a sound job analysis is a valid task list. This list contains the functional or duty areas of a position, the related tasks, and the basic training recommendations. Subject matter experts (incumbents) and supervisors for the position being analyzed need to validate this final list in order to validate the job analysis.[3]”\nI wont profile all the google hits above in this blog article, but if you look at many of them they are identical or very similar- and have many commonalities between them.\nWhat are some of these commonalities?\nThey:\n\nstudy the job to identify its content which can include:\n\nactivities\nresponsibilities\nqualifications\nworking conditions\ntasks\nduties\nknowledge\nskills\nabilities\nimportance of tasks\nhuman qualities\nhow often tasks are performed\n\nhave multiple purposes in mind for the usage of the gathered information.\nhave a job description often as the most immediate end result. Typically this written narrative describes tasks, duties and responsibilities of a position. This relationship between job analysis and job description is described as well in Wikipedia:\n\n\n-   [Wikipedia definition of a job description](https://en.wikipedia.org/wiki/Job_description)\n\n    -   *\"According to Torrington, a job description is usually developed by conducting a [job analysis](https://en.wikipedia.org/wiki/Job_analysis \"Job analysis\"), which includes examining the tasks and sequences of tasks necessary to perform the job. The analysis considers the areas of [knowledge](https://en.wikipedia.org/wiki/Knowledge \"Knowledge\"), [skills](https://en.wikipedia.org/wiki/Skills \"Skills\"), and [abilities](https://en.wiktionary.org/wiki/abilities \"wikt:abilities\") needed to perform the job. Job analysis generally involves the following steps: collecting and recording job information; checking the job information for accuracy; writing job descriptions based on the information; using the information to determine what skills, abilities, and knowledge are required to perform the job; updating the information from time to time.\"*\n\n-   Historically-once produced- these job descriptions would be stored in filing cabinets for retrieval when needed.\n    In the 1980s and later these would be 'Word' documents stored on a network file server somewhere.\n    Some organizations took it a step further and put them in HTML format on web server.\n    Regardless though, these job descriptions are still 'narrative documents' in whatever technology form they're in.\nThe above methods and formats have been in the literature, textbooks and professional HR education for 40-50 years with very little change. Two key questions at this point might be:\n\nwhy is this the past and current paradigm for job analysis?\nDoes this paradigm lend itself to being data-driven?\n\nWe will explore the first of these questions in this section immediately below and the second in the next.\nWhy is this the past and current paradigm for job analysis?\nTo answer this, we have to step back a bit and realize that the job and/or the position is and has almost always been the focal starting point of all job analysis.\nFrom above:\n“Job analysis (also known as work analysis[1]) is a family of procedures to identify the content of a job in terms of activities involved and attributes or job requirements needed to perform the activities.\nJob analysis provides information of organizations which helps to determine which employees are best fit for specific jobs. Through job analysis, the analyst needs to understand what the important tasks of the job are, how they are carried out, and the necessary human qualities needed to complete the job successfully.”\nNotice that the focus is the JOB .\nIn this paradigm, if you wanted to understand the organization and document its work, you study and document the jobs -one by one. The assumption is that the ’work of the entire organization is really seen as the aggregate of all of these together. You start at the individual level and you ‘aggregate’ to get the entire picture of an organization’s work. This is a ‘proxy’ for understanding the business that the organization is in. The assumption here is aggregating from individual jobs and positions will synthesize into an overall ‘coherent’ picture of the work of the organization.\nThis ‘job by job’ analysis paradigm is not independent of the other organization management paradigms they are within. To map out and to know when you have the work of the entire organization covered, you traditionally have depended on organization charts. When you have covered every position/job in the organization chart you supposedly have the work of the organization documented.\nThat got me doing a little detective work on how organization charts originated and why they are so pervasive. This would be a whole other topic in itself which I won’t go into detail here. But that minor detective work led me to the following book:\nThe Leaders Handbook -Peter Scholtes\nIn that book, Peter indicates that organization charts came from ‘train-wreck’ charts (an apt metaphor? ;) tongue in cheek). The intent was to prevent train wrecks that occurred in the 1800s from happening in the future. These ‘train-wreck’ charts among other things illustrated:\n\nchains of commands\nfunctional divisions\nclear descriptions of responsibility\n\nIn his book, Peter goes through a chronological analysis of how the above eventually resulted in Management By Objectives (MBO) as a system of management.\nOut of those larger management systems such as MBO would have likely come the HR methodologies for job analysis. (i.e. clear descriptions of responsibility sound an awful lot like job descriptions to me)\nThe point I am raising is that our HR practices didn’t emerge in a vacuum. They were responses intended to be consistent with the larger management systems they found themselves within.\nThese larger management systems aren’t ‘without’ impact or consequences.\nIn MBO, the consequence is that if you want to ‘understand’ a business, you look at an organization chart. You look at the chain of commands, functional divisions, and descriptions of responsibility. For what it’s worth, personally I never found that to be true. Looking at the above helped me understand how the business was organized or structured - but that isn’t the same as understanding the business.\nIn HR, the impact was ‘job descriptions’ and a methodology for job analysis that was ‘one by one’ and with the assumption that aggregating from the ‘bottom up’, you would then have a picture of the work of the organization and the business it was in. The ‘one by one’ ultimately weaves itself into traditional performance appraisal systems. We evaluate the performance of employees ‘one by one’. MBO requires it, and traditional job analysis and descriptions support it -‘one by one’.\nThe ‘bottom up’, ‘one by one’ paradigm rarely leads to a coherent picture of the work of the organization and the business it is in- more often than not it is an incoherent, fractured picture.\nLet me illustrate why.\nPicture Puzzle Analogy\nMost of you are familiar with picture puzzles (a picture that has been carved up into several hundred pieces). You probably grew up with them as kids. The reason why these puzzles work is that they start with a ‘whole picture’ and carve it up into pieces. When you put those pieces together they from the whole original picture.\nNow imagine instead that you have half a dozen people independently who are attempting to create puzzle pieces from scratch. Even if they know that the end result is ‘a picture’ (hopefully a coherent business purpose), their differences (as human beings) will result in a variety of puzzle pieces which are unlikely to fit together well or at all.\nWhen you start with the pieces, rather than the entire picture, those pieces will find it almost impossible to form an entirely coherent picture. There is very little commonality between those pieces.\nI would suggest that traditional job analysis methods are a lot like that. You may have all the pieces, but you have very little way of linking them together- finding pieces that fit and join together. We are so focused on the ‘job’ as the entity under study that we cant see the larger picture under this paradigm. (It’s also noteworthy in the above wikipedia definition it said ‘also known as work analysis’. I think we have forgotten that.)\nWe need a paradigm with job analysis methodologies and end results (job descriptions) that start with the overall picture and deliver puzzle pieces that stand on their own - AND still fit back together as a unified whole.\nBeing ‘data-driven’ in our job analysis methodologies and job description results is a potential enabler in this direction."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#evaluating-the-degree-to-which-traditional-job-analysis-and-descriptions-meet-the-criteria-of-data-driven",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#evaluating-the-degree-to-which-traditional-job-analysis-and-descriptions-meet-the-criteria-of-data-driven",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "Let’s look at the characteristics of ‘data-driven’ and evaluate the degree to which traditional job analyses and descriptions meet these requirements.\n\n\nLoosely speaking- even traditional job analyses and descriptions meet this criterion. Traditional job analyses are a manual process for ‘generating’ information about the contents of a job. And as the methods mentioned above indicate- there are quite a few ways of generating that information. PASS\n\n\n\nLoosely speaking we could say that this requirement is also met (at least in part). Our study of jobs through job analysis focuses on standardizing the content. This is reflected above in what the typical content areas are:\n\nactivities\nresponsibilities\nqualifications\nworking conditions\ntasks\nduties\nknowledge\nskills\nabilities\nimportance of tasks\nhuman qualities\nhow often tasks are performed\n\nIf you cover some or all of these ‘consistently’ in every job analysis and resulting description you are ‘standardizing’ your ‘approach’ to the content. (but maybe not the expression of that content)\nAt least at a ‘narrative’ level then most organizations achieve standardization through guidelines and procedures. Undoubtedly though, some variation will occur in the standardization due to different people with different writing styles. PASS\n\n\n\nThis means:\n\nnot only are the contents standardized but they also\nare expressed in a way that we can easily see and find the equivalent information from job to job.\n\nThis will be a PASS most of the time if the narrative descriptions are standardized and well written. But in my opinion -a bare pass because there can be a lot of subjectivity and judgment around the equivalence around the job content and characteristics.\nThose of you who have done job classification by comparing job descriptions to ‘narrative’ job class specifications could probably attest to the fact that even when policies, procedures, and guidelines are in place for writing job descriptions- the ease of slotting it into the most appropriate job class is not easy. It is definitely partly a function of how well the job description was written in terms of structure and standardization and the job class specifications as well.\nIn other words, even with the best of intentions and guidelines for writing, the use of traditional narrative job descriptions can be very difficult.\n\n\n\nBecause most traditional job analyses and descriptions end up being narrative documents derived from narrative information gathering, this requirement is typically not met. At best there may be some quantification of time spent (i.e. proportion of time spent) on major responsibilities. But for such things as skills, knowledge, and abilities required, these may be vague statements of whether there is more of something or less of something required, and these are usually ‘narrative’ -NOT quantified.\nEven if minimum information is quantified, if the structuring of job information or the standardization of it is weak, quantification in these circumstances doesn’t help us much. We really don’t know if X% time spent on Y responsibility is really comparable to Z% time on the same responsibility in another job.\nWithout strict attention to standardization- we may or may not be comparing the same responsibility. True standardization means that we capture the same types of common data on EVERY JOB. Narrative job descriptions (unless heavily standardized, structured, and written with an eye to eventual quantification of that information) rarely meet this criteria. FAIL\n\n\n\nRight from the ‘get-go’ this requirement in traditional job analysis and descriptions is rarely, if ever, met. Combing and scouring narrative sources of written information, verbal content from interviewing Subject Matter Experts (SMEs), etc. is not in a format needed for storage in a database. Automating job descriptions to the point of being either Word documents on a file server or HTML documents on a web server for easy access isn’t ‘being stored in a database’.\nFor job analysis information and resulting descriptions to be stored in databases:\n\nEach job must be stored as a single entity (often a record) in the database. (often somewhat comparable to thinking of a row in Excel)\nAll the information used to describe that job and its characteristics must be represented by data elements or fields for that record or sub-records. (often comparable to thinking columns in Excel for a single row)\nThe information must be structured and standardized in the database. (In Excel, all columns appear in every row)\nIf information is to be used to compare one job to another, all information to be used for comparison must be categorized into standardized qualitative information if textual (ie non-numeric) or quantified into measures if numeric. The reason for this is to have common denominators on which to compare and contrast different jobs.\nThe database must hold the information on all documented jobs.\n\nThe verdict on ‘traditional’ job analysis and descriptions: FAIL\nNothing is preventing an organization from thinking non-traditionally here. The idea of storing job information in a structured, quantifiable, measurable way is NOT NEW. I am not the originator of this idea. Early attempts to use the technology to do this are at least 30 years old if not older. Even 30 years ago, it gave me some sense of the future possibilities and how things could change if we wanted them to.\nSo why no huge uptake?\nA lot of the answer to that goes back to the difference that HR Analytics requires. ‘Data-driven’ HR Management and Decision Making absolutely requires structured and standardized information and measurement. Traditional HR doesn’t. It’s why the literature and methods around this part of HR haven’t changed much for 50 years. I’m not sure that’s a good thing.\n\n\n\nThis too is an immediate FAIL on the criteria of ‘data-driven’.\nWord documents on a file server or HTML documents on a web server do not allow for any quick comparison or contrast across all jobs. At best Word documents would be a manual effort of perusal, and HTML documents would offer keyword search capabilities. But there is no ability to quantitatively compare information across all jobs.\nSo, on balance, how do traditional job analysis and job descriptions stack up against ‘Data-Driven’ HR?\nPoorly."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#why-should-we-be-concerned-with-data-driven-in-job-analysis-and-job-descriptions-why-is-this-important",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#why-should-we-be-concerned-with-data-driven-in-job-analysis-and-job-descriptions-why-is-this-important",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "I think the answer to this comes down to a few major reasons.\n\nTraditional job analysis and descriptions and their methodologies make the process of doing this far more manual than it needs to be and far longer than it needs to be.\nSome things in HR are way more difficult to do or do efficiently and well, or do at all -with traditional job analysis and descriptions.\nAs mentioned earlier there are many uses made of job information in HR, so by making improvements here, we improve many areas of HR.\n\nLet’s look at each of these reasons in turn.\n\n\nThose of you whose job is job analysis can probably attest to this. Traditional Job Analysis and Job Descriptions are a laborious, manual, and slow processes- whether its in the creation of these or usage of them when job descriptions are done.\nFor creation:\n\na request for a new job is created.\nyou will likely be required to interview the person to who this job will report to.\nyou will likely need to interview SMEs if there are any\nif it’s an existing job that is changing- you will likely have to interview an incumbent\nyou will likely have to review your organizational standards for job analyses and descriptions\nyou will likely have to review other jobs to understand how this is similar or different from them\nyou will prepare new job description from all your analysis\n\nBest guess-a few days to organize all of this into an end result.\nFor HR usage:\nLet’s use job classification (using narrative job class specs) as an example:\n\nyou will need to read the job description in question- getting some sense of its content.\nif you are a season classification officer, you will form an initial impression of the job family it falls into.\nyou will then need to review at least a few job classification specs, and perhaps many other comparable positions/jobs to ballpark your decision on classification.\nyou meet with the requester and present findings and hopefully justify the decision made to their satisfaction.\n\nBest guess-a few days to organize all of this into a completed decision.\nWhat if being ‘data-driven’ in our approach resulted in job analysis and descriptions taking no more than a couple of hours? What if upon obtaining the information- classification took only minutes?\nDo we in HR actually care about increasing the efficiency of our methods, procedures, and techniques and resulting customer response time?\n\n\n\nThere are at least a few examples here that come to mind.\nConsider the following:\n\n\n\n\nStrategic planning, career planning, and succession planning are all dependent on good job information and to be able to use that information in a specific way. The common denominator in all these planning efforts is the the successful ‘defining and bridging of gaps’.\n\nFor strategic planning it’s defining the gap between who, where, and what you are now as a business and the desired future for your business- desired by you as an organization and by your customers - and bridging that gap with plans and activities. ( strategic planning is really career planning at a ‘business entity’ level).\nFor career planning, it’s defining the gap between who you are and where you are right now and what you desire to be in the future and then bridging that gap with plans and activities. (career planning is really strategic planning at an ‘individual’ level)\nFor succession planning, it’s defining the gap between job requirements needed for the future, and succession candidates current capabilities and coming up with developmental plans and activities to reduce or bridge that gap.\n\n\nDoing any/all of these, at some point, requires us to bring this down to a job content, job analysis, job description level. To truly get down to the nuts and bolts of defining gaps- you have to define differences. To define differences, you must be expressing things in a standardized way that allows you to say for instance:\n\nthese items belong to job 1 and only job 1\nthese items belong to job 2 and only job 2\nthese items belong to both job 1 and job 2\nto get from job 1 to job 2, the person must keep all things common to both and learn items in job 2\n\nThe same goes for strategic planning:\n\nThese activities/products/services are current and only current\nThese activities/products/services are what we desire for future and are only future\nSome activities/products/services will be both current and continue into future\nTo get from current to future successfully we will need to drop some activities /products /services, understand that some will continue to need to be maintained, and some new initiatives will need to be introduced and plan for all of these.\n\nEven strategic planning eventually has to be expressed in terms of how it affects jobs, their contents, and differences.\nIf your job analysis methodologies and job descriptions don’t quantify, standardize, structure job information and stored in a database - allowing for the summary, comparison, and contrast readily and quickly across the entire set of data- the quality of the above planning processes and their implementation may be limited.\nAre your existing traditional job analysis methods and traditional job description format and contents capable of this and up to the challenge? ‘Data-driven’ in this area of HR is likely to be an enabler for this.\n\n\n\nIf you wanted to extend the strategic planning example:\nAt some point in strategic planning, as per the above, you should;\n\nidentify the products and services you are no longer engaging in\nidentify the products and services that you are going to continue to engage in\nidentify the new initiatives, products, and services that will come on stream\n\nEach of these steps requires understanding how this will affect your current jobs and the employees that are in them.\nDo we know how many FTEs will be saved by those products and services no longer engaged in and exactly where?\nDo we know how many FTEs will need to continue to support continued products and services and exactly where?\nDo we know how many FTEs will be needed to properly support new initiatives and exactly where?\nWithout good, robust, and data-driven job analysis methodologies and job description content and storage it’s very difficult for HR to answer these questions and to do it promptly.\n\n\n\nThe primary goal of analytics, and for our purposes HR analytics- is to improve performance in the organization whether it be through\n\nHR metrics- so that the organization knows what is happening to its human resources over time and can be more proactive\nHR Services- being much more aware of the level and quality of services we provide to our organizational customers and making improvements as a result of that\nHR methodologies- efficiency and effectiveness- to do what we do better and quicker (without sacrificing better)\n\n\nJob content is interwoven and related to all of these.\nIn short- THIS IS WHY WE SHOULD CARE ABOUT DATA-DRIVEN JOB ANALYSIS AND DESCRIPTIONS."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#what-is-an-alternative-way-of-thinking-about-job-analysis-and-job-descriptions",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#what-is-an-alternative-way-of-thinking-about-job-analysis-and-job-descriptions",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "The short answer is to change our paradigm and its associated methods, increase our use of technology, and quantify our job information.\nHere are some practical ways to change our ‘50-year-old’ thinking in this area and move on from historical and current approaches.\n\nOur job analysis methods need to start with the entire picture and work back from there.\n\nThe concentration/focus is not on ‘job’ analysis but ‘work’ analysis.\nJobs just happen to be the way we segment out that work into understandable and manageable chunks for a position or person.\nany one job is just a subset or snapshot of the much larger picture of the work of the organization.\nAs a subset of a larger whole- differences and similarities are more readily discernible\n\nUnderstand that starting point of understanding the entire picture of the work of the organization can’t be the organization chart (aka train wreck chart). An organization chart tells you nothing about the work of the organization, only how you are structured. The starting point is:\n\nConfirm what business the organization is in.\nThen identify and enumerate all of the products and services the organization produces/performs in being in that business\nThen the identification and enumeration of the business processes that produce those products and services\nThen identification and enumeration of the tasks, knowledge, skills, qualifications necessary to perform those business processes.\nFinally determine how to apportion those tasks, knowledge, skills, and qualifications into individual jobs and positions. A ‘job’ description then is the documentation of assigning tasks and knowledges within the business processes that lead to the provision of products and services to those individual jobs and positions. That way you can go both directions (up and down) in job analysis/work analysis for the organization. You ‘could’ represent these via an organization chart- but you would probably find it equally and possibly more useful to represent your organization via business process charts and identify (in some manner) how the positions are integrated into them. Each job is a ‘subset’ of those tasks and knowledges.\n\nUnderstand what measurement means in relation to job information and job analyses and realize much is measurable and quantifying that job information. This includes:\n\nunderstanding the presence or absence of tasks, knowledge, and skills, qualifications in any ‘one’ job out of the total picture the tasks knowledge, skills and qualifications needed is a form of measurement\nunderstanding that capturing of ‘levels’ of knowledge, skills, and qualifications on some sort of scale is in fact -measurement.\nunderstanding that time spent on various tasks is measurement\n\nUnderstand that when you standardize the job information and quantify it, you are enabling that information to be more readily stored in a database.\nUnderstand that when job information is structured and standardized in a database, this is what enables much more extensive and robust use of that information.\nUnderstand that when job information is structured, standardized, and quantified in a database that web applications can be developed to serve out much of the functionality expected from the traditional approaches but also ‘much more’. It’s actually a two way street here- if we change to a new paradigm for job analysis- the tools supporting it must change. Traditional job analysis fundamentally requires job descriptions, paper documents, word docs on a file server, or HTML docs on a web server. Data-driven requires databases and computer applications to manage, house, process an analyze the job information."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#what-are-new-possibilities-when-we-become-more-data-driven-in-job-analysis-and-job-descriptions",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#what-are-new-possibilities-when-we-become-more-data-driven-in-job-analysis-and-job-descriptions",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "Job Descriptions that are dynamically kept up to date. Storage in a database means they are as current as the last time they were update and from a central source\nJob Descriptions that are often available in a variety of formats- HTML, Word, PDF etc\nJob Descriptions that are keyword searchable\nJob Descriptions that can be created from online wizards that guide the process or printed questionnaires to select job content from.\nSecurity separation from those who are consumers of job information from those responsible for creating the job content in web applications.\n\n\n\n\n\nBetter organizational HR planning processes - through detailed defining gaps/differences/ similarities between positions for planning purposes including career planning and succession planning\nBetter organizational strategic planning processes - by helping the organization understand the effect on existing FTEs of dropping, maintaining, and adding new product and service expectations will have on the organization.\nMuch more robust HR Management and Decision-Making processes - by putting job information more in the form necessary to be able to be used in machine learning and artificial intelligence to complement the current strengths of this in the organization.\nBetter HR and organization performance through recognizing that organizing job information around products/services and their corresponding business process provides a paradigm for naturally gravitating the organization in the direction of ongoing continuous improvement."
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#conclusion",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#conclusion",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "As mentioned earlier, my intent in this blog article is to stimulate ‘thinking differently’ as to how we understand this area of HR and give some thought as to whether we should operationalize it differently in our HR practices. Are we motivated as HR professionals to do that?\nI have often had the thought that many of us might unintentionally be doing a ‘Rip Van Winkle’ when it comes to some of our HR practices and methodologies. We could go to sleep for 50 years, wake up, and still do HR work with our skill sets because so little changes.\nThe changes that ‘do occur’ are often externally imposed on the organization and HR profession in the form of meeting changes in legislative regulatory requirements. It often isn’t driven with an eye of stepping back and asking fundamental questions about our practices and methodologies on how we could do them better.\n\nHow often do we go to annual HR conferences with an intent of and an eye toward truly learning new skills and applying them to change the way we conduct the business of HR vis-a-vis seeing the latest vendor options that promise to do things for us but require no changes in our skill sets?\nHow often does HR complain about not being taken seriously or being represented at the executive table?\n\nAs much as I believe that HR representation is essential at the executive level, it’s much more likely that representation will be seen as more credible when we actually exhibit leadership- through continuous improvement of our HR practices and methodologies, and enable the whole organization to perform better as a result. Continuous improvement is intentional and proactive. Too many traditional HR practices are reactive, and demonstrate little, if any, innovation.\nTechnology can support innovation in our HR practices. In fact it’s increasingly fundamental to it. Change in our paradigm will require changes in methodologies and supporting tools-technological or otherwise. The biggest obstacle to innovation is often simply being willing to ‘think differently’.\n‘Data-driven’, ‘evidence-based’ HR management and decision making (HR Analytics) helps to mitigate complacency with the status quo. And changing our paradigm of Job Analysis and Descriptions is also very much part of that picture.\nFood for thought …"
  },
  {
    "objectID": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#quick-addendum--for-anyone-interested",
    "href": "posts/DDJobAnalaysis/DDJobAnalysisDescription.html#quick-addendum--for-anyone-interested",
    "title": "Data-Driven’ Job Analysis And Job Descriptions",
    "section": "",
    "text": "As I mentioned above job analysis and description rely on methodologies and tools regardless of traditional or new paradigms.\nI have been prototyping a ‘proof-of-concept’ web application and database for data-driven job analysis as a hobby. (Part of my software development interests and skills). The prototype is being written using Microsoft’s .Net technologies in C#. If and when I get most of the base functionality working, I might put it out in the public domain under the GPL license on Github.com. The intent would be to illustrate one way of how a new paradigm could actually be operationalized for use in job analysis and descriptions.\nIf I succeed in that, the documentation of that ‘proof of concept’ would be the subject of a possible future blog article."
  },
  {
    "objectID": "posts/PowerBIandR/PowerBIAndR.html",
    "href": "posts/PowerBIandR/PowerBIAndR.html",
    "title": "Using Microsoft Power BI and R in People Analytics – A Quick Example",
    "section": "",
    "text": "Introduction\nOne of things I try to do in the field of People Analytics is monitor and try out new tools and technologies that come out that appear to add value to People Analytics.\nIn doing just that, I came across a very cool and nifty unexpected behavior in one tool- Microsoft Power BI.\nAlthough, many of my previous blogs have been lengthy (by necessity), this one will be quite a bit shorter because my real intent is to show a gem of unexpected behavior I came across because of using the above tool on an HR dataset. While this unexpected behavior is just one example, if it suggests the future of where analytic tool design is heading, it is quite exciting.\nIf you have been keeping up with what Microsoft is doing in the Data Science/Analytics space, you will know they are doing plenty. And that “plenty” revolves around integrating R into much of their technology.\nOne of the most recent things I came across was some of their literature and examples of how R is being integrated into their Microsoft Power BI Graphics exploration tool. One of the things you can do in Power BI Desktop is import in “custom visuals’. And it turns out that someone has developed and provided some”R” related custom visuals covering:\n\n        Splines\n        Association Rules\n        Clustering K Means\n        Forecasting\n        Decision Trees\n        Correlation Plots\n\n·        Etc.\nThese visuals seem to use R “interactively’ in the background as you manipulate choices in the tool in the foreground. This means that the tool goes beyond just traditional ‘slice and dice’ tools- which just give you different cuts of data. The”Beyond” is that it shows you the “results” of your choices-whether the choices you are making are improving things or not.\nHuh? What?\nOk maybe the best way to explain is to show by one actual example.\n\n\nThe Example\nIf you have read some of my previous blogs, you will have come across this one from May 1, 2016 on job classification and R\nhttps://www.linkedin.com/pulse/people-analytics-r-job-classification-revisited-sundmark-mba?trk=prof-post\nIn it, I demonstrated how we could use several machine learning algorithms in R to try to predict a job classification based on feature information found in traditional job class specifications.\nThe following link provides the location of the csv dataset used\nhttps://1drv.ms/u/s!AvXQmNu-zPJOsij-zLqDMEkc9qWO\nOne of machine learning algorithms used in that article was decision trees:\nhttps://1drv.ms/b/s!AvXQmNu-zPJOsjEoCQF_edVGbqOL\n \nIn the way, the above graphic was produced, the algorithm ‘rpart’ was run first and then the graphic was produced from that data.\nYou can tweak the settings for ‘rpart’ and rerun the decision tree graphic and get different results from different settings. But the order is\n1.tweak in code\n2.run the code\n3.produce the graphics\nAnd repeat as necessary\n \n \nHow Does Power BI do it?\nThe order for producing a decision tree:\n1.select the Power BI graphic\n2. tweak the graphic\n3. look at changes in graphic and the statistical results (yes- the R decision tree algorithm is run for you behind the scenes- each time you tweak the graphic\n\n\nLet’s See This in Action\n1.Grab the csv file from the link provided previously and store locally.\n2. if you haven’t already done so download Power BI Desktop and install it:\n \nhttps://powerbi.microsoft.com/en-us/documentation/powerbi-desktop-get-the-desktop/\n \n3. Install the R custom visuals for Power BI Desktop- the one of interest for this example is the ‘decision tree’\n \nhttps://app.powerbi.com/visuals/\n\nPlease note that R must be installed for these to work- the installation will tell you that.\n \n4. So now you should have R installed, Power BI Desktop installed, the R powered visuals installed. The next step is to launch Power BI desktop.\n \n5.Get Data\n\n6.Import\n\n\n7. Load the data\n\n8. Now import in the R decision tree custom visual previously downloaded\n\n\n9. Click the new decision tree icon and enable script visuals10. Now click the check boxes on the right, putting PG as target variable and problem solving and supervision as input variables. A decision tree diagram is produced.\n\nThe relative error is .54 with those choices\n11. Now add other input variables by clicking more check boxes\n\nThe relative error has dropped to .21.\nWhat has just happened?\nAs you made choices for extra input variable, the decision tree custom R-powered visual has rerun the algorithm behind the scenes for you- showing that you have improved your model. The decision tree shows (based on the data) what rules lead to what prediction of pay grade, and the error associated with those prediction.\nThis was done with no coding by you.\n\n\nClosing Thoughts\nThe whole point of providing this example and in a hands-on way, was to illustrate several things:\n\nPeople Analytics need not be scary\nYou can use some tools readily available out there to show your organizations live examples of what’s possible in People/HR Analytics\nBeing able to interact with your graphics and visuals as the means to carry out algorithms in the background is very powerful and promising\n\nThis is intended to demonstrate just one example and one tool. None of the above precludes or dismisses the need for good statistical analysis, R, HR, coding, and data science skills for much more robust, proper and comprehensive People / HR analytics. But tools such as this can be very useful as part of what can be brought to bear for organizations on their analytics journey."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "",
    "text": "In my last blog article, I showed an example of how ‘data driven’ analytical approaches could be applied to traditional HR methodologies, by looking at applying statistical discriminant analysis to a traditional non quantitative approach to job classification.\nMy intent in that article, this one, and a few more to come is to try to show HR in a series of small vignettes that ‘data driven’ is indeed possible for HR. It is applicable to:\n\nHR metrics reflecting people activity within our organizations\nHR metrics reflecting HR operations\nHR metrics reflecting deep embedding directly into our HR methodologies.\n\nThe last article and this one are intended to show examples of the last bullet point. (I still intend to write some blog articles on the first two as well). But the third bullet point demanded at least one more blog article because there are so many methods in statistics that focus on ‘prediction’ of classifications and categories.\nIn addition to discriminant analysis in my last article, I mentioned that another method was ‘decision trees’. Still another is random forests. And there are still more beyond that. What I would like to do in this article is show examples of decision trees and random forests. As a continuation of the theme from the last blog article, I will first show these in the statistical package R.\nHowever I will in this article also show these two methods in software packages that don’t require ‘programming in code’ in R, but rather show a couple of more ‘menu driven’ tools that are out there to do this. So the outline of the remainder of this blog article will be demonstration of data driven job classification through:\n\nDecision trees in R\nRandom forests in R\nDecision trees in a web based menu driven solution on the internet called BigML\nRandom forests in Microsoft Azure Machine Learning\n\nIn demonstrating these, my intent again is NOT to provide a full manual or explanation or dissertation on the above. Each of the statistical methods and tools could be articles in and of themselves. The intent is to whet your appetite for ‘data driven’- so that you do further reading and research and study in this area and start applying these approaches to HR where your imagination takes you. I will only share as much detail as is necessary to get the ideas across."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#decision-trees",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#decision-trees",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision tree algorithms attempt to figure out a systematic set of ‘rules’ based on patterns it sees in the a data population which can then be used to relatively consistently correctly categorize items that are not yet in that data population. The decision tree looks like an upside down tree- where the top is a root node, and for which there are subsequent ‘branch’ nodes that are further split by rules the algorithm has found in the data. And finally there are terminal nodes- points at which no further splits occur and a category has been predicted with a reasonable level of certainty. (There are books that define decision trees more formally – but I am trying to keep the description as simple as possible).\nIn any case, each node in the decision tree is a question, for which there are ‘splits’ with respect to the answers- something either being equal to something or less than or greater than something. This then leads to another ‘question’ or ‘rule’ which in turn has splits for answers. By following the questions (rules) and answering them based on the features in the observation- we eventually get to a prediction of a category- based on the features and categories of the ‘known’ population.\nSimilar to discriminant analysis- a ‘known’ population with ‘known’ categories and ‘known’ features is used as a base by the algorithms. When the analysis is done by the algorithm, the intent is to take new observations or records that are NOT in that original population and attempt to predict their category based on the features they evidence.\nAND- similar to discriminant analysis- ‘the proof is in the pudding’. We judge the usefulness of the ‘data mining’ tool based on how ‘well’ it predicts. I mentioned in the last blog article, that we can apply these algorithms on the original data ‘initially’ to see how well the model predicts what it already knows outside of itself- ( i.e. the training data). But in general, training data is often overly optimistic, and so we often hold out some ‘known’ data from the training data as ‘test’ data, to address ‘too optimistic’ prediction and over fitting of the data. Even though you are aware of this if you read my last article, I will use the training data later simply for illustration.\nClear so far?"
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#random-forests",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#random-forests",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Random Forests",
    "text": "Random Forests\nOk- so what are random forests as compared to decision trees and why do we need them?\nAccording to Wikipedia:\nhttp://en.wikipedia.org/wiki/Random_forest\nRandom forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees’ habit of over fitting to their training set.\nWe had mentioned earlier, that ‘over fitting’ of decision trees can be an issue. What random forests are is a process of creating hundreds of individual trees at a time often using a ‘subset’ of the features for each tree constructed. The end result is still a prediction of a category. The difference is that you don’t have a single ‘tree’ you can look at and refer to and explain to someone else.\nThe bottom line for decision trees and random forests is that both of these attempt to predict a category of an item not in the existing population of known data, but based on what it sees as the ‘patterns’ in the data of the known population. And similar to discriminant analysis a confusion matrix (showing predicted versus actual- and proportions correct and incorrect) is a common denominator for comparison in usability in all of these tools- as we shall see in the examples below.\nHopefully the above gives a generally idea of the concepts and the purposes of these ‘data driven’ classification tools. The next sections will show examples of these in code using R, and then with more menu driven approaches.\nLet’s see some examples of these with various tools."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#data-source",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#data-source",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Data Source",
    "text": "Data Source\nIn this step you upload your data file to BigML.\n\nI have uploaded the CSV file"
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#dataset",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#dataset",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Dataset",
    "text": "Dataset\nOnce the file is uploaded BigML can see its metadata and understand the fields. It assumes the last field is the thing you are trying to predict. In mine, the second field is what I am trying to predict so I tell it that. Notice I have the 7 predictor fields- education level through to financial budget."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#model",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#model",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Model",
    "text": "Model\nThe next step is to create the model. This is where you see the decision tree created. I have highlighted one of the paths:\nIt shows you the node rules to the right- for that path."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#predict",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#predict",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "Predict",
    "text": "Predict\nThe final step is to predict. It gives you a couple of options- but I will show just one:\nAt the top is the prediction. Below it are the settings for each of the predictors. What is supercool here is that you can vary the values on those predictors and the predicted category will change- in real time. Why is that cool? The predicted category only changes when the combination of values requires it to. One of the things I found for example was that the predictors can have a level of sensitivity/insensitivity. In other words problem solving could have a series of values that could be changed- for which there was no change in the predicted category (all other predictors held constant). This BigML web based graphical tool allows you to see that sensitivity- which might be difficult to do in other tools.\nFinally with respect to the model- you can download a version of it to your computer in your favorite language. Examples include C#, VB Net, Excel and a number of others.\nOther than taking a few minutes to familiarize myself with the BigML interface- once I did that it took me less than 10 minutes to have a working model that could predict- no code required."
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#the-jobclass-data",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#the-jobclass-data",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "The Jobclass Data",
    "text": "The Jobclass Data"
  },
  {
    "objectID": "posts/ddjobclasspt2/ddjobclasspt2.html#the-evaluated-results",
    "href": "posts/ddjobclasspt2/ddjobclasspt2.html#the-evaluated-results",
    "title": "Data-Driven’ Job Classification Part 2",
    "section": "The Evaluated Results",
    "text": "The Evaluated Results\n\nWhen you are ready to publish your model ( make it visible to yourself and others as a usable outcome, the Azure Machine learning tool provides you with an Excel spreadsheet where you can put in your values:\nIt then shows you the predicted class:\n\nIn the above, I actually haven’t entered any values.\nI should also mention that even though Microsoft’s Azure Machine learning tool is cloud, web, and visually based- you can actually also include R scripts in the models you build. (There is a reason why R is popular as a statistical package).\nWhile I have not shown anything from it, yet another graphic tool that can be used for data mining is RapidMiner found at\nwww.rapidminer.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HRAnalyticsBlogs",
    "section": "",
    "text": "People/HR Analytics - What Is your Paradigm ? It Matters\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nAre You An ‘HR’ Innovator or Caretaker or Undertaker?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2021\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nData-Driven’ Job Analysis And Job Descriptions\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\nLyndon Sundmark\n\n\n\n\n\n\n  \n\n\n\n\nHR Analytics Is A ‘Way of Doing HR’- Not An ‘Add-On’\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2020\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nThe Challenge for HR Analytics -2020 And Beyond\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2020\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nAutomated Machine Learning –Does This Benefit HR Analytics?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2019\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nIs It Time To Revisit The Scope and Breadth of People / HR Analytics?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2018\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nA Good Article On the R ‘Rattle’ GUI\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2018\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nUsing Microsoft R Server and Other Microsoft Technologies to Embed HR Analytics Into HR Practices- An Example\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2018\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nDoing HR Analytics - A Practitioner’s Handbook With R Examples -paperback now available\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2017\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nAnnouncing “Doing HR Analytics – A Practitioner’s Handbook With R Examples\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2017\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nWhy Kaggle Might Be Important To People Analytics\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2017\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nUsing Microsoft Power BI and R in People Analytics – A Quick Example\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2017\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nHow Do I Start Preparing Myself for the Field of People/HR Analytics?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2016\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nPeople Analytics in R - Job Classification ‘Revisited’\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nPeople Analytics Using R - Employee Churn- An Example\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2016\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nPeople Analytics - An Example Using R\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2016\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nHR Analytics – What Stopping Us And Where Do Go From Here?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2015\n\n\nLyndon Sundmark\n\n\n\n\n\n\n  \n\n\n\n\nData-Driven’ HR Operations\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2015\n\n\nLyndon Sundmark\n\n\n\n\n\n\n  \n\n\n\n\nData-Driven’ Job Classification Part 2\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2015\n\n\nLyndon Sundmark\n\n\n\n\n\n\n  \n\n\n\n\nData Driven Job Classification\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2015\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nWHY HR MIGHT NOT BE ABLE TO REINVENT ITSELF\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2015\n\n\nLyndon Sundmark\n\n\n\n\n\n\n  \n\n\n\n\nWorkforce/HR/People Analytics- Is This a Wake-Up Call for HR?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2014\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\n  \n\n\n\n\nIs it Time To Take Another Look at How to Calculate Employee Turnover?\n\n\n\n\n\n\n\nHR Analytics\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2014\n\n\nLyndon Sundmark, MBA\n\n\n\n\n\n\nNo matching items"
  }
]